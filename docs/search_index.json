[["index.html", "ç°ä»£ç²¾ç®—ç»Ÿè®¡æ¨¡å‹ ğŸ‘¨â€ğŸ« æ¬¢è¿ ğŸ¤” ç­”ç–‘ ğŸ—“ï¸ è¯¾ç¨‹å®‰æ’", " ç°ä»£ç²¾ç®—ç»Ÿè®¡æ¨¡å‹ Modern Actuarial Models 2020-11-30 15:57:05 ğŸ‘¨â€ğŸ« æ¬¢è¿ ã€Šç°ä»£ç²¾ç®—ç»Ÿè®¡æ¨¡å‹ã€‹ä¸»è¦è®²è¿°å¦‚ä½•ä½¿ç”¨ç»Ÿè®¡å­¦ä¹ å’Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæå‡ä¼ ç»Ÿçš„ç²¾ç®—ç»Ÿè®¡æ¨¡å‹æˆ–è€…è§£å†³æ–°çš„ç²¾ç®—é—®é¢˜ã€‚è¿™é—¨è¯¾ä¸»è¦å‚è€ƒç‘å£«ç²¾ç®—å¸ˆåä¼šå‘å¸ƒçš„â€œç²¾ç®—æ•°æ®ç§‘å­¦â€ï¼Œè¯¥æ•™ç¨‹çš„ä¸»è¦ç›®çš„æ˜¯â€œä¸ºç²¾ç®—å¸ˆæä¾›ä¸€ä¸ªå¯¹æ•°æ®ç§‘å­¦å…¨é¢ä¸”æ˜“æ‡‚çš„ä»‹ç»â€ï¼Œè¯¥æ•™ç¨‹æä¾›äº†å¤šç¯‡æ–¹æ³•æ€§æ–‡ç« å¹¶å¼€æºä»£ç ï¼Œè¿™æ ·â€œè¯»è€…å¯ä»¥ç›¸å¯¹å®¹æ˜“åœ°æŠŠè¿™äº›æ•°æ®ç§‘å­¦æ–¹æ³•ç”¨åœ¨è‡ªå·±çš„æ•°æ®ä¸Šâ€ã€‚ æˆ‘ä»¬å»ºè®®å¤§å®¶ä»”ç»†é˜…è¯»ä»¥ä¸‹æ–‡çŒ®ï¼Œå°è¯•å¹¶ç†è§£æ‰€æœ‰ä»£ç ã€‚æ­¤ç½‘ç«™å°†ä½œä¸ºè¯¥è¯¾ç¨‹çš„è¾…åŠ©ï¼Œä¸ºå¤§å®¶ç­”ç–‘ï¼Œæ€»ç»“æ–‡çŒ®ï¼Œå¹¶å¯¹æ–‡çŒ®ä¸­çš„æ–¹æ³•åšæ‰©å±•ã€‚è¯¥ç½‘ç«™ç”±æˆè¯¾è€å¸ˆé«˜å…‰è¿œå’ŒåŠ©æ•™å¼ ç®é’°ç®¡ç†ï¼Œæ¬¢è¿å¤§å®¶åé¦ˆæ„è§åˆ°åŠ©æ•™ã€å¾®ä¿¡ç¾¤ã€æˆ–é‚®ç®± guangyuan.gao@ruc.edu.cnã€‚ ğŸ¤” ç­”ç–‘ æˆ‘å®šæœŸæŠŠåŒå­¦ä»¬çš„æ™®éç–‘é—®åœ¨è¿™é‡Œè§£ç­”ï¼Œæ¬¢è¿æé—®ï¼ ğŸ‘‰ éšæœºç§å­æ•°(2020/11/20) è¾“å…¥RNGversion(\"3.5.0\"); set.seed(100)ï¼Œä½¿å¾—ä½ çš„éšæœºç§å­æ•°å’Œpaperçš„ç›¸åŒï¼Œæ¨¡å‹ç»“æœç›¸è¿‘ã€‚ ğŸ‘‰ MAC OS, Linux, WIN (2020/11/16) æ®è§‚å¯Ÿï¼Œåœ¨MAC OSå’ŒLinuxç³»ç»Ÿä¸‹å®‰è£…kerasæˆåŠŸçš„æ¯”ä¾‹è¾ƒé«˜ã€‚WINç³»ç»Ÿä¸‹ï¼ŒPythonå„ä¸ªåŒ…çš„ä¾èµ–ä»¥åŠå’ŒRåŒ…çš„åŒ¹é…æœ‰ä¸€å®šçš„é—®é¢˜ï¼Œä»Šå¤©æ˜¯é€šè¿‡æ›´æ¢é•œåƒæºè§£å†³äº†Rä¸­æ— æ³•åŠ è½½tensorflow.kerasæ¨¡å—çš„é—®é¢˜ï¼Œæ¨æµ‹æ˜¯TUNAæºä¸­WINåŒ…ä¾èµ–å…³ç³»æ²¡æœ‰åŠæ—¶æ›´æ–°ã€‚ ä¸ºäº†è§£å†³é•œåƒæºæ›´æ–°å»¶è¿Ÿã€æˆ–è€…tensorflowç‰ˆæœ¬è¿‡ä½çš„é—®é¢˜ï¼Œè¿™é‡Œå…±äº«WINä¸‹ç»æµ‹è¯•çš„condaç¯å¢ƒé…ç½®ã€‚ä¸‹è½½è¯¥æ–‡æ¡£ï¼Œä»è¯¥æ–‡æ¡£æ‰€åœ¨æ–‡ä»¶å¤¹å¯åŠ¨å‘½ä»¤è¡Œï¼Œä½¿ç”¨å‘½ä»¤conda env create --name &lt;env&gt; --file filename.yamlï¼Œå®‰è£…è¯¥condaç¯å¢ƒã€‚åœ¨Rä¸­ä½¿ç”¨reticulate::use_condaenv(\"&lt;env&gt;\",required=T)å…³è”è¯¥ç¯å¢ƒã€‚ å¦å¤–ï¼Œå¯ä¸‹è½½MAC OSç³»ç»Ÿä¸‹ç»æµ‹è¯•çš„condaç¯å¢ƒé…ç½®ã€‚å¯é€šè¿‡conda env create --name &lt;env&gt; --file filename.yamlå®‰è£…ã€‚ ğŸ‘‰ CASdatasets (2020/11/13) æºæ–‡ä»¶åœ¨http://cas.uqam.ca/ï¼Œä½†ä¸‹è½½é€Ÿåº¦å¾ˆæ…¢ï¼Œæˆ‘æŠŠå®ƒæ”¾åœ¨åšæœäº‘å…±äº«ã€‚ä¸‹è½½åé€‰æ‹©install from local archive fileã€‚ ğŸ‘‰ å¾®ä¿¡ç¾¤ (2020/11/08) ğŸ—“ï¸ è¯¾ç¨‹å®‰æ’ ä»¥ä¸‹å®‰æ’ä¸ºåˆæ­¥è®¡åˆ’ï¼Œæ ¹æ®å¤§å®¶çš„éœ€æ±‚å’ŒèƒŒæ™¯ï¼Œæˆ‘ä»¬å¯èƒ½è¦èŠ±æ›´å¤šçš„æ—¶é—´åœ¨æŸäº›é‡è¦çš„æ–¹æ³•åŠå…¶åœ¨ç²¾ç®—ä¸Šçš„åº”ç”¨ã€‚ ç¬¬10å‘¨ï¼š å‡†å¤‡å·¥ä½œã€‚ ç¬¬11å‘¨: 1 - French Motor Third-Party Liability Claims https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764 æœºåŠ¨ 2 - Inisghts from Inside Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3226852 3 - Nesting Classical Actuarial Models into Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525 ç¬¬12å‘¨ï¼š 4 - On Boosting: Theory and Applications https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3402687 ç¬¬13å‘¨ï¼š 5 - Unsupervised Learning: What is a Sports Car https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3439358 ç¬¬14å‘¨ï¼š 6 - Lee and Carter go Machine Learning: Recurrent Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3441030 ç¬¬15å‘¨ï¼š 7 - The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547887 ç¬¬16å‘¨ï¼š 8 - Peeking into the Black Box: An Actuarial Case Study for Interpretable Machine Learning https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3595944 ç¬¬17å‘¨ï¼š 9 - Convolutional neural network case studies: (1) Anomalies in Mortality Rates (2) Image Recognition https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3656210 "],["intro.html", "ç®€ä»‹", " ç®€ä»‹ ä¿é™©å…¬å¸ä¸ºç¤¾ä¼šä¸­æŸäº›ä¸å¯é¢„çŸ¥çš„ç»æµæŸå¤±å¸¦æ¥äº†ä¿éšœã€‚ä¿é™©å…¬å¸æ‰¿æ‹…äº†è¢«ä¿é™©äººçš„ä¸ç¡®å®šç»æµæŸå¤±çš„é£é™©ï¼Œè¢«ä¿é™©äººè·å¾—äº†ä¿éšœï¼Œä¿é™©å…¬å¸è·å¾—äº†ä¿è´¹ã€‚ é€šå¸¸ï¼Œä¿é™©å…¬å¸éœ€è¦åœ¨ä¿å•çš„ä¿é™©æœŸé™å¼€å§‹æ—¶ç¡®å®šä¿è´¹ï¼Œå³åœ¨ä¿é™©æŸå¤±è¿˜æœªå‘ç”Ÿæ—¶ç¡®å®šä¿è´¹ã€‚ç”±äºè¿™ç§å®šä»·æ–¹å¼ï¼Œä¿é™©äº§å“å’Œä¸€èˆ¬æ¶ˆè´¹äº§å“çš„ç”Ÿäº§å‘¨æœŸç›¸åï¼Œæœä»é€†ç”Ÿäº§å‘¨æœŸã€‚ å› æ­¤ï¼Œé¢„æµ‹æ¨¡å‹åœ¨ä¿é™©äº§å“çš„å®šä»·ä¸­æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚ è®¡ç®—æœºæŠ€æœ¯çš„å‘å±•å¸¦æ¥äº†è®¡ç®—é€Ÿåº¦çš„æå¤§æå‡å’Œå­˜å‚¨èƒ½åŠ›çš„æå¤§æé«˜ï¼Œå¯ä»¥çœ‹åˆ°å½“å‰å¾ˆå¤šé¢†åŸŸçš„å‘å±•éƒ½å’Œè®¡ç®—æœºæŠ€æœ¯çš„é©æ–°å¯†åˆ‡ç›¸å…³ã€‚ æœºå™¨å­¦ä¹ ç®—æ³•ä½œä¸ºä¸€ç§é¢„æµ‹æ¨¡å‹ï¼Œç»™ä¼ ç»Ÿçš„ç²¾ç®—å›å½’æ¨¡å‹å¸¦æ¥äº†æŒ‘æˆ˜å’Œæœºé‡ã€‚ æœºå™¨å­¦ä¹ ç®—æ³•çš„é¢„æµ‹èƒ½åŠ›ç›¸è¾ƒäºä¼ ç»Ÿå›å½’æ¨¡å‹æ›´é«˜; æœºå™¨å­¦ä¹ ç®—æ³•çš„è§£é‡Šæ€§æ¯”è¾ƒå·®ã€‚ åŸºäºæœºå™¨å­¦ä¹ ç®—æ³•çš„å®šä»·æ¨¡å‹æœ‰åŠ©äºä¿é™©å…¬å¸ç»†åˆ†é£é™©ï¼Œç²¾ç¡®å®šä»·ï¼Œå‡å°‘é€†é€‰æ‹©é£é™©ã€‚ å‡è®¾ä¿é™©å…¬å¸Aåœ¨å®šä»·ä¸­æ²¡æœ‰è€ƒè™‘åˆ°æŸä¸ªé‡è¦çš„é£é™©å› å­ï¼Œå³å¯¹äºæ˜¯å¦æœ‰è¯¥ç±»é£é™©çš„è¢«ä¿é™©äººéƒ½æ”¶å–ç›¸åŒä¿è´¹ï¼›è€Œä¿é™©å…¬å¸Båœ¨å®šä»·ä¸­è€ƒè™‘åˆ°è¯¥é£é™©å› å­ã€‚ ä¿é™©å…¬å¸Bä¼šå‡­å€Ÿä½ä¿è´¹å¸å¼•ä½é£é™©å®¢æˆ·ï¼Œå‡­å€Ÿé«˜ä¿è´¹ä½¿å¾—é«˜é£é™©å®¢æˆ·ç•™åœ¨ä¿é™©å…¬å¸Aã€‚ ç”±äºè¢«ä¿é™©äººçš„é€‰æ‹©æ•ˆåº”ï¼Œæœ€ç»ˆä¿é™©å…¬å¸Aæ”¶å–çš„ä¿è´¹ä¸è¶³ä»¥æ”¯ä»˜è¢«ä¿é™©äººçš„æŸå¤±ï¼Œæˆ–è€…éš¾ä»¥è·å–é¢„æœŸçš„ä¿é™©æ”¶ç›Šã€‚ å¦ä¸€æ–¹é¢ï¼Œä¿é™©å…¬å¸æ‰¿æ‹…ç€é£é™©è½¬ç§»å’Œé£é™©å…±æ‹…çš„ç¤¾ä¼šè§’è‰²ï¼Œè¿‡åº¦çš„ç»†åˆ†é£é™©ä¼šé€ æˆé£é™©ä¸ªä½“åŒ–ï¼Œä½¿å¾—ä¿é™©å…¬å¸çš„é£é™©è½¬ç§»å’Œå…±æ‹…çš„ä½œç”¨æ¶ˆå¤±ã€‚ æ¯”å¦‚ï¼ŒåŸºäºè¢«ä¿é™©äººçš„é«˜é£é™©ç‰¹å¾ï¼Œä¿é™©å…¬å¸ä¼šæ”¶å–æé«˜çš„ä¿è´¹ï¼Œä½¿å¾—è¢«ä¿é™©äººçš„é£é™©è½¬ç§»ä»·å€¼è¡ç„¶æ— å­˜ï¼Œä¹Ÿæ²¡æœ‰è´­ä¹°ä¿é™©çš„åŠ¨åŠ›ã€‚ æ‰€ä»¥ï¼Œä¿é™©å…¬å¸éœ€è¦å¹³è¡¡é£é™©ç»†åˆ†å’Œé£é™©å…±æ‹…ï¼Œä½¿å¾—ä¿é™©å…¬å¸æ—¢å¯ä»¥é¿å…é€†é€‰æ‹©ï¼Œä¹Ÿèƒ½æä¾›æœ‰æ•ˆçš„é£é™©è½¬ç§»çš„ä¿é™©äº§å“ã€‚ ä¸€èˆ¬åœ°ï¼Œä¿é™©å®šä»·æ¨¡å‹å—åˆ°ä¿é™©ç›‘ç®¡æœºæ„çš„ä¸¥æ ¼çº¦æŸï¼Œè¿™äº›æ¨¡å‹åœ¨åº”ç”¨åˆ°å®é™…ä¸­æ—¶ï¼Œå¿…é¡»æ»¡è¶³ä¸€å®šçš„æ¡ä»¶ã€‚è¿™ç»™æœºå™¨å­¦ä¹ ç®—æ³•åœ¨ä¿é™©å®šä»·ä¸­çš„åº”ç”¨å¸¦æ¥äº†å¾ˆå¤šé˜»ç¢ã€‚ åœ¨ä¸­å›½ï¼Œä¿ç›‘ä¼šé™å®šäº†å•†ä¸šè½¦é™©ä¿è´¹çš„åŒºé—´ï¼Œéšç€å•†ä¸šè½¦é™©è´¹ç‡æ”¹é©ï¼Œè¿™ä¸ªåŒºé—´åœ¨ä¸æ–­æ‰©å¤§ï¼Œä¿é™©å…¬å¸çš„å®šä»·æ¨¡å‹å‘æŒ¥çš„ä½œç”¨è¶Šæ¥è¶Šå¤§ã€‚ European Unionâ€™s General Data Protection Regulation (2018) å»ºç«‹äº†algorithmic accountability of decision-making machine algorithmsåˆ¶åº¦ï¼Œè¯¥åˆ¶åº¦èµ‹äºˆäº†å‚ä¸è€…å¯¹äºæœºå™¨å­¦ä¹ ç®—æ³•èƒŒåé€»è¾‘çš„çŸ¥æƒ…æƒã€‚ æ€»ä¹‹ï¼Œå®šä»·æ¨¡å‹å¿…é¡»åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¯ä»¥è§£é‡Šç»™è¢«ä¿é™©äººã€ä¿é™©ç›‘ç®¡æœºæ„ç­‰ã€‚ä»è¢«ä¿é™©äººå’Œä¿é™©ç›‘ç®¡çš„è§’åº¦å‡ºå‘ï¼Œä»–ä»¬ä¹Ÿå¸Œæœ›äº§å“å®šä»·å’Œé£é™©ç®¡ç†æ˜¯å»ºç«‹åœ¨ä¸€ä¸ªè¾ƒé€æ˜çš„æ¨¡å‹è€Œä¸æ˜¯ä¸€ä¸ªé»‘ç›’å­ï¼Œè¿™æ ·æœ‰åˆ©äºç»´æŠ¤å¸‚åœºå…¬å¹³ã€ä¿éšœè¢«ä¿é™©äººçš„åˆ©ç›Šã€æ£€æµ‹é‡è¦é£é™©å› å­ã€å»ºç«‹é˜²èŒƒé£é™©æªæ–½ã€‚ "],["pre.html", "1 å‡†å¤‡å·¥ä½œ 1.1 å¸¸ç”¨é“¾æ¥ 1.2 å…‹éš†ä»£ç  1.3 R interface to Keras 1.4 R interface to Python 1.5 Python", " 1 å‡†å¤‡å·¥ä½œ â€œå·¥æ¬²å–„å…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨ã€‚â€ åœ¨ä»¥ä¸‹æ­¥éª¤ä¸­ï¼Œå½“ä½ å‘ç°å®‰è£…éå¸¸æ…¢æ—¶ï¼Œå¯ä»¥å°è¯•4Gç½‘ç»œï¼Œå°è¯•VPNï¼Œå°è¯•æ”¹å˜CRANçš„é•œåƒæºï¼Œæˆ–å°è¯•æ”¹å˜condaçš„é•œåƒæºã€‚condaé•œåƒæºé€šè¿‡ä¿®æ”¹ç”¨æˆ·ç›®å½•ä¸‹çš„.condarcæ–‡ä»¶ä½¿ç”¨TUNAé•œåƒæºï¼Œä½†è¯¥é•œåƒæºå¯èƒ½æœ‰æ›´æ–°å»¶è¿Ÿã€‚ 1.1 å¸¸ç”¨é“¾æ¥ å‡†å¤‡å·¥ä½œä¸­å¸¸ç”¨çš„é“¾æ¥æœ‰ GitHub Git SSH key GitHub and RStudio Jupyter Notebook Anaconda Miniconda å¸¸ç”¨Condaå‘½ä»¤ TUNAé•œåƒæº R interface to Tensorflow and Keras reticulate Tensorflow Pytorch æ ¡çº§è®¡ç®—äº‘ CUDA cuDNN 1.2 å…‹éš†ä»£ç  GitHubæä¾›äº†å¤§é‡å¼€æºä»£ç ï¼Œè¿™é—¨è¯¾çš„ä»£ç ä¸»è¦æ¥è‡ªæ­¤é“¾æ¥ã€‚é€šå¸¸ï¼Œä½¿ç”¨GitHubå¼€æºä»£ç æœ€æ–¹ä¾¿çš„æ˜¯forkåˆ°è‡ªå·±GitHubè´¦æˆ·ä¸‹ï¼Œç„¶åcloneåˆ°æœ¬åœ°ã€‚å…·ä½“è€Œè¨€ï¼Œéœ€è¦è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š æ³¨å†ŒGitHubè´¦æˆ·ã€‚ Forkæ­¤é“¾æ¥åˆ°è‡ªå·±è´¦æˆ·ä¸‹çš„æ–°ä»“åº“,å¯é‡æ–°å‘½åä¸ºå¦‚Modern-Actuarial-Modelsæˆ–å…¶ä»–åç§°ã€‚ å®‰è£…gitã€‚åœ¨å‘½ä»¤çª—å£ä½¿ç”¨$ git config --global user.name \"Your Name\" å’Œ $ git config --global user.email \"youremail@yourdomain.com\" é…ç½®gitçš„ç”¨æˆ·åå’Œé‚®ç®±åˆ†åˆ«ä¸ºGitHubè´¦æˆ·çš„ç”¨æˆ·åå’Œé‚®ç®±ã€‚æœ€åå¯ä½¿ç”¨$ git config --listæŸ¥çœ‹é…ç½®ä¿¡æ¯ã€‚ (é€‰åš)åœ¨æœ¬åœ°ç”µè„‘åˆ›å»ºssh public keyï¼Œå¹¶æ‹·è´åˆ°GitHubä¸­Settingä¸‹SSH and GPG keysã€‚ssh public keyä¸€èˆ¬ä¿å­˜åœ¨æœ¬äººç›®å½•ä¸‹çš„éšè—æ–‡ä»¶å¤¹.sshä¸­ï¼Œæ‰©å±•åä¸º.pubã€‚è¯¦è§é“¾æ¥ã€‚è®¾ç«‹SSHå¯ä»¥é¿å…åç»­pushä»£ç åˆ°äº‘ç«¯æ—¶ï¼Œæ¯æ¬¡éƒ½éœ€è¦è¾“å…¥å¯†ç çš„éº»çƒ¦ ç”µè„‘è¿æ¥æ‰‹æœº4Gçƒ­ç‚¹ã€‚ä¸€èˆ¬åœ°ï¼Œåœ¨æ‰‹æœº4Gç½‘ç»œä¸‹å…‹éš†çš„é€Ÿåº¦æ¯”è¾ƒå¿«ã€‚ åœ¨RStudioä¸­åˆ›å»ºæ–°çš„é¡¹ç›®ï¼Œé€‰æ‹©Version Controlï¼Œç„¶åGitï¼Œåœ¨Repository URLä¸­è¾“å…¥ä½ çš„GitHubä¸­åˆšæ‰forkçš„æ–°ä»“åº“åœ°å€ï¼ˆåœ¨Codeä¸‹èƒ½æ‰¾åˆ°å…‹éš†åœ°å€ï¼Œå¦‚æœç¬¬4æ­¥å®Œæˆå¯ä»¥é€‰æ‹©SSHåœ°å€ï¼Œå¦‚æœç¬¬4æ­¥æ²¡å®Œæˆå¿…é¡»é€‰æ‹©HTTPSåœ°å€ï¼‰ï¼Œè¾“å…¥æ–‡ä»¶å¤¹åç§°ï¼Œé€‰æ‹©å­˜æ”¾ä½ç½®ï¼Œç‚¹å‡»create projectï¼ŒRStudioå¼€å§‹å…‹éš†GitHubä¸Šè¯¥ä»“åº“çš„æ‰€æœ‰å†…å®¹ã€‚ æ­¤æ—¶ï¼Œä½ åœ¨GitHubä¸Šä»“åº“çš„å†…å®¹å…¨éƒ¨å…‹éš†åˆ°äº†æœ¬åœ°ï¼Œä¸”æ”¾åœ¨äº†ä¸€ä¸ªR Projectä¸­ã€‚åœ¨è¯¥Projectä¸­ï¼Œä¼šå¤šä¸¤ä¸ªæ–‡ä»¶ï¼Œ.Rprojå’Œ.gitignoreï¼Œç¬¬ä¸€ä¸ªæ–‡ä»¶ä¿å­˜äº†Projectçš„è®¾ç½®ï¼Œç¬¬äºŒæ–‡ä»¶å‘Šè¯‰gitåœ¨pushæœ¬åœ°æ–‡ä»¶åˆ°GitHubæ—¶å“ªäº›æ–‡ä»¶è¢«å¿½ç•¥ã€‚ å¦‚æœä½ ä¿®æ”¹äº†æœ¬åœ°æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡Rä¸­å†…åµŒçš„Gitä¸Šä¼ åˆ°GitHubï¼ˆå…ˆcommitå†pushï¼‰ï¼Œè¿™æ ·æ–¹ä¾¿åœ¨ä¸åŒç”µè„‘ä¸ŠåŒæ­¥æ–‡ä»¶ã€‚gitæ˜¯ä»£ç ç‰ˆæœ¬æ§åˆ¶å·¥å…·ï¼Œåœ¨pushä¹‹å‰ï¼Œä½ å¯ä»¥æ¯”è¾ƒå’Œä¸Šä¸ªä»£ç ç‰ˆæœ¬çš„å·®å¼‚ã€‚GitHubè®°å½•äº†ä½ æ¯æ¬¡pushçš„è¯¦ç»†ä¿¡æ¯ï¼Œä¸”å­˜æ”¾åœ¨æœ¬åœ°æ–‡ä»¶å¤¹.gitä¸­ã€‚åŒæ—¶ï¼Œå¦‚æœGitHubä¸Šä»£ç æœ‰å˜åŒ–ï¼Œä½ å¯ä»¥pullåˆ°æœ¬åœ°ã€‚å¦‚æœç»å¸¸åœ¨ä¸åŒç”µè„‘ä¸Šä½¿ç”¨æœ¬ä»“åº“ï¼Œä¸€èˆ¬éœ€è¦å…ˆpullæˆæœ€æ–°ç‰ˆæœ¬ï¼Œç„¶åå†ç¼–è¾‘ä¿®æ”¹ï¼Œæœ€åcommit-pushåˆ°GitHubã€‚ (é€‰åš) ä½ å¯ä»¥å»ºç«‹æ–°çš„branchï¼Œä½¿è‡ªå·±çš„ä¿®æ”¹å’Œæºä»£ç åˆ†å¼€ã€‚å…·ä½“æ“ä½œå¯å‚è€ƒé“¾æ¥ï¼Œæˆ–è€…å‚è€ƒè´¦æˆ·å»ºç«‹æ—¶è‡ªåŠ¨äº§ç”Ÿçš„getting-startedä»“åº“ã€‚ (é€‰åš) ä½ å¯ä»¥å°è¯•Github Desktopæˆ–è€…Jupyter Labï¼ˆåŠ è½½git extensionï¼‰ç®¡ç†ï¼Œä½†å¯¹äºè¿™é—¨è¯¾ï¼Œè¿™ä¸¤ç§æ–¹å¼ä¸æ˜¯æœ€ä¼˜ã€‚ ç†è®ºä¸Šï¼ŒGitHubä¸Šæ‰€æœ‰ä»“åº“éƒ½å¯ä»¥é‡‡ç”¨ä»¥ä¸Šæ–¹æ³•åœ¨RStudioä¸­ç®¡ç†ï¼Œå½“ç„¶ï¼ŒRStudioå¯¹äºRä»£ç ä»“åº“ç®¡ç†æœ€æœ‰æ•ˆï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨RStudioä¸­è¿è¡Œä»“åº“ä¸­çš„ä»£ç ã€‚ 1.3 R interface to Keras è¿™é‡Œä¸»è¦è¯´æ˜kerasåŒ…çš„å®‰è£…å’Œä½¿ç”¨ã€‚Kerasæ˜¯tensorflowçš„APIï¼Œåœ¨kerasä¸­å»ºç«‹çš„ç¥ç»ç½‘ç»œæ¨¡å‹éƒ½ç”±tensorflowè®­ç»ƒã€‚å®‰è£…kerasåŒ…ä¸»è¦æ˜¯å®‰è£…Pythonåº“tensorflowï¼Œå¹¶è®©Rä¸ä¹‹ç›¸å…³è”ã€‚ 1.3.1 Rè‡ªåŠ¨å®‰è£… æœ€ç®€å•çš„å®‰è£…æ–¹å¼å¦‚ä¸‹ï¼š ä½¿ç”¨install.packages(\"tensorflow\")å®‰è£…æ‰€æœ‰ç›¸å…³çš„åŒ…ï¼Œç„¶ålibrary(\"tensorflow\")ã€‚ install_tensorflow() è¿™æ—¶å¤§æ¦‚ç‡ä¼šå‡ºç° No non-system installation of Python could be found. Would you like to download and install Miniconda? Miniconda is an open source environment management system for Python. See https://docs.conda.io/en/latest/miniconda.html for more details. Would you like to install Miniconda? [Y/n]: è™½ç„¶ä½ å¯èƒ½å·²ç»æœ‰Anacondaå’ŒPythonï¼Œä½†Ræ²¡æœ‰â€œæ™ºèƒ½â€åœ°è¯†åˆ«å‡ºæ¥ï¼Œè¿™æ—¶ä»å»ºè®®ä½ é€‰Yï¼Œè®©Rè‡ªå·±è£…ä¸€ä¸‹è‡ªå·±èƒ½æ›´å¥½è¯†åˆ«çš„Miniconda, è¿™ä¸ªå‘½ä»¤è¿˜ä¼šè‡ªåŠ¨å»ºç«‹ä¸€ä¸ªç‹¬ç«‹condaç¯å¢ƒr-reticulateï¼Œå¹¶åœ¨å…¶ä¸­è£…å¥½tensorflow, kerasç­‰ã€‚ ä¸Šæ­¥å¦‚æœæ­£å¸¸è¿è¡Œï¼Œç»“æŸåä¼šè‡ªåŠ¨é‡å¯Rã€‚è¿™æ—¶ä½ è¿è¡Œlibrary(tensorflow)ç„¶åtf$constant(\"Hellow Tensorflow\")ï¼Œå¦‚æœæ²¡æŠ¥é”™ï¼Œé‚£ç»§ç»­install_packages(\"keras\"),library(\"keras\")ã€‚ ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) å¦‚æœå‡ºç°ä»¥ä¸‹é”™è¯¯ é”™è¯¯: Installation of TensorFlow not found. Python environments searched for &#39;tensorflow&#39; package: C:\\Users\\...\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\python.exe You can install TensorFlow using the install_tensorflow() function. è¿™ä¸ªé”™è¯¯é€šå¸¸æ˜¯ç”±äºr-reticulateä¸­tensorflowå’Œå…¶ä»–åŒ…çš„ä¾èµ–å…³ç³»å‘ç”Ÿé”™è¯¯ï¼Œæˆ–è€…tensorflowç‰ˆæœ¬å¤ªä½ï¼Œä½ å¯ä»¥æ›´æ¢é•œåƒæºã€ä½¿ç”¨conda/pip installè°ƒæ•´è¯¥ç¯å¢ƒä¸­çš„tensorflowç‰ˆæœ¬å’Œä¾èµ–å…³ç³»ã€‚ æ›´å¥½çš„æ–¹å¼æ˜¯åœ¨condaä¸‹å®‰è£…å¥½æŒ‡å®šç‰ˆæœ¬çš„tensorflowç„¶åå…³è”åˆ°Rï¼Œæˆ–è€…ç”¨å…¶ä»–æ–¹å¼è®©Ræ‰¾åˆ°å…¶ä»–æ–¹å¼å®‰è£…çš„tensorflowã€‚è¿™æ—¶ï¼Œä½ å…ˆæŠŠä¹‹å‰å¤±è´¥çš„å®‰è£…C:\\Users\\...\\AppData\\Local\\r-minicondaï¼Œè¿™ä¸ªæ–‡ä»¶å¤¹å®Œå…¨åˆ æ‰ã€‚ç„¶åå‚è€ƒä»¥ä¸‹å®‰è£…æ­¥éª¤ã€‚ 1.3.2 ä½¿ç”¨reticulateå…³è”condaç¯å¢ƒ ä¸‹è½½å¹¶å®‰è£…Anacondaæˆ–è€…Minicondaã€‚ è¿è¡ŒAnaconda Promptæˆ–è€…Anaconda Powershell Promptï¼Œåœ¨å‘½ä»¤è¡Œè¾“å…¥conda create -n r-tensorflow tensorflow=2.1.0ï¼Œcondaä¼šåˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„r-tensorflowç¯å¢ƒï¼Œå¹¶åœ¨å…¶ä¸­å®‰è£…tensorflowåŒ…ã€‚ ç»§ç»­åœ¨å‘½ä»¤è¡Œè¿è¡Œconda activate r-tensorflowåŠ è½½åˆšåˆšå®‰è£…çš„ç¯å¢ƒï¼Œå¹¶pip install h5py pyyaml requests Pillow scipyåœ¨è¯¥ç¯å¢ƒä¸‹å®‰è£…kerasä¾èµ–çš„åŒ…ã€‚è‡³æ­¤ï¼ŒRéœ€è¦çš„tensorflowç¯å¢ƒå·²ç»å‡†å¤‡å¥½ï¼Œæ¥ä¸‹æ¥è®©Rå…³è”æ­¤ç¯å¢ƒã€‚ é‡å¯Rï¼Œlibrary(\"reticulate\")ç„¶åuse_condaenv(\"r-tensorflow\",required=T),è¿™æ—¶Rå°±å’Œä¸Šé¢å»ºç«‹çš„ç¯å¢ƒå…³è”å¥½ã€‚ library(\"kerasâ€œ)ã€‚è¿™é‡Œå‡è®¾ä½ å·²ç»è£…å¥½tensorflowå’ŒkerasåŒ…ã€‚ ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 1.3.3 æŒ‡å®šcondaå®‰è£… ä¸‹è½½å¹¶å®‰è£…Anacondaæˆ–è€…Minicondaã€‚ å‘½ä»¤è¡Œè¾“å…¥which -a pythonï¼Œæ‰¾åˆ°Anacondaä¸­Pythonçš„è·¯å¾„è®°ä¸ºanapyã€‚ Rä¸­install_packages(\"tensorflow\")ï¼Œç„¶å install_tensorflow(method = &quot;conda&quot;, conda = &quot;anapy&quot;, envname = &quot;r-tensorflow&quot;, version = &quot;2.1.0&quot;) æ­¤å‘½ä»¤ä¼šåœ¨condaä¸‹åˆ›å»ºr-tensorflowçš„ç¯å¢ƒå¹¶è£…å¥½tensorflowåŒ…ã€‚ install_packages(\"keras\"); library(\"keras\") ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 1.3.4 ä½¿ç”¨reticulateå®‰è£… é‡å¯Rï¼Œlibrary(\"reticulate\")ã€‚ options(timeout=300)ï¼Œé˜²æ­¢ä¸‹è½½æ—¶é—´è¿‡é•¿ä¸­æ–­ã€‚ install_miniconda()ï¼Œå°†ä¼šå®‰è£…minicondaå¹¶åˆ›å»ºä¸€ä¸ªr-reticulatecondaç¯å¢ƒã€‚æ­¤ç¯å¢ƒä¸ºRé»˜è®¤è°ƒç”¨çš„Pythonç¯å¢ƒã€‚ ï¼ˆé‡å¯Rï¼‰library(\"tensorflow\"); install_tensorflow(version=\"2.1.0\")ï¼Œå°†ä¼šåœ¨r-reticulateå®‰è£…tensorflowã€‚ install_packages(\"keras\"); library(\"keras\") ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 1.4 R interface to Python RåŒ…reticulateä¸ºtensorflowçš„ä¾èµ–åŒ…ï¼Œå½“ä½ è£…tensorflowå®ƒä¹Ÿè¢«è‡ªåŠ¨å®‰è£…ã€‚å®ƒå¯ä»¥å»ºç«‹Rä¸Pythonçš„äº¤äº’ã€‚ 1.4.1 reticulate å¸¸è§å‘½ä»¤ conda_list()åˆ—å‡ºå·²å®‰è£…çš„condaç¯å¢ƒ virtualenv_list()åˆ—å‡ºå·²å­˜åœ¨çš„è™šæ‹Ÿç¯å¢ƒ use_python, use_condaenv, use_virtualenvå¯ä»¥æŒ‡å®šä¸Rå…³è”çš„pythonã€‚ py_config()å¯ä»¥æŸ¥çœ‹å½“å‰Pythonå…³è”ä¿¡æ¯ã€‚ å¾ˆå¤šæ—¶å€™ï¼ŒRä¼šåˆ›å»ºä¸€ä¸ªç‹¬ç«‹condaç¯å¢ƒr-miniconda/envs/r-reticulateã€‚ 1.4.2 åˆ‡æ¢Rå…³è”çš„condaç¯å¢ƒ æ ¹æ®éœ€è¦ï¼Œä½ å¯ä»¥åˆ‡æ¢Rå…³è”çš„condaç¯å¢ƒã€‚å…·ä½“æ­¥éª¤ä¸º é‡å¯R library(\"reticulate\") conda_list()åˆ—å‡ºå¯ä»¥å…³è”çš„ç¯å¢ƒå’Œè·¯å¾„ã€‚ use_condaenv(\"env-name\")ã€‚env-nameä¸ºå…³è”çš„condaç¯å¢ƒã€‚ py_configæŸ¥çœ‹æ˜¯å¦å…³è”æˆåŠŸã€‚ 1.5 Python ä¸€èˆ¬åœ¨æ¯ä¸ªPythonï¼ˆCondaï¼‰ç¯å¢ƒéƒ½éœ€è¦å®‰è£…ä¸€ä¸ªJupyter Notebook (conda install notebook)ã€‚ 1.5.1 Condaç¯å¢ƒ Pythonï¼ˆcondaï¼‰ç¯å¢ƒå»ºç«‹æ¯”è¾ƒç®€å•ï¼Œåœ¨ä½¿ç”¨reticulateå…³è”condaç¯å¢ƒæˆ‘ä»¬å·²ç»å»ºç«‹è¿‡ä¸€ä¸ªç¯å¢ƒr-tensorflowã€‚å…·ä½“æ“ä½œå¦‚ä¸‹: å»ºç«‹ç‹¬ç«‹ç¯å¢ƒconda create -n env-name python=3.8 tensorflow=2.1.0 notebookã€‚è¯¥å‘½ä»¤ä¼šå»ºç«‹env-nameçš„ç¯å¢ƒï¼Œå¹¶åœ¨å…¶ä¸­å®‰è£…python=3.8,tensorflowï¼ŒnotebookåŒ…åŠå…¶ä¾èµ–åŒ…ã€‚ æ¿€æ´»ç¯å¢ƒconda activate env-name. cd åˆ°ä½ çš„å·¥ä½œç›®å½•ã€‚ å¯åŠ¨jupyter notebook jupyter notebookã€‚ å¦‚é‡åˆ°ç¼ºå°‘çš„åŒ…ï¼Œåœ¨è¯¥ç¯å¢ƒenv-nameä¸‹ä½¿ç”¨conda install ***å®‰è£…ç¼ºå°‘çš„åŒ…ã€‚ 1.5.2 å¸¸ç”¨çš„Condaå‘½ä»¤ conda create -n env-name2 --clone env-name1:å¤åˆ¶ç¯å¢ƒ conda env listï¼šåˆ—å‡ºæ‰€æœ‰ç¯å¢ƒ conda deactivateï¼šé€€å‡ºå½“å‰ç¯å¢ƒ conda remove -n env-name --allï¼šåˆ é™¤ç¯å¢ƒenv-nameä¸­çš„æ‰€æœ‰åŒ… conda list -n env-name: åˆ—å‡ºç¯å¢ƒenv-nameæ‰€å®‰è£…çš„åŒ… conda clean -pï¼šåˆ é™¤ä¸ä½¿ç”¨çš„åŒ… conda clean -tï¼šåˆ é™¤ä¸‹è½½çš„åŒ… conda clean -aï¼šåˆ é™¤æ‰€æœ‰ä¸å¿…è¦çš„åŒ… pip freeze &gt; pip_pkg.txt, pip install -r pip_pkg.txt ä¿å­˜å½“å‰ç¯å¢ƒPyPIåŒ…ç‰ˆæœ¬ï¼Œä»æ–‡ä»¶å®‰è£…PyPIåŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ conda env export &gt; conda_pkg.yaml, conda env export --name env_name &gt; conda_pkg.yaml, conda env create --name env-name2 --file conda_pkg.yaml ä¿å­˜å½“å‰/env-nameç¯å¢ƒæ‰€æœ‰åŒ…ï¼Œä»æ–‡ä»¶å®‰è£…æ‰€æœ‰åŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ conda list --explicit &gt; spec-list.txt, conda create --name env-name2 --file spec-list.txt ä¿å­˜å½“å‰ç¯å¢ƒCondaåŒ…ä¸‹è½½åœ°å€ï¼Œä»æ–‡ä»¶å®‰è£…CondaåŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ conda list --export &gt; spec-list.txt, conda create --name env-name2 --file spec-list.txt ä¿å­˜å½“å‰ç¯å¢ƒæ‰€æœ‰åŒ…ï¼ˆç±»ä¼¼conda env exportï¼‰ï¼Œä»æ–‡ä»¶å®‰è£…æ‰€æœ‰åŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ 1.5.3 Tensorflow/Pytorch GPU version Tensorflowå¯ä»¥ç»¼åˆä½¿ç”¨CPUå’ŒGPUè¿›è¡Œè®¡ç®—ï¼ŒGPUçš„ç¡¬ä»¶ç»“æ„é€‚è¿›è¡Œå·ç§¯è¿ç®—ï¼Œæ‰€ä»¥é€‚äºCNNï¼ŒRNNç­‰æ¨¡å‹çš„æ±‚è§£ã€‚ ä½ å¯ä»¥ç”³è¯·ä½¿ç”¨æ ¡çº§è®¡ç®—äº‘æˆ–è€…ä½¿ç”¨å­¦é™¢è®¡ç®—äº‘ï¼Œå®ƒä»¬çš„æœåŠ¡å™¨éƒ½é…ç½®äº†GPUï¼Œå¹¶è£…å¥½äº†å¯ä»¥ä½¿ç”¨GPUçš„Tensorflowæˆ–è€…Pytorchã€‚ä½¿ç”¨æ ¡çº§è®¡ç®—äº‘æ—¶ï¼Œä½ é€šå¸¸åªéœ€è¦è¿è¡ŒJupyter Notebookå°±å¯ä»¥ä½¿ç”¨äº‘ç«¯GPUè¿›è¡Œè®¡ç®—ã€‚ä½¿ç”¨å­¦é™¢è®¡ç®—äº‘æ—¶ï¼Œä½ é€šå¸¸éœ€è¦çŸ¥é“ä¸€äº›å¸¸ç”¨çš„Linuxå‘½ä»¤ï¼Œä½ ä¹Ÿå¯ä»¥å®‰è£…Ubuntuæ¥ç†Ÿæ‚‰Linuxç³»ç»Ÿã€‚ æ ¡çº§è®¡ç®—äº‘å’Œå­¦é™¢è®¡ç®—äº‘æœ‰ä¸“é—¨çš„ITäººå‘˜å¸®ä½ è§£å†³å¦‚æœ¬é¡µæ‰€ç¤ºçš„å¤§éƒ¨åˆ†ITé—®é¢˜ã€‚ ä½ çš„æœºå™¨å¦‚æœæœ‰GPUï¼Œå¯ä»¥æŒ‰å¦‚ä¸‹æ­¥éª¤è®©GPUå‘æŒ¥å®ƒçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œå…³é”®ç‚¹æ˜¯è®©GPUå‹å·ã€GPUé©±åŠ¨ã€CUDAç‰ˆæœ¬ã€Tensorflowæˆ–Pytorchç‰ˆæœ¬å½¼æ­¤åŒ¹é…ï¼Œä¸”å½¼æ­¤â€œç›¸è¿â€ã€‚ç™¾åº¦æˆ–è€…å¿…åº”ä¸Šæœ‰å¾ˆå¤šç›¸å…³èµ„æ–™å¯ä»¥ä½œä¸ºå‚è€ƒã€‚ æŸ¥çœ‹ç”µè„‘GPUå’Œé©±åŠ¨ï¼Œä»¥åŠæ”¯æŒçš„CUDAç‰ˆæœ¬ã€‚ æˆ–è€…åœ¨ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼šnvidia-smiï¼ŒæŸ¥çœ‹ä½ çš„NVIDIAæ˜¾å¡é©±åŠ¨æ”¯æŒçš„CUDAç‰ˆæœ¬ã€‚ æŸ¥çœ‹å„ä¸ªTensorflowç‰ˆæœ¬ï¼ŒPytorchç‰ˆæœ¬å¯¹åº”çš„CUDAå’ŒcuDNN. ä¸‹è½½å¹¶å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„CUDAã€‚æ³¨å†Œã€ä¸‹è½½å¹¶å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„cuDNN é…ç½®CUDAå’ŒcuDNN. å®‰è£…Tensorflowæˆ–è€…Pytorch. "],["french.html", "2 è½¦é™©ç´¢èµ”é¢‘ç‡é¢„æµ‹ 2.1 èƒŒæ™¯ä»‹ç» 2.2 é¢„æµ‹æ¨¡å‹æ¦‚è¿° 2.3 ç‰¹å¾å·¥ç¨‹ 2.4 è®­ç»ƒé›†-éªŒè¯é›†-æµ‹è¯•é›† 2.5 æ³Šæ¾åå·®æŸå¤±å‡½æ•° 2.6 æ³Šæ¾å›å½’æ¨¡å‹ 2.7 æ³Šæ¾å¯åŠ æ¨¡å‹ 2.8 æ³Šæ¾å›å½’æ ‘ 2.9 éšæœºæ£®æ— 2.10 æ³Šæ¾æå‡æ ‘ 2.11 æ¨¡å‹æ¯”è¾ƒ", " 2 è½¦é™©ç´¢èµ”é¢‘ç‡é¢„æµ‹ â€œè§å¤šè¯†å¹¿ã€éšæœºåº”å˜â€ 2.1 èƒŒæ™¯ä»‹ç» è½¦é™©æ•°æ®é‡å¤§ï¼Œé£é™©ç‰¹å¾å¤šï¼Œå¯¹è½¦é™©æ•°æ®åˆ†ææ—¶å¯ä»¥ä½“ç°å‡ºæœºå™¨å­¦ä¹ ç®—æ³•çš„ä¼˜åŠ¿ï¼Œå³ä½¿ç”¨ç®—æ³•ä»å¤§æ•°æ®ä¸­æŒ–æ˜æœ‰ç”¨ä¿¡æ¯ã€æå–ç‰¹å¾ã€‚ åœ¨ç²¾ç®—ä¸­ï¼Œå¸¸å¸¸ä½¿ç”¨è½¦é™©ä¿å•æ•°æ®å’Œå†å²ç´¢èµ”æ•°æ®è¿›è¡Œé£é™©åˆ†æã€è½¦é™©å®šä»·ç­‰ã€‚ä¿å•æ•°æ®åº“æ˜¯åœ¨æ‰¿ä¿çš„æ—¶å€™å»ºç«‹çš„ï¼Œç´¢èµ”æ•°æ®åº“æ˜¯åœ¨ç´¢èµ”å‘ç”Ÿæ—¶å»ºç«‹çš„ï¼Œå¤§éƒ¨åˆ†ä¿å•æ²¡æœ‰å‘ç”Ÿç´¢èµ”ï¼Œæ‰€ä»¥å®ƒä»¬ä¸ä¼šåœ¨ç´¢èµ”æ•°æ®åº“ä¸­ä½“ç°ã€‚ ä¿å•æ•°æ®åº“è®°å½•äº†è½¦é™©çš„é£é™©ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š é©¾é©¶å‘˜ç‰¹å¾ï¼šå¹´é¾„ã€æ€§åˆ«ã€å·¥ä½œã€å©šå§»ã€åœ°å€ç­‰ è½¦è¾†ç‰¹å¾ï¼šå“ç‰Œã€è½¦åº§æ•°ã€è½¦é¾„ã€ä»·æ ¼ã€é©¬åŠ›ç­‰ ä¿å•ä¿¡æ¯ï¼šä¿å•ç¼–å·ã€æ‰¿ä¿æ—¥æœŸã€åˆ°æœŸæ—¥æœŸ å¥–æƒ©ç³»æ•° ç´¢èµ”æ•°æ®åº“è®°å½•äº†ä¿å•çš„ç´¢èµ”ä¿¡æ¯ï¼Œå¯ä»¥å¾—åˆ°ç´¢èµ”æ¬¡æ•°\\(N\\)å’Œæ¯æ¬¡çš„ç´¢èµ”é‡‘é¢\\(Y_l,l=1,\\ldots,N\\)ã€‚ç†è®ºä¸Šï¼Œè½¦é™©çš„çº¯ä¿è´¹ä¸ºä»¥ä¸‹éšæœºå’Œçš„æœŸæœ› \\[S=\\sum_{l=1}^N Y_l\\] å‡è®¾ç´¢èµ”æ¬¡æ•°\\(N\\)å’Œç´¢èµ”é‡‘é¢\\(Y_l\\)ç‹¬ç«‹ä¸”\\(Y_l\\)æœä»ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œåˆ™ \\[\\mathbf{E}(S)=\\mathbf{E}(N)\\times\\mathbf{E}(Y)\\] æ‰€ä»¥ï¼Œè½¦é™©å®šä»·é—®é¢˜å¾ˆå¤šæ—¶å€™éƒ½è½¬åŒ–ä¸ºä¸¤ä¸ªç‹¬ç«‹æ¨¡å‹ï¼šç´¢èµ”æ¬¡æ•°ï¼ˆé¢‘ç‡ï¼‰æ¨¡å‹å’Œç´¢èµ”é‡‘é¢ï¼ˆå¼ºåº¦ï¼‰æ¨¡å‹ã€‚å¯¹äºç´¢èµ”æ¬¡æ•°æ¨¡å‹ï¼Œé€šå¸¸å‡è®¾å› å˜é‡æœä»æ³Šæ¾åˆ†å¸ƒï¼Œå»ºç«‹æ³Šæ¾å›å½’æ¨¡å‹ï¼Œä½¿ç”¨çš„æ•°æ®é‡ç­‰äºä¿å•æ•°ï¼›å¯¹äºç´¢èµ”é‡‘é¢æ¨¡å‹ï¼Œé€šå¸¸å‡è®¾å› å˜é‡æœä»ä¼½é©¬åˆ†å¸ƒï¼Œå»ºç«‹ä¼½é©¬å›å½’æ¨¡å‹ï¼Œä½¿ç”¨çš„æ•°æ®é‡ç­‰äºå‘ç”Ÿç´¢èµ”çš„ä¿å•æ•°ã€‚é€šå¸¸ï¼Œåœ¨æ•°æ®é‡ä¸å¤§æ—¶ï¼Œç´¢èµ”é‡‘é¢æ¨¡å‹çš„å»ºç«‹éš¾äºç´¢èµ”æ¬¡æ•°æ¨¡å‹ï¼Œå› ä¸ºåªæœ‰å‘ç”Ÿç´¢èµ”çš„ä¿å•æ‰èƒ½ç”¨äºç´¢èµ”é‡‘é¢æ¨¡å‹çš„å»ºç«‹ã€‚ è®°ç¬¬\\(i\\)ä¸ªä¿å•çš„é£é™©ä¿¡æ¯ä¸º\\(x_i\\in\\mathcal{X}\\)ï¼Œä¿é™©å…¬å¸å®šä»·çš„ç›®æ ‡å°±æ˜¯æ‰¾åˆ°ä¸¤ä¸ªï¼ˆæœ€ä¼˜ï¼‰å›å½’æ–¹ç¨‹ï¼ˆæ˜ å°„ï¼‰ï¼Œä½¿ä¹‹å°½å¯èƒ½å‡†ç¡®åœ°é¢„æµ‹ç´¢èµ”é¢‘ç‡å’Œç´¢èµ”å¼ºåº¦: \\[\\lambda: \\mathcal{X}\\rightarrow \\mathbf{R}_+, ~~~ x \\mapsto \\lambda(x_i)\\] \\[\\mu: \\mathcal{X}\\rightarrow \\mathbf{R}_+, ~~~ x \\mapsto \\mu(x_i)\\] è¿™é‡Œï¼Œ\\(\\lambda(x_i)\\)æ˜¯å¯¹\\(N\\)çš„æœŸæœ›çš„ä¼°è®¡ï¼Œ\\(\\mu(x_i)\\)æ˜¯å¯¹\\(Y\\)çš„æœŸæœ›çš„ä¼°è®¡ã€‚åŸºäºè¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œçº¯ä¿è´¹ä¼°è®¡ä¸º\\(\\lambda(x_i)\\mu(x_i)\\)ã€‚ 2.2 é¢„æµ‹æ¨¡å‹æ¦‚è¿° å¦‚ä½•å¾—åˆ°ä¸€ä¸ªå¥½çš„é¢„æµ‹æ¨¡å‹å‘¢ï¼Ÿå¯ä»¥ä»ä¸¤ä¸ªæ–¹é¢è€ƒè™‘ï¼š è®©é£é™©ä¿¡æ¯ç©ºé—´\\(\\mathcal{X}\\)ä¸°å¯Œï¼Œä¹Ÿç§°ä¸ºç‰¹å¾å·¥ç¨‹ï¼Œæ¯”å¦‚åŒ…å«\\(x,x^2,\\ln x\\)ã€æˆ–è€…åŠ å…¥è½¦è”ç½‘ä¿¡æ¯ã€‚ è®©æ˜ å°„ç©ºé—´\\(\\lambda\\in{\\Lambda},\\mu\\in M\\)ä¸°å¯Œï¼Œå¦‚GLMåªåŒ…å«çº¿æ€§æ•ˆåº”ã€ç›¸åŠ æ•ˆåº”ï¼Œæ˜ å°„ç©ºé—´è¾ƒå°ï¼Œç¥ç»ç½‘ç»œåŒ…å«éçº¿æ€§æ•ˆåº”ã€äº¤äº’ä½œç”¨ï¼Œæ˜ å°„ç©ºé—´è¾ƒå¤§ã€‚ å½“ä½ é€‰å–äº†æ˜ å°„ç©ºé—´è¾ƒå°çš„GLMï¼Œé€šå¸¸éœ€è¦è¿›è¡Œä»”ç»†çš„ç‰¹å¾å·¥ç¨‹ï¼Œä½¿å¾—é£é™©ä¿¡æ¯ç©ºé—´é€‚äºGLMï¼›å½“ä½ é€‰å–äº†æ˜ å°„ç©ºé—´è¾ƒå¤§çš„ç¥ç»ç½‘ç»œï¼Œé€šå¸¸ä¸éœ€è¦è¿›è¡Œç‰¹åˆ«ä»”ç»†çš„ç‰¹å¾å·¥ç¨‹ï¼Œç¥ç»ç½‘ç»œå¯ä»¥è‡ªåŠ¨è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Œå‘æ˜é£é™©ä¿¡æ¯ä¸­çš„æœ‰ç”¨ç‰¹å¾ã€‚ å¯¹äºä¼ ç»Ÿçš„ç»Ÿè®¡å›å½’æ¨¡å‹ï¼ŒGLMï¼ŒGAMï¼ŒMARSï¼Œæˆ‘ä»¬ä½¿ç”¨æå¤§ä¼¼ç„¶æ–¹æ³•åœ¨æ˜ å°„ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä¼˜çš„å›å½’æ–¹ç¨‹ï¼Œåœ¨æå¤§ä¼¼ç„¶ä¸­ä½¿ç”¨çš„æ•°æ®é›†ç§°ä¸ºå­¦ä¹ é›†ï¼ˆlearning data setï¼‰ã€‚ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œåå˜é‡é€‰æ‹©ï¼Œå¯ä»¥åˆ æ‰ä¸æ˜¾è‘—çš„åå˜é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é€æ­¥å›å½’ã€æœ€ä¼˜å­é›†ã€LASSOç­‰ï¼Œåˆ¤æ–­æ ‡å‡†ä¸ºAICç­‰ã€‚ å¯¹äºæ ‘æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ recursive partitioning by binary splits ç®—æ³•å¯¹é£é™©ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œä½¿å¾—å„å­ç©ºé—´å†…çš„åº”å˜é‡å·®å¼‚æœ€å°ï¼Œå·®å¼‚é€šå¸¸ä½¿ç”¨åå·®æŸå¤±ï¼ˆdeviance lossï¼‰åº¦é‡ã€‚ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé€šå¸¸ä½¿ç”¨äº¤å‰éªŒè¯å¯¹æ ‘çš„æ·±åº¦è¿›è¡Œæ§åˆ¶ã€‚æ ‘æ¨¡å‹è®­ç»ƒä½¿ç”¨çš„æ•°æ®ä¸ºå­¦ä¹ é›†ã€‚ æ ‘æ¨¡å‹çš„æ‰©å±•ä¸ºbootstrap aggregationï¼ˆbaggingï¼‰å’Œrandom forestã€‚ç¬¬ä¸€ç§ç®—æ³•æ˜¯å¯¹æ¯ä¸ªbootstrapæ ·æœ¬å»ºç«‹æ ‘æ¨¡å‹ï¼Œç„¶åå¹³å‡æ¯ä¸ªæ ‘æ¨¡å‹çš„é¢„æµ‹ï¼›ç¬¬äºŒç§ç®—æ³•ç±»ä¼¼ç¬¬ä¸€ç§ï¼Œä½†åœ¨å»ºç«‹æ ‘æ¨¡å‹æ—¶ï¼Œè¦æ±‚åªåœ¨æŸäº›éšæœºé€‰å®šçš„åå˜é‡ä¸Šåˆ†æ”¯ã€‚è¿™ä¸¤ç§æ‰©å±•éƒ½å±äºé›†æˆå­¦ä¹ ï¼ˆensemble learningï¼‰ã€‚ æå‡ç®—æ³•æœ‰å¤šç§ä¸åŒå½¢å¼ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³ç±»ä¼¼é€æ­¥å›å½’ï¼ŒåŒºåˆ«æ˜¯æ¯æ­¥å›å½’ä¸­éœ€è¦ä¾æ®ä¸Šæ­¥çš„é¢„æµ‹ç»“æœè°ƒæ•´å„ä¸ªæ ·æœ¬çš„æƒé‡ï¼Œè®©ä¸Šæ­¥é¢„æµ‹ç»“æœå·®çš„æ ·æœ¬åœ¨ä¸‹æ­¥å›å½’ä¸­å çš„æƒé‡è¾ƒå¤§ã€‚é€šå¸¸ï¼Œæ¯æ­¥å›å½’ä½¿ç”¨çš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œå¦‚æ·±åº¦ä¸º3çš„æ ‘æ¨¡å‹ã€‚æå‡ç®—æ³•ä¹Ÿå±äºé›†æˆå­¦ä¹ ï¼Œå’Œå‰é¢ä¸åŒæ˜¯å®ƒçš„å¼±å­¦ä¹ å™¨ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œè€Œbaggingå’Œrandom forestçš„å¼±å­¦ä¹ å™¨æ˜¯å½¼æ­¤ç‹¬ç«‹çš„ã€‚ å¯¹äºé›†æˆç®—æ³•ï¼Œé€šå¸¸éœ€è¦è°ƒæ•´å¼±å­¦ä¹ å™¨çš„ç»“æ„å‚æ•°ï¼Œå¦‚æ ‘çš„æ·±åº¦ï¼Œä¹Ÿè¦åˆ¤æ–­å¼±å­¦ä¹ å™¨çš„ä¸ªæ•°ï¼Œè¿™äº›ç§°ä¸ºtuning parametersï¼Œé€šå¸¸é€šè¿‡æ¯”è¾ƒåœ¨éªŒè¯é›†ï¼ˆvalidationï¼‰çš„æŸå¤±è¿›è¡Œè°ƒå‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚å¼±å­¦ä¹ å™¨ä¸­çš„å‚æ•°é€šè¿‡åœ¨è®­ç»ƒé›†ï¼ˆtrainingï¼‰ä¸Šè®­ç»ƒæ¨¡å‹å¾—åˆ°ã€‚è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„å¹¶é›†ä¸ºå­¦ä¹ é›†ã€‚ å‰é¦ˆç¥ç»ç½‘ç»œçš„è¾“å…¥ç¥ç»å…ƒä¸ºé£é™©ä¿¡æ¯ï¼Œä¸‹ä¸€å±‚ç¥ç»å…ƒä¸ºä¸Šä¸€å±‚ç¥ç»å…ƒçš„çº¿æ€§ç»„åˆå¹¶é€šè¿‡æ¿€æ´»å‡½æ•°çš„éçº¿æ€§å˜æ¢ï¼Œæœ€åè¾“å‡ºç¥ç»å…ƒä¸ºç¥ç»ç½‘ç»œå¯¹å› å˜é‡æœŸæœ›çš„é¢„æµ‹ï¼Œé€šè¿‡å‡å°è¾“å‡ºç¥ç»å…ƒä¸å› å˜é‡è§‚å¯Ÿå€¼çš„å·®å¼‚ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œä¸­çš„å‚æ•°ã€‚ç¥ç»ç½‘ç»œå«æœ‰éå¸¸å¤šçš„å‚æ•°ï¼Œå¾ˆéš¾æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ï¼Œè€Œä¸”æœ€ä¼˜è§£å¿…ç„¶é€ æˆè¿‡æ‹Ÿåˆï¼Œæ‰€ä»¥ä¸€èˆ¬é‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•å¯¹å‚æ•°è¿›è¡Œè¿­ä»£ï¼Œä½¿å¾—è®­ç»ƒé›†æŸå¤±åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½æœ‰ä¸‹é™è¶‹åŠ¿ã€‚é€šè¿‡æ¯”è¾ƒéªŒè¯é›†æŸå¤±ç¡®å®šè¿­ä»£æ¬¡æ•°å’Œç¥ç»ç½‘ç»œçš„ç»“æ„å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚ å¦‚ä½•è¯„ä»·ä¸€ä¸ªé¢„æµ‹æ¨¡å‹çš„å¥½åå‘¢ï¼Ÿé€šå¸¸ç”¨æ ·æœ¬å¤–æŸå¤±ï¼ˆtest errorï¼‰è¯„ä»·ã€‚å¯¹äºç´¢èµ”é¢‘ç‡ï¼Œä½¿ç”¨æ³Šæ¾åå·®æŸå¤±ï¼Œå¯¹äºç´¢èµ”å¼ºåº¦ï¼Œä½¿ç”¨ä¼½é©¬åå·®æŸå¤±ï¼Œå¯ä»¥è¯æ˜è¿™ä¸¤ä¸ªæŸå¤±å‡½æ•°å’Œä¼¼ç„¶å‡½æ•°æˆè´Ÿç›¸å…³ã€‚å…¶ä¸­ï¼Œå¹³å‡æ³Šæ¾åå·®æŸå¤±ä¸ºï¼š \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{2}{|\\mathbf{N}|}\\sum_{i}N_i\\left[\\frac{\\hat{N}_i}{N_i}-1-\\ln\\left(\\frac{\\hat{N}_i}{N_i}\\right)\\right]\\] Kerasä¸­å®šä¹‰çš„æŸå¤±å‡½æ•°ä¸º \\[\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{1}{|\\mathbf{N}|}\\sum_{i}\\left[\\hat{N}_i-N_i\\ln\\left(\\hat{N}_i\\right)\\right]\\] 2.3 ç‰¹å¾å·¥ç¨‹ åŠ è½½åŒ…ã€‚ rm(list=ls()) library(CASdatasets) # data # library(keras) # neural network library(data.table) # fread,fwrite library(glmnet) # lasso library(plyr) # ddply library(mgcv) # gam library(rpart) # tree # library(rpart.plot) library(Hmisc) # error bar # devtools::install_github(&#39;henckr/distRforest&#39;) # library(distRforest) library(gbm) # boosting data(freMTPL2freq) #data(freMTPL2sev) # textwidth&lt;-7.3 #inch # fwrite(freMTPL2freq,&quot;data/freMTPL2freq.txt&quot;) # freMTPL2freq&lt;-fread(&quot;data/freMTPL2freq_mac.txt&quot;) &#39;data.frame&#39;: 678013 obs. of 12 variables: $ IDpol : num 1 3 5 10 11 13 15 17 18 21 ... $ ClaimNb : &#39;table&#39; num [1:678013(1d)] 1 1 1 1 1 1 1 1 1 1 ... $ Exposure : num 0.1 0.77 0.75 0.09 0.84 0.52 0.45 0.27 0.71 0.15 ... $ VehPower : int 5 5 6 7 7 6 6 7 7 7 ... $ VehAge : int 0 0 2 0 0 2 2 0 0 0 ... $ DrivAge : int 55 55 52 46 46 38 38 33 33 41 ... $ BonusMalus: int 50 50 50 50 50 50 50 68 68 50 ... $ VehBrand : Factor w/ 11 levels &quot;B1&quot;,&quot;B10&quot;,&quot;B11&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ VehGas : chr &quot;Regular&quot; &quot;Regular&quot; &quot;Diesel&quot; &quot;Diesel&quot; ... $ Area : Factor w/ 6 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 4 4 2 2 2 5 5 3 3 2 ... $ Density : int 1217 1217 54 76 76 3003 3003 137 137 60 ... $ Region : Factor w/ 21 levels &quot;Alsace&quot;,&quot;Aquitaine&quot;,..: 21 21 18 2 2 16 16 13 13 17 ... 2.3.1 æˆªæ–­ å‡å°‘outliers/influential points çš„å½±å“ éœ€æ ¹æ®æ¯ä¸ªå˜é‡çš„åˆ†å¸ƒç¡®å®šåœ¨å“ªé‡Œæˆªæ–­ ç´¢èµ”æ¬¡æ•°åœ¨4æˆªæ–­ é£é™©æš´éœ²åœ¨1æˆªæ–­ é©¬åŠ›åœ¨9æˆªæ–­ è½¦é¾„åœ¨20æˆªæ–­ å¹´é¾„åœ¨90æˆªæ–­ å¥–æƒ©ç³»æ•°åœ¨150æˆªæ–­ 2.3.2 ç¦»æ•£åŒ– ç›®çš„æ˜¯ä¸ºäº†åˆ»ç”»éçº¿æ€§æ•ˆåº” éœ€ç”»å‡ºåå˜é‡çš„è¾¹ç¼˜ç»éªŒç´¢èµ”é¢‘ç‡åˆ¤æ–­ ç¦»æ•£åŒ–é©¬åŠ›ã€è½¦é¾„ã€å¹´é¾„ VehPowerFac, VehAgeFacï¼ŒDrivAgeFac 2.3.3 è®¾å®šåŸºç¡€æ°´å¹³ æ–¹ä¾¿å‡è®¾æ£€éªŒ è®¾å®šå«æœ‰æœ€å¤šé£é™©æš´éœ²çš„æ°´å¹³ä¸ºåŸºå‡†æ°´å¹³ 2.3.4 åå˜é‡å˜å½¢ ç›®çš„æ˜¯ä¸ºäº†åˆ»ç”»éçº¿æ€§æ•ˆåº” è€ƒè™‘åå˜é‡åˆ†å¸ƒï¼Œä½¿ä¹‹å˜å½¢åè¿‘ä¼¼æœä»å¯¹ç§°åˆ†å¸ƒ DriveAgeLn/2/3/4, DensityLn dat1 &lt;- freMTPL2freq # claim number dat1$ClaimNb &lt;- pmin(dat1$ClaimNb, 4) # exposure dat1$Exposure &lt;- pmin(dat1$Exposure, 1) # vehicle power dat1$VehPowerFac &lt;- as.factor(pmin(dat1$VehPower,9)) aggregate(dat1$Exposure,by=list(dat1$VehPowerFac),sum) dat1[,&quot;VehPowerFac&quot;] &lt;-relevel(dat1[,&quot;VehPowerFac&quot;], ref=&quot;6&quot;) # vehicle age dat1$VehAge &lt;- pmin(dat1$VehAge,20) VehAgeFac &lt;- cbind(c(0:110), c(1, rep(2,5), rep(3,5),rep(4,5), rep(5,5), rep(6,111-21))) dat1$VehAgeFac &lt;- as.factor(VehAgeFac[dat1$VehAge+1,2]) aggregate(dat1$Exposure,by=list(dat1$VehAgeFac),sum) dat1[,&quot;VehAgeFac&quot;] &lt;-relevel(dat1[,&quot;VehAgeFac&quot;], ref=&quot;2&quot;) # driver age dat1$DrivAge &lt;- pmin(dat1$DrivAge,90) DrivAgeFac &lt;- cbind(c(18:100), c(rep(1,21-18), rep(2,26-21), rep(3,31-26), rep(4,41-31), rep(5,51-41), rep(6,71-51), rep(7,101-71))) dat1$DrivAgeFac &lt;- as.factor(DrivAgeFac[dat1$DrivAge-17,2]) aggregate(dat1$Exposure,by=list(dat1$DrivAgeFac),sum) dat1[,&quot;DrivAgeFac&quot;] &lt;-relevel(dat1[,&quot;DrivAgeFac&quot;], ref=&quot;6&quot;) dat1$DrivAgeLn&lt;-log(dat1$DrivAge) dat1$DrivAge2&lt;-dat1$DrivAge^2 dat1$DrivAge3&lt;-dat1$DrivAge^3 dat1$DrivAge4&lt;-dat1$DrivAge^4 # bm dat1$BonusMalus &lt;- as.integer(pmin(dat1$BonusMalus, 150)) # vehicle brand dat1$VehBrand &lt;- factor(dat1$VehBrand) # consider VehGas as categorical aggregate(dat1$Exposure,by=list(dat1$VehBrand),sum) dat1[,&quot;VehBrand&quot;] &lt;-relevel(dat1[,&quot;VehBrand&quot;], ref=&quot;B1&quot;) # vehicle gas dat1$VehGas &lt;- factor(dat1$VehGas) # consider VehGas as categorical aggregate(dat1$Exposure,by=list(dat1$VehGas),sum) dat1[,&quot;VehGas&quot;] &lt;-relevel(dat1[,&quot;VehGas&quot;], ref=&quot;Regular&quot;) # area (related to density) dat1$Area &lt;- as.integer(dat1$Area) # density dat1$DensityLn &lt;- as.numeric(log(dat1$Density)) # region aggregate(dat1$Exposure,by=list(dat1$Region),sum)[order( aggregate(dat1$Exposure,by=list(dat1$Region),sum)$x),] dat1[,&quot;Region&quot;] &lt;-relevel(dat1[,&quot;Region&quot;], ref=&quot;Centre&quot;) str(dat1) # model matrix for GLM design_matrix&lt;- model.matrix( ~ ClaimNb + Exposure + VehPowerFac + VehAgeFac + DrivAge + DrivAgeLn + DrivAge2 + DrivAge3 + DrivAge4 + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data=dat1)[,-1] # VehPower, VehAge as factor variables # design_matrix2&lt;- # model.matrix( ~ ClaimNb + Exposure + VehPower + VehAge + DrivAge + # BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data=dat1)[,-1] # VehPower, VehAge, and DrivAge as continuous variables # dim(design_matrix2) 2.4 è®­ç»ƒé›†-éªŒè¯é›†-æµ‹è¯•é›† æ¯”ä¾‹ä¸º\\(0.6:0.2:0.2\\) æ ¹æ®ç´¢èµ”æ¬¡æ•°åˆ†å±‚æŠ½æ · ç»éªŒç´¢èµ”é¢‘ç‡çº¦ä¸º\\(10\\%\\) seed_split&lt;-11 # claim 0/1 proportions index_zero&lt;-which(dat1$ClaimNb==0) index_one&lt;-which(dat1$ClaimNb&gt;0) prop_zero&lt;-round(length(index_zero)/(length(index_one)+length(index_zero)),2) prop_zero prop_one&lt;-round(length(index_one)/(length(index_one)+length(index_zero)),2) prop_one # 0.6:0.2:0.2 size_valid&lt;-round(nrow(dat1)*0.2,0) size_test&lt;-size_valid size_train&lt;-nrow(dat1)-2*size_valid # stratified sampling set.seed(seed_split) index_train_0&lt;-sample(index_zero,size_train*prop_zero) index_train_1&lt;-sample(index_one, size_train-length(index_train_0)) index_train&lt;-union(index_train_0,index_train_1) length(index_train);size_train index_valid&lt;-c(sample(setdiff(index_zero,index_train_0),round(size_valid*prop_zero,0)), sample(setdiff(index_one,index_train_1),size_valid-round(size_valid*prop_zero,0))) length(index_valid);size_valid index_test&lt;-setdiff(union(index_zero,index_one),union(index_train,index_valid)) index_learn&lt;-union(index_train,index_valid) length(index_train);length(index_valid);length(index_test) # train-validation-test; learn-test dat1_train&lt;-dat1[index_train,] dat1_valid&lt;-dat1[index_valid,] dat1_test&lt;-dat1[index_test,] dat1_learn&lt;-dat1[index_learn,] sum(dat1_train$ClaimNb)/sum(dat1_train$Exposure) sum(dat1_valid$ClaimNb)/sum(dat1_valid$Exposure) sum(dat1_test$ClaimNb)/sum(dat1_test$Exposure) sum(dat1_learn$ClaimNb)/sum(dat1_learn$Exposure) # glm matrix matrix_train&lt;-design_matrix[index_train,] matrix_valid&lt;-design_matrix[index_valid,] matrix_test&lt;-design_matrix[index_test,] matrix_learn&lt;-design_matrix[index_learn,] # gbm matrix (learn) dat1_learn_gbm&lt;-dat1_learn[,c(&quot;ClaimNb&quot;, &quot;Exposure&quot;, &quot;VehPower&quot;, &quot;VehAge&quot;, &quot;DrivAge&quot;, &quot;BonusMalus&quot;, &quot;VehBrand&quot;, &quot;VehGas&quot;, &quot;Area&quot;, &quot;DensityLn&quot;, &quot;Region&quot;)] class(dat1_learn_gbm) train_pro&lt;-size_train/(size_train+size_valid) 2.5 æ³Šæ¾åå·®æŸå¤±å‡½æ•° å¹³å‡æ³Šæ¾åå·®æŸå¤± \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{2}{|\\mathbf{N}|}\\sum_{i}N_i\\left[\\frac{\\hat{N}_i}{N_i}-1-\\ln\\left(\\frac{\\hat{N}_i}{N_i}\\right)\\right]\\] Keraså®šä¹‰å¹³å‡æ³Šæ¾åå·®æŸå¤±ä¸º \\[\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{1}{|\\mathbf{N}|}\\sum_{i}\\left[\\hat{N}_i-N_i\\ln\\left(\\hat{N}_i\\right)\\right]\\] å› ä¸ºå¯¹äºå¤§éƒ¨åˆ†ä¿å•ï¼Œ\\(N_i-N_i\\ln N_i\\approx0\\)ï¼Œæ‰€ä»¥æ³Šæ¾åå·®æŸå¤±å‡½æ•°çº¦ä¸ºKeraså®šä¹‰çš„2å€ï¼ˆè‡³å°‘åœ¨ä¸€ä¸ªé‡çº§ï¼‰ã€‚ \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})\\approx2\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})\\] Poisson.Deviance &lt;- function(pred,obs) {200*(sum(pred)-sum(obs)+sum(log((obs/pred)^(obs))))/length(pred)} keras_poisson_dev&lt;-function(y_hat,y_true) {100*sum(y_hat-y_true*log(y_hat))/length(y_true)} f_keras&lt;-function(x) 100*(x-x*log(x)) f_keras(0.1);f_keras(0.2) # png(&quot;./plots/1/poi_dev.png&quot;) plot(seq(0.05,0.15,0.01),f_keras(seq(0.05,0.15,0.01)),type=&quot;l&quot;, xlab=&quot;frequency&quot;,ylab=&quot;approximated Poisson deviance&quot;,main=&quot;100(freq - freq * ln freq)&quot;) abline(v=0.1,lty=2);abline(h=f_keras((0.1)),lty=2) # dev.off() 2.6 æ³Šæ¾å›å½’æ¨¡å‹ ä½¿ç”¨æå¤§ä¼¼ç„¶æ–¹æ³•åœ¨æ˜ å°„ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä¼˜çš„å›å½’æ–¹ç¨‹ åœ¨æå¤§ä¼¼ç„¶ä¸­ä½¿ç”¨çš„æ•°æ®é›†ç§°ä¸ºå­¦ä¹ é›†ï¼ˆlearning data setï¼‰ã€‚ ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œåå˜é‡é€‰æ‹©ï¼Œå¯ä»¥åˆ æ‰ä¸æ˜¾è‘—çš„åå˜é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é€æ­¥å›å½’ã€æœ€ä¼˜å­é›†ã€LASSOç­‰ï¼Œåˆ¤æ–­æ ‡å‡†ä¸ºAICç­‰ã€‚ åŒè´¨æ¨¡å‹ \\[\\mathbf{E}(N)=\\beta_0\\] å…¨æ¨¡å‹ \\[\\ln \\mathbf{E}(N)=\\ln e + \\beta_0 + \\beta_{\\text{VehPowerFac}} + \\beta_{\\text{VehAgeFac}} \\\\ + \\beta_1\\text{DrivAge} + \\beta_2\\ln\\text{DrivAge} + \\beta_3\\text{DrivAge}^2 + \\beta_4\\text{DrivAge}^3 + \\beta_5\\text{DrivAge}^4 \\\\ \\beta_6\\text{BM} + \\beta_{\\text{VehBrand}} + \\beta_{\\text{VehGas}} + \\beta_7\\text{Area} + \\beta_8\\text{DensityLn} + \\beta_{\\text{Region}}\\] # homogeneous model d.glm0 &lt;- glm(ClaimNb ~ 1 + offset(log (Exposure)), data=data.frame(matrix_learn), family=poisson()) #summary(d.glm0) dat1_test$fitGLM0 &lt;- predict(d.glm0, newdata=data.frame(matrix_test), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM0,matrix_test[,1]) Poisson.Deviance(dat1_test$fitGLM0,matrix_test[,1]) # full GLM names(data.frame(matrix_learn)) {t1 &lt;- proc.time() d.glm1 &lt;- glm(ClaimNb ~ .-Exposure + offset(log(Exposure)), data=data.frame(matrix_learn), family=poisson()) (proc.time()-t1)} # summary(d.glm1) dat1_train$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_train), type=&quot;response&quot;) dat1_valid$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_valid), type=&quot;response&quot;) dat1_test$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_test), type=&quot;response&quot;) dat1_learn$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_learn), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM1,matrix_test[,1]) Poisson.Deviance(dat1_test$fitGLM1,matrix_test[,1]) Step wiseã€LASSOåå˜é‡é€‰æ‹© é€æ­¥å›å½’éå¸¸æ…¢ï¼Œåœ¨Linux 8æ ¸i7 3.4GHz 16Gå†…å­˜éƒ½éœ€è¦50å¤šåˆ†é’Ÿã€‚ä¸”æ ·æœ¬å¤–æŸå¤±å’Œå…¨æ¨¡å‹æ²¡æœ‰æ˜æ˜¾å‡å°ã€‚ 5æŠ˜CV Lassoåœ¨Linux 8æ ¸i7 3.4GHz 16Gå†…å­˜éœ€è¦5åˆ†é’Ÿã€‚ æ ¹æ®5æŠ˜CV-erroré€‰å–Lassoæ­£åˆ™å‚æ•°beta=4*10^-5ã€‚ ä¸¤ç§æ–¹æ³•çš„æ ·æœ¬å¤–æŸå¤±å’Œå…¨æ¨¡å‹æ²¡æœ‰æ˜æ˜¾å‡å°ï¼Œè¯´æ˜æ²¡æœ‰å‘ç”Ÿæ˜æ˜¾è¿‡æ‹Ÿåˆã€‚ä¹Ÿè¯´æ˜éœ€è¦ä»éçº¿æ€§æ•ˆåº”å’Œäº¤äº’é¡¹å‡ºå‘æå‡æ¨¡å‹ã€‚ # step wise selectionï¼› this takes a long time (more than 50 minutes!) # d.glm00 &lt;- glm(ClaimNb ~ VehAgeFac1 + VehAgeFac3 + VehAgeFac4 + VehAgeFac5 + # DrivAge + DrivAge2 + DrivAge3 + DrivAge4 + DrivAgeLn + # BonusMalus + VehBrandB12 + VehGasDiesel + DensityLn + # offset(log (Exposure)), # data=data.frame(matrix_learn), family=poisson()) # {t1 &lt;- proc.time() # d.glm2&lt;-step(d.glm00,direction=&quot;forward&quot;,trace = 1, # scope =list(lower=formula(d.glm00), upper=formula(d.glm1))) # (proc.time()-t1)} d.glm2&lt;-glm(ClaimNb ~ VehAgeFac1 + VehAgeFac3 + VehAgeFac4 + VehAgeFac5 + DrivAge + DrivAge2 + DrivAge3 + DrivAge4 + DrivAgeLn + BonusMalus + VehBrandB12 + VehGasDiesel + DensityLn + VehPowerFac4 + VehPowerFac8 + RegionNord.Pas.de.Calais + VehPowerFac7 + RegionRhone.Alpes + RegionBretagne + RegionAuvergne + RegionLimousin + RegionLanguedoc.Roussillon + RegionIle.de.France + RegionAquitaine + RegionMidi.Pyrenees + RegionPays.de.la.Loire + RegionProvence.Alpes.Cotes.D.Azur + RegionPoitou.Charentes + RegionHaute.Normandie + VehBrandB5 + VehBrandB11 + RegionBasse.Normandie + VehBrandB14 + RegionCorse + offset(log(Exposure)), data=data.frame(matrix_learn), family=poisson()) summary(d.glm2) dat1_test$fitGLM2 &lt;- predict(d.glm2, newdata=data.frame(matrix_test), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM2,data.frame(matrix_test)$ClaimNb) Poisson.Deviance(dat1_test$fitGLM2,matrix_test[,1]) # lasso regressionï¼› this takes a few minutes alpha0=1 # 1 for lasso, 0 for ridge. set.seed(7) # {t1 &lt;- proc.time() # cvfit = cv.glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], # family = &quot;poisson&quot;,offset=log(matrix_learn[,2]), # alpha = alpha0,nfolds = 5,trace.it = 1) # (proc.time()-t1)} # cvfit$lambda.min #4*10^-5 # cvfit$lambda.1se # 0.0016 # plot(cvfit) d.glm3 = glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], family = &quot;poisson&quot;, offset=log(matrix_learn[,2]), alpha=alpha0, lambda=4.024746e-05, trace.it = 1) dat1_test$fitLasso&lt;-predict(d.glm3, newx = matrix_test[,-c(1,2)], newoffset=log(matrix_test[,2]),type = &quot;response&quot;) keras_poisson_dev(dat1_test$fitLasso, matrix_test[,1]) Poisson.Deviance(dat1_test$fitLasso, matrix_test[,1]) 2.7 æ³Šæ¾å¯åŠ æ¨¡å‹ GAMè¾¹ç¼˜æå‡æ¨¡å‹ æ ·æœ¬å¤–æŸå¤±å‡å°‘ï¼Œè¯´æ˜éçº¿æ€§æ•ˆåº”å­˜åœ¨ã€‚ \\[\\ln \\mathbf{E}(N)=\\ln\\hat{\\lambda}^{\\text{GLM}}+s_1(\\text{VehAge})+s_2(\\text{BM})\\] \\(s_1,s_2\\)ä¸ºæ ·æ¡å¹³æ»‘å‡½æ•°ã€‚ ä½¿ç”¨ddplyèšåˆæ•°æ®ï¼Œæ‰¾åˆ°å……åˆ†ç»Ÿè®¡é‡ï¼ŒåŠ å¿«æ¨¡å‹æ‹Ÿåˆé€Ÿåº¦ã€‚ # GAM marginals improvement (VehAge and BonusMalus) {t1 &lt;- proc.time() dat.GAM &lt;- ddply(dat1_learn, .(VehAge, BonusMalus), summarise, fitGLM1=sum(fitGLM1), ClaimNb=sum(ClaimNb)) set.seed(1) d.gam &lt;- gam(ClaimNb ~ s(VehAge, bs=&quot;cr&quot;)+s(BonusMalus, bs=&quot;cr&quot;) + offset(log(fitGLM1)), data=dat.GAM, method=&quot;GCV.Cp&quot;, family=poisson) (proc.time()-t1)} summary(d.gam) dat1_train$fitGAM1 &lt;- predict(d.gam, newdata=dat1_train,type=&quot;response&quot;) dat1_valid$fitGAM1 &lt;- predict(d.gam, newdata=dat1_valid,type=&quot;response&quot;) dat1_test$fitGAM1 &lt;- predict(d.gam, newdata=dat1_test,type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGAM1, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitGAM1,matrix_test[,1]) 2.8 æ³Šæ¾å›å½’æ ‘ ä½¿ç”¨ recursive partitioning by binary splits ç®—æ³•å¯¹é£é™©ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œä½¿å¾—å„å­ç©ºé—´å†…çš„åº”å˜é‡å·®å¼‚æœ€å°ã€‚ ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½¿ç”¨äº¤å‰éªŒè¯ç¡®å®šcost-complexity parameterã€‚ cp=10^-3.421(1-SD rule)å‰ªææˆsplit=22çš„æ ‘ï¼Œæˆ–è€…cp=10^-3.949(min CV rule)å‰ªææˆsplit=55çš„æ ‘ã€‚ split=55(min CV rule)æ ‘çš„æ ·æœ¬å¤–æŸå¤±è¾ƒå°ã€‚ Variable importance (min CV rule) BonusMalus VehAge VehBrand DrivAge VehGas VehPower Region DensityLn 4675.0231 4396.8667 1389.2909 877.9473 795.6308 715.3584 480.3459 140.5463 # cross validation using xval in rpart.control names(dat1_learn) set.seed(1) {t1 &lt;- proc.time() tree0&lt;-rpart(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_learn, method = &quot;poisson&quot;, control = rpart.control (xval=5, minbucket=1000, cp=10^-5, maxcompete = 0, maxsurrogate = 0)) (proc.time()-t1)} x0 &lt;- log10(tree0$cptable[,1]) err0&lt;-tree0$cptable[,4] std0&lt;-tree0$cptable[,5] xmain &lt;- &quot;cross-validation error plot&quot; xlabel &lt;- &quot;cost-complexity parameter (log-scale)&quot; ylabel &lt;- &quot;relative CV error&quot; (cp_min&lt;-x0[which.min(err0)]) (cp_1sd&lt;-x0[min(which(err0&lt;min(err0)+std0[which.min(err0)]))]) (nsplit_min&lt;-tree0$cptable[which.min(err0),2]) (nsplit_1sd&lt;-tree0$cptable[min(which(err0&lt;min(err0)+std0[which.min(err0)])),2]) # png(&quot;./plots/1/tree_cv.png&quot;) errbar(x=x0, y=err0*100, yplus=(err0+std0)*100, yminus=(err0-std0)*100, xlim=rev(range(x0)), col=&quot;blue&quot;, main=xmain, ylab=ylabel, xlab=xlabel) lines(x=x0, y=err0*100, col=&quot;blue&quot;) abline(h=c(min(err0+std0)*100), lty=1, col=&quot;orange&quot;) abline(h=c(min(err0)*100), lty=1, col=&quot;magenta&quot;) abline(v=c(cp_1sd,cp_min),lty=2,col=c(&quot;orange&quot;,&quot;magenta&quot;)) legend(x=&quot;topright&quot;, col=c(&quot;blue&quot;, &quot;orange&quot;, &quot;magenta&quot;,&quot;orange&quot;,&quot;magenta&quot;), lty=c(1,1,1,2,2), lwd=c(1,1,1,1,1), pch=c(19,-1,-1,-1,-1), c(&quot;tree0&quot;, &quot;1-SD rule&quot;, &quot;min.CV rule&quot;, paste(&quot;log cp = &quot;,round(cp_1sd,3)),paste(&quot;log cp = &quot;, round(cp_min,3)))) # dev.off() tree1 &lt;- prune(tree0, cp=10^mean(cp_min,min(x0[x0&gt;cp_min]))) tree11&lt;- prune(tree0, cp=10^mean(cp_1sd,min(x0[x0&gt;cp_1sd]))) tree1$cptable[nrow(tree1$cptable),2];nsplit_min tree11$cptable[nrow(tree11$cptable),2];nsplit_1sd dat1_test$fitRT_min &lt;- predict(tree1, newdata=dat1_test)*dat1_test$Exposure dat1_test$fitRT_sd &lt;- predict(tree11, newdata=dat1_test)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitRT_min, dat1_test$ClaimNb) keras_poisson_dev(dat1_test$fitRT_sd, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT_min, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT_sd, dat1_test$ClaimNb) tree1$variable.importance tree11$variable.importance äº¤å‰éªŒè¯å¯ä½¿ç”¨rpart(..., control=rpart.control(xval= ,...))æˆ–è€…xpred.rpart(tree, group)ã€‚ ä»¥ä¸Šä¸¤ç§æ–¹å¼å¾—åˆ°å¾ˆç›¸è¿‘çš„min CV ruleå‰ªææ ‘55 vs 51ï¼Œä½†1-SD ruleç›¸å·®è¾ƒå¤š22 vs 12ã€‚ # K-fold cross-validation using xpred.rpart set.seed(1) {t1 &lt;- proc.time() tree00&lt;-rpart(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_learn, method = &quot;poisson&quot;, control = rpart.control (xval=1, minbucket=1000 ,cp=10^-5, maxcompete = 0, maxsurrogate = 0)) (proc.time()-t1)} (n_subtrees &lt;- dim(tree00$cptable)[1]) std1&lt;- numeric(n_subtrees) err1 &lt;- numeric(n_subtrees) K &lt;- 5 xgroup &lt;- rep(1:K, length = nrow(dat1_learn)) xfit &lt;- xpred.rpart(tree00, xgroup) dim(xfit);dim(dat1_learn) for (i in 1:n_subtrees){ err_group&lt;-rep(NA,K) for (k in 1:K){ ind_group &lt;- which(xgroup ==k) err_group[k] &lt;- keras_poisson_dev(dat1_learn[ind_group,&quot;Exposure&quot;]*xfit[ind_group,i], dat1_learn[ind_group,&quot;ClaimNb&quot;]) } err1[i] &lt;- mean(err_group) std1[i] &lt;- sd(err_group) } x1 &lt;- log10(tree00$cptable[,1]) (cp_min1&lt;-x1[which.min(err1)]) (cp_1sd1&lt;-x1[min(which(err1&lt;min(err1)+std1[which.min(err1)]))]) (nsplit_min1&lt;-tree00$cptable[which.min(err1),2]) (nsplit_1sd1&lt;-tree00$cptable[min(which(err1&lt;min(err1)+std1[which.min(err1)])),2]) xmain &lt;- &quot;cross-validation error plot&quot; xlabel &lt;- &quot;cost-complexity parameter (log-scale)&quot; ylabel &lt;- &quot;CV error (in 10^(-2))&quot; errbar(x=x1, y=err1*100, yplus=(err1+std1)*100, yminus=(err1-std1)*100, xlim=rev(range(x1)), col=&quot;blue&quot;, main=xmain, ylab=ylabel, xlab=xlabel) lines(x=x1, y=err1*100, col=&quot;blue&quot;) abline(h=c(min(err1+std1)*100), lty=1, col=&quot;orange&quot;) abline(h=c(min(err1)*100), lty=1, col=&quot;magenta&quot;) abline(v=c(cp_1sd1,cp_min1),lty=2,col=c(&quot;orange&quot;,&quot;magenta&quot;)) legend(x=&quot;topright&quot;, col=c(&quot;blue&quot;, &quot;orange&quot;, &quot;magenta&quot;,&quot;orange&quot;,&quot;magenta&quot;), lty=c(1,1,1,2,2), lwd=c(1,1,1,1,1), pch=c(19,-1,-1,-1,-1), c(&quot;tree0&quot;, &quot;1-SD rule&quot;, &quot;min.CV rule&quot;, paste(&quot;log cp = &quot;,round(cp_1sd1,3)),paste(&quot;log cp = &quot;, round(cp_min1,3)))) tree2 &lt;- prune(tree00, cp=10^mean(cp_min1,min(x1[x1&gt;cp_min1]))) tree22 &lt;- prune(tree00, cp=10^mean(cp_1sd1,min(x1[x1&gt;cp_1sd1]))) printcp(tree2) printcp(tree22) dat1_test$fitRT2 &lt;- predict(tree2, newdata=dat1_test)*dat1_test$Exposure dat1_test$fitRT22 &lt;- predict(tree22, newdata=dat1_test)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitRT2, dat1_test$ClaimNb) keras_poisson_dev(dat1_test$fitRT22, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT2, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT22, dat1_test$ClaimNb) sum((dat1_test$fitRT2-dat1_test$fitRT_min)^2) sum((dat1_test$fitRT22-dat1_test$fitRT_sd)^2) tree2$variable.importance tree1$variable.importance tree22$variable.importance tree11$variable.importance 2.9 éšæœºæ£®æ— ä½¿ç”¨https://github.com/henckr/distRforestå»ºç«‹æ³Šæ¾éšæœºæ£®æ—ã€‚ ncandæ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„åå˜é‡ä¸ªæ•°ï¼›subsampleè®­ç»ƒæ¯æ£µæ ‘çš„æ ·æœ¬ã€‚ ä½¿ç”¨éªŒè¯æŸå¤±ç¡®å®šæ ‘çš„æ•°é‡ã€‚ # fit the random forest library(distRforest) ntrees0&lt;-200 set.seed(1) {t1 &lt;- proc.time() forest1&lt;-rforest(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_train, method = &quot;poisson&quot;, control = rpart.control (xval=0, minbucket=1000 ,cp=10^-4, maxcompete = 0,maxsurrogate = 0, seed=1), parms=list(shrink=1), ncand=5,ntrees = ntrees0, subsample = 0.5, red_mem = T) (proc.time()-t1)} # determine number of trees using validation error fit_valid&lt;-rep(0,nrow(dat1_valid)) error_valid&lt;-rep(0,ntrees0) for (i in 1:ntrees0){ fit_valid&lt;-fit_valid + predict(forest1$trees[[i]], newdata=dat1_valid) * dat1_valid$Exposure fit_valid_norm &lt;- fit_valid/i error_valid[i]&lt;-Poisson.Deviance(fit_valid_norm, dat1_valid$ClaimNb) } # png(&quot;./plots/1/random_forest_error.png&quot;) plot(error_valid,type=&quot;l&quot;,xlab=&quot;number of trees&quot;,ylab=&quot;validation error in 10^-2&quot;) abline(v=which.min(error_valid),lty=2) # dev.off() (best.trees=which.min(error_valid)) # test error fitRF&lt;-rep(0,nrow(dat1_test)) for (i in 1:best.trees){ fitRF&lt;-fitRF+predict(forest1$trees[[i]], newdata=dat1_test)*dat1_test$Exposure } dat1_test$fitRF &lt;- fitRF/best.trees keras_poisson_dev(dat1_test$fitRF, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRF, dat1_test$ClaimNb) names(forest1$trees[[2]]$variable.importance) sum(forest1$trees[[3]]$variable.importance) 2.10 æ³Šæ¾æå‡æ ‘ n.trees æ ‘çš„æ•°é‡ï¼›shrinkage å­¦ä¹ æ­¥é•¿ï¼Œå’Œæ ‘çš„æ•°é‡æˆåæ¯”ï¼›interaction.depth äº¤äº’é¡¹æ·±åº¦ï¼›bag.fraction æ¯æ£µæ ‘ä½¿ç”¨çš„æ•°æ®æ¯”ä¾‹ï¼›train.fraction è®­ç»ƒé›†æ¯”ä¾‹ï¼›n.minobsinnodeå¶å­ä¸Šæœ€å°‘æ ·æœ¬é‡ã€‚ set.seed(1) {t1 &lt;- proc.time() gbm1 &lt;- gbm( ClaimNb ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region + offset(log(Exposure)), data = dat1_learn_gbm, distribution = &quot;poisson&quot;, n.trees = 500, shrinkage = 0.1, interaction.depth = 5, bag.fraction = 0.5, train.fraction = train_pro, cv.folds = 0, n.minobsinnode = 1000, verbose = T ) (proc.time()-t1)} # plot the performance # png(&quot;./plots/1/gbm_error.png&quot;) gbm.perf(gbm1,method=&quot;test&quot;) legend(&quot;topright&quot;,lty=c(1,1,2),col=c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;), c(&quot;training error&quot;, &quot;validation error&quot;, &quot;best iterations&quot;)) # dev.off() best.iter&lt;-gbm.perf(gbm1,method=&quot;test&quot;) dat1_test$fitGBM1&lt;- predict(gbm1, dat1_test,n.trees=best.iter,type=&quot;response&quot;)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitGBM1,dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitGBM1,dat1_test$ClaimNb) æ ¹æ®éªŒè¯é›†æŸå¤±ç¡®å®šè¿­ä»£æ¬¡æ•°ã€‚ Variable importance rel.inf BonusMalus 27.687137 VehAge 19.976441 VehBrand 13.515198 Region 13.495375 DrivAge 9.284520 VehGas 7.082648 VehPower 4.583522 DensityLn 4.375159 Area 0.000000 é‡è¦å˜é‡çš„è¾¹ç¼˜æ•ˆåº” é‡è¦å˜é‡çš„äº¤äº’æ•ˆåº” 2.11 æ¨¡å‹æ¯”è¾ƒ ## model test_error test_error_keras ## 1 Intercept 33.5695 21.7647 ## 2 GLM 31.7731 20.8665 ## 3 GLM Lasso 31.8132 20.8866 ## 4 GAM 31.6651 20.8125 ## 5 Decision tree 30.9780 20.4690 ## 6 Random forest 30.9652 20.4626 ## 7 Generalized boosted model 30.8972 20.4286 ## 8 Neural network 31.0607 20.5080 Boosting &gt; RF &gt; Tree &gt; GAM &gt; GLM &gt; Homo "],["nn.html", "3 ç¥ç»ç½‘ç»œ 3.1 å»ºç«‹ç¥ç»ç½‘ç»œçš„ä¸€èˆ¬æ­¥éª¤ 3.2 æ•°æ®é¢„å¤„ç† 3.3 ç¥ç»ç½‘ç»œæå‡æ¨¡å‹ ï¼ˆcombined actuarial neural networkï¼‰ 3.4 ç¥ç»ç½‘ç»œç»“æ„ 3.5 è®­ç»ƒç¥ç»ç½‘ç»œ 3.6 æ€»ç»“ 3.7 å…¶å®ƒæ¨¡å‹", " 3 ç¥ç»ç½‘ç»œ æ­£å¦‚è®¡ç®—æœºé€Ÿåº¦çš„æå‡å’ŒMCMCæ–¹æ³•ç»™è´å¶æ–¯ç»Ÿè®¡å¸¦æ¥äº†ç”Ÿæœºï¼Œè®¡ç®—æœºè¿ç®—èƒ½åŠ›çš„æå‡å’Œåå‘ä¼ æ’­ç®—æ³•ï¼ˆback propagationï¼‰ä¹Ÿç»™ç¥ç»ç½‘ç»œå¸¦æ¥äº†é£é€Ÿå‘å±•ã€‚ Tensorflowå’ŒPytorchçš„æ›´æ–°é€Ÿåº¦åæ˜ äº†è¿™ä¸ªé¢†åŸŸçš„çƒ­åº¦ã€‚ 3.1 å»ºç«‹ç¥ç»ç½‘ç»œçš„ä¸€èˆ¬æ­¥éª¤ 3.1.1 æ˜ç¡®ç›®æ ‡å’Œæ•°æ®ç±»å‹ ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§éå‚éçº¿æ€§å›å½’æ¨¡å‹ï¼Œå®ƒå¯ä»¥åˆ»ç”»éçº¿æ€§æ•ˆåº”å’Œäº¤äº’æ•ˆåº”ã€‚ åœ¨ä½¿ç”¨ç¥ç»ç½‘ç»œå‰ï¼Œéœ€è¦ç†è§£ç ”ç©¶ç›®æ ‡å’Œæ•°æ®ç»“æ„ï¼Œæœ‰ä¸€äº›ç‰¹æ®Šçš„layerï¼Œå¦‚å·ç§¯å±‚ï¼Œä¸“é—¨ä¸ºæŸç§ä»»åŠ¡ã€æˆ–æŸç§æ•°æ®ç»“æ„è€Œè®¾ç«‹ï¼Œä¸æ˜¯å¯ä»¥ç”¨åœ¨ä»»ä½•çš„æ•°æ®ä¸Šã€‚ ç¥ç»ç½‘ç»œæœ‰å¤§é‡çš„å‚æ•°ï¼Œå…¨å±€æœ€ä¼˜è§£å¿…ç„¶ä¼šé€ æˆè¿‡æ‹Ÿåˆï¼Œé€šå¸¸åˆ©ç”¨éªŒè¯é›†æŸå¤±æ¥åˆ¤æ–­æ¢¯åº¦ä¸‹é™çš„æ¬¡æ•°ã€‚ 3.1.2 æ•°æ®é¢„å¤„ç† æè¿°æ€§ç»Ÿè®¡åˆ†æ ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼å¤„ç† è¿ç»­å‹å˜é‡æ ‡å‡†åŒ–ï¼Œä¸»è¦æ˜¯ä¸ºäº†è®©æ¢¯åº¦ä¸‹é™æ³•æ›´æœ‰æ•ˆåœ°å·¥ä½œã€‚å¸¸ç”¨çš„æ ‡å‡†åŒ–æ–¹æ³•æœ‰MinMaxScaler \\[x^*=2\\frac{x-\\min x}{\\max x - \\min x}-1\\] åˆ†ç±»å˜é‡ï¼Œå¯ä»¥ä½¿ç”¨dummy codingã€one-hot encodingï¼Œæˆ–è€…ä½¿ç”¨ç¥ç»ç½‘ç»œä¸­çš„embedding layerã€‚ç¬¬ä¸‰ç§åŠæ³•å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘å‚æ•°ä¸ªæ•°ã€‚ è®­ç»ƒ-éªŒè¯-æµ‹è¯•æ•°æ®åˆ†å‰²ï¼šè®­ç»ƒé›†ç”¨äºæ¢¯åº¦ä¸‹é™æ³•æ±‚è§£å‚æ•°ï¼ŒéªŒè¯é›†ç”¨äºåˆ¤æ–­epochæ¬¡æ•°ã€è°ƒæ•´æ¨¡å‹ç»“æ„çš„è¶…å‚æ•°ï¼Œæµ‹è¯•é›†ç”¨äºæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ ·æœ¬å¤–é¢„æµ‹èƒ½åŠ›ã€‚ 3.1.3 é€‰å–åˆé€‚çš„ç¥ç»ç½‘ç»œç±»å‹ å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼šé€‚ç”¨äºä¸€èˆ¬çš„å›å½’é—®é¢˜ï¼Œå¦‚ç´¢èµ”é¢‘ç‡é¢„æµ‹ã€‚ä¿¡æ¯ä¸€ç›´å‘å‰ä¼ é€’ã€‚å‚æ•°ä¸ªæ•°è¾ƒå¤šã€‚å¯è§£é‡Šæ€§å·®ã€‚ å·ç§¯ç¥ç»ç½‘ç»œï¼šé€‚ç”¨äºæ•°æ®æœ‰ç©ºé—´ç»“æ„ï¼Œä¸”ç›¸åŒçš„æ¨¡å¼å¯èƒ½å‡ºç°åœ¨ä¸åŒçš„ä½ç½®ï¼Œå¦‚å›¾åƒè¯†åˆ«ã€‚ä¿¡æ¯ä¸€ç›´å‘å‰ä¼ é€’ã€‚å‚æ•°ä¸ªæ•°è¾ƒå°‘ã€‚æœ‰å¯è§£é‡Šæ€§ã€‚ é€’å½’ç¥ç»ç½‘ç»œï¼šé€‚ç”¨äºæ•°æ®æœ‰æ—¶é—´åºåˆ—ç‰¹å¾ï¼Œä¸”ç›¸åŒçš„æ¨¡å¼å¯èƒ½å‡ºç°åœ¨ä¸åŒæ—¶é—´ç‚¹ï¼Œå¦‚å¤©æ°”é¢„æŠ¥ã€è¯­éŸ³è¯†åˆ«ã€‚ä¿¡æ¯å¯ä»¥è¿”å›åˆ°å‰é¢çš„ç¥ç»å…ƒã€‚å‚æ•°ä¸ªæ•°è¾ƒå¤šã€‚å¯è§£é‡Šæ€§å·®ã€‚ 3.1.4 å»ºç«‹ç¥ç»ç½‘ç»œï¼ˆå…¨è¿æ¥ç¥ç»ç½‘ç»œï¼‰ åœ¨å»ºç«‹ç¥ç»ç½‘ç»œæ—¶ï¼Œéœ€è¦è€ƒè™‘ä»¥ä¸‹å‡ ç‚¹ï¼š è¾“å…¥å±‚æ•°æ®ç±»å‹ éšè—å±‚å±‚æ•°ï¼Œéšè—å±‚æ€§è´¨ï¼Œç¥ç»å…ƒä¸ªæ•°ï¼Œæ¿€æ´»å‡½æ•°ï¼Œæ­£åˆ™åŒ–ï¼Œdropout è¾“å‡ºç¥ç»å…ƒæ•°æ®ç±»å‹ï¼Œè¾“å‡ºç¥ç»å…ƒæ¿€æ´»å‡½æ•° æŸå¤±å‡½æ•°é€‰æ‹© 3.1.5 è®­ç»ƒç¥ç»ç½‘ç»œ åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œéœ€è¦è€ƒè™‘ä»¥ä¸‹å‡ ç‚¹ æ¢¯åº¦ä¸‹é™æ³•ï¼ˆoptimizerï¼‰ è¿­ä»£æ¬¡æ•°ï¼ˆpatienceï¼‰ éå†æ¬¡æ•°ï¼ˆepochï¼‰ï¼Œæ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰ 3.1.6 è°ƒå‚ è¿”å›ç¬¬4æ­¥ï¼Œè°ƒæ•´æ¨¡å‹ç»“æ„çš„è¶…å‚æ•°ï¼ˆhyper-parameter tuningï¼‰ï¼Œè§‚å¯ŸéªŒè¯æŸå¤±çš„å˜åŒ–ï¼Œé€‰å–æœ€ç»ˆæ¨¡å‹ã€‚ 3.2 æ•°æ®é¢„å¤„ç† &#39;data.frame&#39;: 678013 obs. of 12 variables: $ IDpol : num 1 3 5 10 11 13 15 17 18 21 ... $ ClaimNb : &#39;table&#39; num [1:678013(1d)] 1 1 1 1 1 1 1 1 1 1 ... $ Exposure : num 0.1 0.77 0.75 0.09 0.84 0.52 0.45 0.27 0.71 0.15 ... $ VehPower : int 5 5 6 7 7 6 6 7 7 7 ... $ VehAge : int 0 0 2 0 0 2 2 0 0 0 ... $ DrivAge : int 55 55 52 46 46 38 38 33 33 41 ... $ BonusMalus: int 50 50 50 50 50 50 50 68 68 50 ... $ VehBrand : Factor w/ 11 levels &quot;B1&quot;,&quot;B10&quot;,&quot;B11&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ VehGas : chr &quot;Regular&quot; &quot;Regular&quot; &quot;Diesel&quot; &quot;Diesel&quot; ... $ Area : Factor w/ 6 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 4 4 2 2 2 5 5 3 3 2 ... $ Density : int 1217 1217 54 76 76 3003 3003 137 137 60 ... $ Region : Factor w/ 21 levels &quot;Alsace&quot;,&quot;Aquitaine&quot;,..: 21 21 18 2 2 16 16 13 13 17 ... åœ¨è¿›è¡Œä¸‹é¢codeä¹‹å‰ï¼Œéœ€è¦è¿è¡Œä¸Šä¸€ç« çš„ä»£ç ç›´åˆ°Treeä¹‹å‰ã€‚ è¿ç»­å‹å˜é‡ï¼šæ ‡å‡†åŒ–å¤„ç†ã€‚ PreProcess.Continuous &lt;- function(var1, dat1){ names(dat1)[names(dat1) == var1] &lt;- &quot;V1&quot; dat1$X &lt;- as.numeric(dat1$V1) dat1$X &lt;- 2*(dat1$X-min(dat1$X))/(max(dat1$X)-min(dat1$X))-1 names(dat1)[names(dat1) == &quot;V1&quot;] &lt;- var1 names(dat1)[names(dat1) == &quot;X&quot;] &lt;- paste(var1,&quot;X&quot;, sep=&quot;&quot;) dat1 } Features.PreProcess &lt;- function(dat1){ dat1$VehPower &lt;- pmin(dat1$VehPower,9) dat1 &lt;- PreProcess.Continuous(&quot;VehPower&quot;, dat1) dat1$VehAge &lt;- pmin(dat1$VehAge,20) dat1 &lt;- PreProcess.Continuous(&quot;VehAge&quot;, dat1) dat1$DrivAge &lt;- pmin(dat1$DrivAge,90) dat1 &lt;- PreProcess.Continuous(&quot;DrivAge&quot;, dat1) dat1$BonusMalus &lt;- pmin(dat1$BonusMalus,150) dat1 &lt;- PreProcess.Continuous(&quot;BonusMalus&quot;, dat1) dat1$VehBrandX &lt;- as.integer(dat1$VehBrand)-1 # categorical variable dat1$VehGas &lt;- as.factor(dat1$VehGas) dat1$VehGasX &lt;- as.integer(dat1$VehGas) - 1.5 # binary: continuous or categorical dat1 &lt;- PreProcess.Continuous(&quot;Area&quot;, dat1) dat1 &lt;- PreProcess.Continuous(&quot;Density&quot;, dat1) dat1$RegionX &lt;- as.integer(dat1$Region) - 1 # categorical dat1 } dat2 &lt;- Features.PreProcess(freMTPL2freq) names(dat2) dat2_train&lt;-dat2[index_train,] dat2_valid&lt;-dat2[index_valid,] dat2_test&lt;-dat2[index_test,] dat2_learn&lt;-dat2[index_learn,] åˆ†ç±»å˜é‡ï¼š é‡‡ç”¨embedding layerã€‚ è®­ç»ƒé›†-éªŒè¯é›†-æµ‹è¯•é›†ï¼šåˆ†å±‚æŠ½æ ·ã€‚ è°ƒæ•´æ•°æ®ç»“æ„ï¼Œä½¿å…¶åŒ¹é…ç¥ç»ç½‘ç»œçš„è¾“å…¥å±‚åŠå…¶ç»´åº¦ã€‚è¾“å…¥æ•°æ®é›†åŒ…æ‹¬Xtrain, Brtain, Retrain, Vtrainï¼Œå› å˜é‡æ•°æ®é›†ä¸ºYtrainã€‚ lambda.hom &lt;- sum(dat2_train$ClaimNb)/sum(dat2_train$Exposure);lambda.hom names(dat2) # index of continous variables (non-categorical) features &lt;- c(13:16, 18:20) names(dat2_learn)[features] (q0 &lt;- length(features)) # training data Xtrain&lt;- as.matrix(dat2_train[, features]) # design matrix learning sample Brtrain &lt;- as.matrix(dat2_train$VehBrandX) Retrain &lt;- as.matrix(dat2_train$RegionX) Ytrain&lt;- as.matrix(dat2_train$ClaimNb) Vtrain&lt;-as.matrix(log(dat2_train$Exposure*lambda.hom)) # validation data Xvalid&lt;- as.matrix(dat2_valid[, features]) # design matrix learning sample Brvalid &lt;- as.matrix(dat2_valid$VehBrandX) Revalid &lt;- as.matrix(dat2_valid$RegionX) Yvalid&lt;- as.matrix(dat2_valid$ClaimNb) Vvalid&lt;-as.matrix(log(dat2_valid$Exposure*lambda.hom)) xxvalid&lt;-list(Xvalid,Brvalid,Revalid,Vvalid) # testing data Xtest &lt;- as.matrix(dat2_test[, features]) # design matrix test sample Brtest &lt;- as.matrix(dat2_test$VehBrandX) Retest &lt;- as.matrix(dat2_test$RegionX) Ytest &lt;- as.matrix(dat2_test$ClaimNb) Vtest &lt;- as.matrix(log(dat2_test$Exposure*lambda.hom)) 3.3 ç¥ç»ç½‘ç»œæå‡æ¨¡å‹ ï¼ˆcombined actuarial neural networkï¼‰ åŸºæœ¬ç»“æ„ \\[\\ln \\lambda(\\mathbf{x})= e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\hat{\\lambda}^{\\text{NN}}(\\mathbf{x})\\] å…¶ä¸­ï¼Œ\\(\\hat{\\lambda}^{\\text{GAM}}\\)ä¸ºå¹¿ä¹‰å¯åŠ è¾¹ç¼˜æå‡æ¨¡å‹çš„ç´¢èµ”é¢‘ç‡ä¼°è®¡å€¼ï¼ˆå‚è§ä¸Šä¸€ç« ï¼‰ï¼Œ\\(\\hat{\\lambda}^{\\text{NN}}\\)ä¸ºç¥ç»ç½‘ç»œç´¢èµ”é¢‘ç‡çš„ä¼°è®¡å€¼ï¼Œç¬¬ä¸€é¡¹åœ¨æ¨¡å‹è®­ç»ƒä¸­ä¿æŒä¸å˜ã€‚ ä½¿ç”¨ä¸Šè¿°æ¨¡å‹çš„ä¼˜ç‚¹ï¼š \\(\\hat{\\lambda}^{\\text{GAM}}\\)çš„éƒ¨åˆ†å¯è§£é‡Šæ€§ã€‚ ç¥ç»ç½‘ç»œä»ä¸€ä¸ªç›¸å¯¹â€œè¾ƒå¥½â€çš„åˆå§‹çŠ¶æ€\\(e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\)å¼€å§‹è®­ç»ƒï¼Œå¾ˆå¿«æ”¶æ•›ã€‚ åœ¨ä»£ç å®ç°ä¸­ï¼Œå¯ä»¥æŠŠ\\(e\\hat{\\lambda}^{\\text{GAM}}\\)å½“ä½œä¼ªé£é™©æš´éœ²æ•°ã€‚ CANN &lt;- 1 # 0 = normal NN, 1=CANN if (CANN==1){ Vtrain &lt;- as.matrix(log(dat1_train$fitGAM1)) Vvalid&lt;- as.matrix(log(dat1_valid$fitGAM1)) Vtest &lt;- as.matrix(log(dat1_test$fitGAM1)) } 3.4 ç¥ç»ç½‘ç»œç»“æ„ åœ¨æ„å»ºç¥ç»ç½‘ç»œæ—¶ï¼Œéœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š 3.4.1 ç»“æ„å‚æ•° ç¥ç»ç½‘ç»œç»“æ„çš„è¶…å‚æ•°é€‰æ‹©ï¼ŒBrLabelä¸ºè½¦å‹ä¸ªæ•°ï¼ŒReLabelä¸ºåœ°åŒºä¸ªæ•°ï¼Œq1-q4ä¸ºå››ä¸ªéšè—å±‚ä¸­ç¥ç»å…ƒä¸ªæ•°ï¼Œdä¸ºembedding layerä¸­ç¥ç»å…ƒä¸ªæ•°ã€‚ # hyperparameters of the neural network architecture (BrLabel &lt;- length(unique(dat2_train$VehBrandX))) (ReLabel &lt;- length(unique(dat2_train$RegionX))) q1 &lt;- 20 q2 &lt;- 15 q3 &lt;- 10 q4 &lt;- 5 d &lt;- 1 # dimensions embedding layers for categorical features 3.4.2 è¾“å…¥å±‚ è¾“å…¥å±‚åŒ…æ‹¬Design, VehBrand, Region, LogVolï¼Œå…¶ä¸­Designä¸ºè¿ç»­å‹åå˜é‡çš„è¾“å…¥å±‚ï¼ŒVehBrand, Regionä¸ºåˆ†ç±»å˜é‡çš„è¾“å…¥å±‚ï¼ŒLogVolç›´æ¥è¿æ¥åˆ°æ¨¡å‹è¾“å‡ºç¥ç»å…ƒã€‚ shapeè¡¨ç¤ºè¾“å…¥ï¼ˆè¾“å‡ºï¼‰ç»´åº¦ï¼ˆç¥ç»å…ƒä¸ªæ•°ï¼‰,dtypeè¡¨ç¤ºæ•°æ®ç±»å‹ï¼Œnameè¡¨ç¤ºå±‚åã€‚ shape=(None, 7) ä¸­Noneè¡¨ç¤ºæ ·æœ¬å¤§å°ï¼Œå› ä¸ºè¿˜æ²¡æœ‰æ•°æ®è¿›å…¥ç¥ç»ç½‘ç»œï¼Œæ•…æ­¤æ—¶ä¸ç¡®å®šã€‚ Tensor(â€œDesign_1:0â€, shape=(None, 7), dtype=float32) # input layer (Design &lt;- layer_input(shape = c(q0), dtype = &#39;float32&#39;, name = &#39;Design&#39;)) (VehBrand &lt;- layer_input(shape = c(1), dtype = &#39;int32&#39;, name = &#39;VehBrand&#39;)) (Region &lt;- layer_input(shape = c(1), dtype = &#39;int32&#39;, name = &#39;Region&#39;)) (LogVol &lt;- layer_input(shape = c(1), dtype = &#39;float32&#39;, name = &#39;LogVol&#39;)) 3.4.3 Embedding layer å»ºç«‹ä¸€ä¸ªlayerï¼Œéœ€è¦æ˜ç¡®è¾“å…¥ç¥ç»å…ƒä¸ªæ•°input_dimå’Œè¾“å‡ºç¥ç»å…ƒä¸ªæ•°output_dimï¼Œé€šå¸¸éœ€è¦æŒ‡å®šè¾“å‡ºç¥ç»å…ƒä¸ªæ•°ï¼Œè€Œè¾“å…¥ç¥ç»å…ƒä¸ªæ•°ç”±å®ƒçš„ä¸Šå±‚è¾“å‡ºç¥ç»å…ƒä¸ªæ•°å†³å®šã€‚ æŠŠåˆ†ç±»å˜é‡ç”¨layer_embeddingå¤„ç†ï¼Œè¿™ä¸¤ä¸ªlayer_embeddingçš„è¾“å‡ºç¥ç»å…ƒä¸ªæ•°ä¸º\\(d\\)ï¼Œå³æ¯ä¸ªæ°´å¹³é€šè¿‡layer_embeddingè¾“å‡º\\(d\\)ä¸ªè¿ç»­å‹å˜é‡ï¼Œå½“\\(d=1\\)ï¼Œlayer_embeddingç±»ä¼¼äºGLMå¯¹åˆ†ç±»å˜é‡çš„å¤„ç†ã€‚input_lengthä¸»è¦ç”¨äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œå¦‚æ¯ä¸ªæ ·æœ¬ä¸ºå¤šä¸ªè¯ç»„æˆçš„ä¸€å¥è¯ï¼Œè¿™é‡Œä¸€ä¸ªæ ·æœ¬åªæœ‰ä¸€ä¸ªæ°´å¹³ï¼Œæ•…input_length = 1ã€‚ layer_flattenç”¨äºè°ƒæ•´ç»´åº¦ï¼Œlayer_flattençš„è¾“å…¥ç»´åº¦æ˜¯\\(n\\times 1\\times d\\)ï¼Œè¾“å‡ºç»´åº¦æ˜¯\\(n\\times d\\)ï¼Œè¯¥å±‚æ²¡æœ‰å‚æ•°ã€‚è¯¥è¾“å‡ºç»´åº¦æ˜¯layer_denseè¦æ±‚çš„è¾“å…¥ç»´åº¦ã€‚å»ºç«‹ç¥ç»ç½‘ç»œéœ€è¦æ³¨æ„å±‚é—´ç»´åº¦åŒ¹é…ã€‚ BrandEmbå»ºç«‹çš„æ˜ å°„ä¸º\\(\\{1,\\ldots,11\\}\\rightarrow 1\\times\\mathbf{R}^d\\rightarrow\\mathbf{R}^d\\). RegionEmbå»ºç«‹çš„æ˜ å°„ä¸º\\(\\{1,\\ldots,21\\}\\rightarrow 1\\times\\mathbf{R}^d\\rightarrow\\mathbf{R}^d\\) # embedding layer (BrandEmb = VehBrand %&gt;% layer_embedding(input_dim = BrLabel, output_dim = d, input_length = 1, name = &#39;BrandEmb&#39;) %&gt;% layer_flatten(name=&#39;Brand_flat&#39;)) # input_dim is the size of vocabulary; input_length is the length of input sequences (RegionEmb = Region %&gt;% layer_embedding(input_dim = ReLabel, output_dim = d, input_length = 1, name = &#39;RegionEmb&#39;) %&gt;% layer_flatten(name=&#39;Region_flat&#39;)) 3.4.4 éšè—å±‚ 9 Networkå»ºç«‹çš„æ˜ å°„ä¸º\\[[-1,1]^{q0}\\times\\mathbf{R}^d\\times\\mathbf{R}^d\\rightarrow (-1,1)^{q1}\\rightarrow (-1,1)^{q2}\\\\ \\rightarrow (-1,1)^{q3}\\rightarrow (-1,1)^{q4}\\rightarrow\\mathbf{R}\\] layer_concatenateæŠŠä¸‰ä¸ªè¾“å…¥å±‚è¿èµ·æ¥ï¼Œlayer_dropoutä¸ºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œlayer_batch_normalizationä¸ºé˜²æ­¢vanishing gradient problemï¼Œè¿™ä¸‰ç§å±‚å†…æ— å‚æ•°ï¼Œä¸”ä¸ä¼šæ”¹å˜ä¸Šå±‚çš„ç»´åº¦ã€‚layer_dropoutä»¤ä¸€å®šæ¯”ä¾‹çš„ä¸Šå±‚ç¥ç»å…ƒä¸º0ï¼Œæ­£åˆ™åŒ–æ–¹æ³•è¿˜åŒ…æ‹¬åœ¨layer_denseä¸­ä½¿ç”¨\\(L^2\\)èŒƒæ•°æ­£åˆ™åŒ–kernel_regularizer = regularizer_l2ã€‚layer_batch_normalizationæŠŠè¾“å‡ºç¥ç»å…ƒæ˜ å°„åˆ°\\((-1,1)\\)ï¼Œé€šå¸¸åœ¨æ¿€æ´»å‡½æ•°ä¸ºreluæ›´æœ‰ç”¨ã€‚ å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ä¸ºtanh, relu, linear, exponential, softmax, sigmoidã€‚å…¶ä¸­ï¼Œsigmoid, softmaxé€‚ç”¨äºäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»çš„è¾“å‡ºç¥ç»å…ƒï¼Œexponentialé€‚ç”¨äºå› å˜é‡ä¸ºæ­£ï¼Œå¦‚æ­¤æ—¶çš„ç´¢èµ”é¢‘ç‡é¢„æµ‹ã€‚æ­¤å¤–sigmoidå’Œtanhæœ‰çº¿æ€§å…³ç³»ï¼Œå¯ä»¥åªè€ƒè™‘å…¶ä¸­ä¸€ä¸ªã€‚ layer_denseçš„æ˜ å°„ä¸ºoutput = activation (dot (input, kernal) + bias)ï¼Œæ‰€ä»¥æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒéƒ½å«æœ‰è¾“å…¥ç¥ç»å…ƒçš„ä¿¡æ¯ã€‚å¦‚æœè€ƒè™‘å¤šä¸ªå…¨è¿æ¥å±‚ï¼Œå¯ä»¥åˆ»ç”»åå˜é‡çš„äº¤äº’æ•ˆåº”ç°ã€‚æ¿€æ´»å‡½æ•°å¦‚æœå–éçº¿æ€§å‡½æ•°ï¼Œåˆ™å¯ä»¥åˆ»ç”»åå˜é‡çš„éçº¿æ€§æ•ˆåº”ã€‚ Networkä¸­æœ€åä¸€å±‚çš„å‚æ•°è®¾å®šä¸º0ï¼Œä½¿å¾—Networkåˆå§‹å€¼ä¸º0ï¼Œè¿™æ ·ç¥ç»ç½‘ç»œåˆå§‹çŠ¶æ€ä¸ºGAMï¼Œæ¢¯åº¦ä¸‹é™å°†ä»GAMå¼€å§‹ã€‚ Network = list(Design, BrandEmb, RegionEmb) %&gt;% layer_concatenate(name=&#39;concate&#39;) %&gt;% layer_dense(units=q1, activation=&#39;tanh&#39;, name=&#39;hidden1&#39;) %&gt;% layer_batch_normalization()%&gt;% layer_dropout(rate =0.05) %&gt;% layer_dense(units=q2, activation=&#39;tanh&#39;, name=&#39;hidden2&#39;) %&gt;% layer_batch_normalization()%&gt;% layer_dropout(rate =0.05) %&gt;% layer_dense(units=q3, activation=&#39;tanh&#39;, name=&#39;hidden3&#39;) %&gt;% layer_batch_normalization()%&gt;% layer_dropout(rate =0.05) %&gt;% layer_dense(units=q4, activation=&#39;tanh&#39;, name=&#39;hidden4&#39;) %&gt;% layer_batch_normalization()%&gt;% layer_dropout(rate =0.05) %&gt;% layer_dense(units=1, activation=&#39;linear&#39;, name=&#39;Network&#39;, weights = list(array(0, dim=c(q4,1)), array(0, dim=c(1)))) 3.4.5 è¾“å‡ºå±‚ Responseå»ºç«‹çš„æ˜ å°„ä¸º\\(\\mathbf{R}\\times \\mathbf{R}\\rightarrow \\mathbf{R}^+\\)ï¼Œä¸”è¦æ±‚è¯¥æ˜ å°„ä¸­çš„å‚æ•°ä¸å‚åŠ æ¢¯åº¦ä¸‹é™æ³•ã€‚å¯ä»¥çœ‹åˆ°Networkçš„è¾“å‡ºç¥ç»å…ƒä¸º\\(\\ln \\hat{\\lambda}^{\\text{NN}}(\\mathbf{x})\\)ï¼Œè¾“å…¥å±‚LogVolä¸º\\(\\ln e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\)ï¼ŒResponseçš„è¾“å‡ºç¥ç»å…ƒä¸º\\[\\exp\\left(\\ln \\hat{\\lambda}^{\\text{NN}}(\\mathbf{x}) + \\ln e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\right)=e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\hat{\\lambda}^{\\text{NN}}(\\mathbf{x}).\\] é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•ä½¿å¾—è¾“å‡ºç¥ç»å…ƒ\\(e\\hat{\\lambda}^{\\text{GAM}}(\\mathbf{x})\\hat{\\lambda}^{\\text{NN}}(\\mathbf{x})\\)ä¸è§‚å¯Ÿå€¼\\(N\\)æœ€æ¥è¿‘ï¼ˆç”¨æ³Šæ¾åå·®æŸå¤±åº¦é‡ï¼‰ï¼Œè¿›è€Œè®­ç»ƒç¥ç»ç½‘ç»œ\\(\\hat{\\lambda}^{\\text{NN}}(\\mathbf{x})\\)ä¸­çš„å‚æ•°ã€‚ Keraså®šä¹‰å¹³å‡æ³Šæ¾åå·®æŸå¤±ä¸º \\[\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{1}{|\\mathbf{N}|}\\sum_{i}\\left[\\hat{N}_i-N_i\\ln\\left(\\hat{N}_i\\right)\\right]\\] Response = list(Network, LogVol) %&gt;% layer_add(name=&#39;Add&#39;) %&gt;% layer_dense(units=1, activation=k_exp, name = &#39;Response&#39;, trainable=FALSE, weights=list(array(1, dim=c(1,1)), array(0, dim=c(1)))) model &lt;- keras_model(inputs = c(Design, VehBrand, Region, LogVol), outputs = c(Response)) model %&gt;% compile(optimizer = optimizer_nadam(), loss = &#39;poisson&#39;) summary(model) ä¸‹è¡¨åˆ—å‡ºäº†ç¥ç»ç½‘ç»œçš„ç»“æ„ï¼ŒåŒ…æ‹¬å±‚çš„åç§°ã€(å±‚çš„ç‰¹æ€§)ã€è¾“å‡ºç¥ç»å…ƒä¸ªæ•°ã€å‚æ•°ä¸ªæ•°ã€ä¸Šå±‚çš„åç§°ã€‚ Model: &quot;model&quot; ________________________________________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================================================ VehBrand (InputLayer) [(None, 1)] 0 ________________________________________________________________________________________________________________________________ Region (InputLayer) [(None, 1)] 0 ________________________________________________________________________________________________________________________________ BrandEmb (Embedding) (None, 1, 1) 11 VehBrand[0][0] ________________________________________________________________________________________________________________________________ RegionEmb (Embedding) (None, 1, 1) 21 Region[0][0] ________________________________________________________________________________________________________________________________ Design (InputLayer) [(None, 7)] 0 ________________________________________________________________________________________________________________________________ Brand_flat (Flatten) (None, 1) 0 BrandEmb[0][0] ________________________________________________________________________________________________________________________________ Region_flat (Flatten) (None, 1) 0 RegionEmb[0][0] ________________________________________________________________________________________________________________________________ concate (Concatenate) (None, 9) 0 Design[0][0] Brand_flat[0][0] Region_flat[0][0] ________________________________________________________________________________________________________________________________ hidden1 (Dense) (None, 20) 200 concate[0][0] ________________________________________________________________________________________________________________________________ batch_normalization_1 (BatchNormalization (None, 20) 80 hidden1[0][0] ________________________________________________________________________________________________________________________________ dropout (Dropout) (None, 20) 0 batch_normalization_1[0][0] ________________________________________________________________________________________________________________________________ hidden2 (Dense) (None, 15) 315 dropout[0][0] ________________________________________________________________________________________________________________________________ batch_normalization_2 (BatchNormalization (None, 15) 60 hidden2[0][0] ________________________________________________________________________________________________________________________________ dropout_1 (Dropout) (None, 15) 0 batch_normalization_2[0][0] ________________________________________________________________________________________________________________________________ hidden3 (Dense) (None, 10) 160 dropout_1[0][0] ________________________________________________________________________________________________________________________________ batch_normalization_3 (BatchNormalization (None, 10) 40 hidden3[0][0] ________________________________________________________________________________________________________________________________ dropout_2 (Dropout) (None, 10) 0 batch_normalization_3[0][0] ________________________________________________________________________________________________________________________________ hidden4 (Dense) (None, 5) 55 dropout_2[0][0] ________________________________________________________________________________________________________________________________ batch_normalization_4 (BatchNormalization (None, 5) 20 hidden4[0][0] ________________________________________________________________________________________________________________________________ dropout_3 (Dropout) (None, 5) 0 batch_normalization_4[0][0] ________________________________________________________________________________________________________________________________ Network (Dense) (None, 1) 6 dropout_3[0][0] ________________________________________________________________________________________________________________________________ LogVol (InputLayer) [(None, 1)] 0 ________________________________________________________________________________________________________________________________ Add (Add) (None, 1) 0 Network[0][0] LogVol[0][0] ________________________________________________________________________________________________________________________________ Response (Dense) (None, 1) 2 Add[0][0] ================================================================================================================================ Total params: 970 Trainable params: 868 Non-trainable params: 102 ________________________________________________________________________________________________________________________________ 3.5 è®­ç»ƒç¥ç»ç½‘ç»œ è®­ç»ƒç¥ç»ç½‘ç»œéœ€è¦æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š åˆå§‹åŒ–ç¥ç»ç½‘ç»œï¼Œå°†ä»GAMå¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œï¼Œä¸”GAMé¢„æµ‹éƒ¨åˆ†ä¿æŒä¸å˜ã€‚ å½“batch_sizeä¸ºå…¨ä½“è®­ç»ƒé›†æ—¶ï¼Œä¸ºsteepest gradient decent methodï¼Œå‚æ•°åœ¨ä¸€ä¸ªepochåªè¿­ä»£ä¸€æ¬¡ã€‚ å½“batch_sizeæ¯”å…¨ä½“è®­ç»ƒé›†å°æ—¶ï¼Œä¸ºstochastic gradient decent methodï¼Œå‚æ•°åœ¨ä¸€ä¸ªepochè¿­ä»£æ¬¡æ•°çº¦ä¸ºtraining size / batch sizeã€‚ æ¢¯åº¦ä¸‹é™æ³•å¸¸å¼•å…¥momentumï¼Œè¿›è€Œæå‡ä¼˜åŒ–æ•ˆç‡ï¼Œå¦‚adam, nadam,rmspropç­‰ï¼Œè¿™äº›ç®—æ³•è‡ªåŠ¨é€‰æ‹©learning rate, momentum parametersç­‰ã€‚ callback_early_stopping (monitor = \"val_loss\", patience =10)è¡¨ç¤ºå¦‚æœéªŒè¯é›†æŸå¤±åœ¨10æ¬¡å†…æ²¡æœ‰æå‡ï¼Œé‚£ä¹ˆåœæ­¢è®­ç»ƒï¼Œç”±æ­¤å¯ä»¥æ§åˆ¶è¿­ä»£æ¬¡æ•°ã€‚ ä½¿ç”¨predictåœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹ã€‚ # fitting the neural network early_stop &lt;- callback_early_stopping(monitor = &quot;val_loss&quot;, patience =10) # print_dot_callback &lt;- callback_lambda( # on_epoch_end = function(epoch, logs) { # if (epoch %% 50 == 0) cat(&quot;\\n&quot;) # cat(&quot;.&quot;) # } # ) {t1 &lt;- proc.time(); fit &lt;- model %&gt;% fit(list(Xtrain, Brtrain, Retrain, Vtrain), Ytrain, epochs=500, batch_size=5000, verbose=1, validation_data=list(xxvalid,Yvalid), callbacks=list(early_stop)); (proc.time()-t1)} # png(&quot;./plots/1/nn.png&quot;) matplot(cbind(fit$metrics$loss,fit$metrics$val_loss), type=&quot;l&quot;,xlab=&quot;epoch&quot;,ylab=&quot;Keras Poisson Loss&quot;) legend(&quot;topright&quot;,c(&quot;training loss&quot;,&quot;validation loss&quot;),lty=c(1,2),col=1:2) # dev.off() # calculating the predictions dat2_test$fitNN &lt;- as.vector(model %&gt;% predict(list(Xtest, Brtest, Retest, Vtest))) keras_poisson_dev(dat2_test$fitNN, dat2_test$ClaimNb) Poisson.Deviance(dat2_test$fitNN, dat2_test$ClaimNb) 3.6 æ€»ç»“ dev_sum&lt;-fread(&quot;./plots/1/dev_sum.csv&quot;)[,-1] AD&lt;-data.frame(model=&quot;Neural network&quot;,test_error=0,test_error_keras=0) dev_sum&lt;-rbind(dev_sum,AD) dev_sum$test_error[8]&lt;-round(Poisson.Deviance(dat2_test$fitNN, dat2_test$ClaimNb),4) dev_sum$test_error_keras[8]&lt;-round(keras_poisson_dev(dat2_test$fitNN, dat2_test$ClaimNb),4) # write.csv(dev_sum,&quot;./plots/1/dev_sum.csv&quot;) Boosting &gt; RF &gt; Tree &gt; NN &gt; GAM &gt; GLM &gt; Homo Boosting, RF, Tree, NNç›¸è¾ƒäºGAMçš„æå‡ä¸»è¦åœ¨äºäº¤äº’ä½œç”¨ï¼›GAMç›¸è¾ƒäºGLMçš„æå‡ä¸å¤§ï¼ŒåŸå› æ˜¯åœ¨GLMä¸­è¿›è¡Œäº†åˆé€‚çš„ç‰¹å¾å·¥ç¨‹ï¼Œå¯ä»¥åˆ»ç”»éçº¿æ€§æ•ˆåº”ã€‚ 3.7 å…¶å®ƒæ¨¡å‹ é€šè¿‡å°è¯•å‘ç°ï¼Œä¸»è¦å­˜åœ¨ä¸¤ä¸ªäº¤äº’ä½œç”¨ï¼šVehPower, VehAge, VehGas, VehBrandå’ŒDriAge, BonusMalusï¼Œå¯è®¾ç«‹å¦‚ä¸‹ç®€åŒ–çš„ç¥ç»ç½‘ç»œæå‡æ¨¡å‹ã€‚ train.x &lt;- list(as.matrix(dat2_train[,c(&quot;VehPowerX&quot;, &quot;VehAgeX&quot;, &quot;VehGasX&quot;)]), as.matrix(dat2_train[,&quot;VehBrandX&quot;]), as.matrix(dat2_train[,c(&quot;DrivAgeX&quot;, &quot;BonusMalus&quot;)]), as.matrix(log(dat1_train$fitGAM1)) ) valid.x &lt;- list(as.matrix(dat2_valid[,c(&quot;VehPowerX&quot;, &quot;VehAgeX&quot;, &quot;VehGasX&quot;)]), as.matrix(dat2_valid[,&quot;VehBrandX&quot;]), as.matrix(dat2_valid[,c(&quot;DrivAgeX&quot;, &quot;BonusMalus&quot;)]), as.matrix(log(dat1_valid$fitGAM1)) ) test.x &lt;- list(as.matrix(dat2_test[,c(&quot;VehPowerX&quot;, &quot;VehAgeX&quot;, &quot;VehGasX&quot;)]), as.matrix(dat2_test[,&quot;VehBrandX&quot;]), as.matrix(dat2_test[,c(&quot;DrivAgeX&quot;, &quot;BonusMalus&quot;)]), as.matrix(log(dat1_test$fitGAM1)) ) neurons &lt;- c(15,10,5) model.2IA &lt;- function(Brlabel){ Cont1 &lt;- layer_input(shape = c(3), dtype = &#39;float32&#39;, name=&#39;Cont1&#39;) Cat1 &lt;- layer_input(shape = c(1), dtype = &#39;int32&#39;, name=&#39;Cat1&#39;) Cont2 &lt;- layer_input(shape = c(2), dtype = &#39;float32&#39;, name=&#39;Cont2&#39;) LogExposure &lt;- layer_input(shape = c(1), dtype = &#39;float32&#39;, name = &#39;LogExposure&#39;) x.input &lt;- c(Cont1, Cat1, Cont2, LogExposure) # Cat1_embed = Cat1 %&gt;% layer_embedding(input_dim = Brlabel, output_dim = 1, trainable=TRUE, input_length = 1, name = &#39;Cat1_embed&#39;) %&gt;% layer_flatten(name=&#39;Cat1_flat&#39;) # NNetwork1 = list(Cont1, Cat1_embed) %&gt;% layer_concatenate(name=&#39;cont&#39;) %&gt;% layer_dense(units=neurons[1], activation=&#39;relu&#39;, name=&#39;hidden1&#39;) %&gt;% layer_dense(units=neurons[2], activation=&#39;relu&#39;, name=&#39;hidden2&#39;) %&gt;% layer_dense(units=neurons[3], activation=&#39;relu&#39;, name=&#39;hidden3&#39;) %&gt;% layer_dense(units=1, activation=&#39;linear&#39;, name=&#39;NNetwork1&#39;, weights=list(array(0, dim=c(neurons[3],1)), array(0, dim=c(1)))) # NNetwork2 = Cont2 %&gt;% layer_dense(units=neurons[1], activation=&#39;relu&#39;, name=&#39;hidden4&#39;) %&gt;% layer_dense(units=neurons[2], activation=&#39;relu&#39;, name=&#39;hidden5&#39;) %&gt;% layer_dense(units=neurons[3], activation=&#39;relu&#39;, name=&#39;hidden6&#39;) %&gt;% layer_dense(units=1, activation=&#39;linear&#39;, name=&#39;NNetwork2&#39;, weights=list(array(0, dim=c(neurons[3],1)), array(0, dim=c(1)))) # NNoutput = list(NNetwork1, NNetwork2, LogExposure) %&gt;% layer_add(name=&#39;Add&#39;) %&gt;% layer_dense(units=1, activation=k_exp, name = &#39;NNoutput&#39;, trainable=FALSE, weights=list(array(c(1), dim=c(1,1)), array(0, dim=c(1)))) model &lt;- keras_model(inputs = x.input, outputs = c(NNoutput)) model %&gt;% compile(optimizer = optimizer_nadam(), loss = &#39;poisson&#39;) model } model &lt;- model.2IA(BrLabel) summary(model) early_stop &lt;- callback_early_stopping(monitor = &quot;val_loss&quot;, patience =10) # print_dot_callback &lt;- callback_lambda( # on_epoch_end = function(epoch, logs) { # if (epoch %% 50 == 0) cat(&quot;\\n&quot;) # cat(&quot;.&quot;) # } # ) # may take a couple of minutes if epochs is more than 100 {t1 &lt;- proc.time() fit &lt;- model %&gt;% fit(train.x, as.matrix(dat2_train$ClaimNb), epochs=500, batch_size=10000, verbose=1, validation_data=list(valid.x,dat2_valid$ClaimNb), callback=list(early_stop)) (proc.time()-t1)} matplot(cbind(fit$metrics$loss,fit$metrics$val_loss), type=&quot;l&quot;) dat2_test$fitGAMPlus &lt;- as.vector(model %&gt;% predict(test.x)) Poisson.Deviance(dat2_test$fitGAMPlus, dat2_test$ClaimNb) keras_poisson_dev(dat2_test$fitGAMPlus, dat2_test$ClaimNb) "],["boosting.html", "4 æå‡æ–¹æ³• (Boosting) 4.1 AdaBoost (0,1) 4.2 Logit Boost (real, discrete, gentle AdaBoost) 4.3 AdaBoost.M1 4.4 SAMME (Stage-wise Additive Modeling using a Multi-class Exponential loss function) 4.5 SAMME.R (multi-class real AdaBoost) 4.6 Gradient Boosting 4.7 Newton Boosting 4.8 XGBoost 4.9 Gradient Boost 4.10 XGBoost 4.11 Case study", " 4 æå‡æ–¹æ³• (Boosting) Breiman called AdaBoost the â€˜best off-the-shelf classifier in the worldâ€™ (NIPS Workshop 1996). On the data science competition platform Kaggle, among 29 challenge winning solutions in 2015, 17 used XGBoost, a boosting algorithm introduced by Chen and Guestrin. AdaBoostæ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯è®­ç»ƒä¸åŒçš„åˆ†ç±»å™¨(å¼±åˆ†ç±»å™¨\\(T\\))ï¼Œç„¶åæŠŠè¿™äº›å¼±åˆ†ç±»å™¨çº¿æ€§ç»„åˆèµ·æ¥ï¼Œæ„æˆä¸€ä¸ªæ›´å¼ºçš„æœ€ç»ˆåˆ†ç±»å™¨ï¼ˆå¼ºåˆ†ç±»å™¨\\(C\\)ï¼‰ã€‚ è¯¥ç®—æ³•æ˜¯ä¸€ä¸ªç®€å•çš„å¼±åˆ†ç±»ç®—æ³•æå‡è¿‡ç¨‹ï¼Œè¿™ä¸ªè¿‡ç¨‹é€šè¿‡ä¸æ–­çš„è®­ç»ƒï¼Œå¯ä»¥æé«˜å¯¹æ•°æ®çš„åˆ†ç±»èƒ½åŠ›ã€‚æ•´ä¸ªè¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š é€šè¿‡å¯¹è®­ç»ƒæ ·æœ¬\\((\\mathcal{D},\\mathbb{\\omega})\\)çš„å­¦ä¹ å¾—åˆ°ç¬¬\\(m-1\\)ä¸ªå¼±åˆ†ç±»å™¨WeakClassifier m-1, \\(T^{(m-1)}\\)ï¼› è®¡ç®—å¾—å‡ºå…¶åˆ†ç±»é”™è¯¯ç‡\\(\\epsilon^{(m-1)}\\)ï¼Œä»¥æ­¤è®¡ç®—å‡ºå…¶å¼±åˆ†ç±»å™¨æƒé‡\\(\\alpha^{(m-1)}\\)ä¸æ•°æ®æƒé‡\\(\\omega^{(m-1)}_i\\); ç”¨æƒé‡ä¸º\\(\\omega^{(m-1)}_i\\)çš„æ•°æ®é›†è®­ç»ƒå¾—åˆ°è®­ç»ƒå¼±åˆ†ç±»å™¨WeakClassifier m, \\(T^{(m)}\\); é‡å¤ä»¥ä¸Šä¸æ–­è¿­ä»£çš„è¿‡ç¨‹; æœ€ç»ˆç»“æœé€šè¿‡åŠ æƒæŠ•ç¥¨è¡¨å†³çš„æ–¹æ³•ï¼Œè®©æ‰€æœ‰å¼±åˆ†ç±»å™¨\\(T^{(m)}\\)è¿›è¡Œæƒé‡ä¸º\\(\\alpha^{(m)}\\)çš„æŠ•ç¥¨è¡¨å†³çš„æ–¹æ³•å¾—åˆ°æœ€ç»ˆé¢„æµ‹è¾“å‡ºã€‚ AdaBoost: Schapire and Freund (1997, 2012) LogitBoost: Friedman, Hastie, Tibshirani (1998) AdaBoost.M1: Schapire and Freund (1996, 1997) SAMME: Zhu, Zou, Rosset et al.Â (2006) SAMME.R: Zhu, Zou, Rosset et al.Â (2006) 4.1 AdaBoost (0,1) \\(Y\\in\\{0,1\\}\\) åˆå§‹æƒé‡ \\(\\omega^{(0)}_i=\\frac{1}{n}\\). å¯¹äº \\(m=1,\\ldots,M\\), é‡å¤ä»¥ä¸‹2-5: ä½¿ç”¨\\((\\mathcal{D},\\mathbf{\\omega}^{(m-1)})\\)ï¼Œè®­ç»ƒå¼±å­¦ä¹ æœº\\(T^{(m-1)}\\). è®¡ç®—åŠ æƒåˆ†ç±»é”™è¯¯ \\(\\epsilon^{(m-1)}=\\sum_{i=1}^n\\omega^{(m-1)}_i \\mathbb{I}(y_i \\neq T^{(m-1)}(\\mathbf{x}_i))\\). è®¡ç®—æ¨¡å‹æƒé‡ \\(\\alpha^{(m-1)}=\\ln\\beta^{(m-1)}\\), å…¶ä¸­\\(\\beta^{(m-1)}=\\frac{1-\\epsilon^{(m-1)}}{\\epsilon^{(m-1)}}\\). è®¡ç®—æ ·æœ¬æƒé‡\\(\\omega^{(m)}_i=\\omega^{(m-1)}_i\\exp\\left( \\alpha^{(m-1)}\\mathbb{I}(y_i \\neq T^{(m-1)}(\\mathbf{x}_i)) \\right)/w^{(m)}\\), å…¶ä¸­\\(w^{(m)}\\)ä¸ºæ ‡å‡†åŒ–å¸¸æ•°ã€‚ æœ€ç»ˆé¢„æµ‹ç»“æœä¸ºæ¨¡å‹çš„æƒé‡ä¹‹å’Œè¾ƒå¤§çš„é‚£ä¸ªåˆ†ç±»ï¼Œå³ \\[C(\\mathbf{x})= \\underset{k}{\\arg \\max} \\sum_{m=1}^M\\alpha^{(m)}\\mathbb{I}(T^{(m)}(\\mathbf{x})=k)\\]ã€‚ å¦å¤–ä¸€ç§ç­‰ä»·ç®—æ³• \\(Y\\in\\{-1,1\\}\\) åˆå§‹æƒé‡ \\(D_1(i)=\\frac{1}{n}\\). ä½¿ç”¨\\((\\mathcal{D},D_m)\\)ï¼Œè®­ç»ƒå¼±å­¦ä¹ æœº\\(h_m\\). è®¡ç®—åŠ æƒåˆ†ç±»é”™è¯¯\\(\\epsilon_m=D_m(i)\\mathbf{I}(Y_i\\neq h_m(\\mathbf{x}_i))\\). è®¡ç®—æ¨¡å‹æƒé‡\\(\\alpha_m=\\frac{1}{2}\\ln\\beta_m\\), å…¶ä¸­\\(\\beta_m=\\frac{\\epsilon_m}{1-\\epsilon_m}\\). è®¡ç®—æ ·æœ¬æƒé‡\\(D_{m+1}(i)=\\frac{D_m(i)}{Z_m}\\exp\\left(-\\alpha_mY_ih_m(\\mathbf{x_i})\\right)\\), å…¶ä¸­\\(Z_m\\)ä¸ºæ ‡å‡†åŒ–å¸¸æ•°ã€‚ æœ€ç»ˆé¢„æµ‹ç»“æœä¸º\\(H(\\mathbf{x})= \\text{sign}\\left(\\sum_{m=1}^M \\alpha_mh_m(\\mathbf{x}) \\right)\\)ã€‚ 4.2 Logit Boost (real, discrete, gentle AdaBoost) \\(Y\\in\\{-1,1\\}\\) åˆå§‹å¼±å­¦ä¹ æœº \\(H_0(\\mathbf)=h_0(\\mathbf{x})=0\\). è®¡ç®—é¢„æµ‹æ¦‚ç‡ \\(p_m(Y_i|\\mathbf{x_i})=\\frac{1}{1+\\exp(-Y_ih_{m-1}(\\mathbf{x_i}))}\\)ã€‚æ³¨ï¼š\\(p_m(Y_i=1|\\mathbf{x_i})+p_m(Y_i=-1|\\mathbf{x_i})=1\\) è®¡ç®—æ ·æœ¬æƒé‡ \\(D_m(i)=p_m(Y_i=y_i|\\mathbf{x_i})(1-p_m(Y_i=y_i|\\mathbf{x_i}))\\). è®¡ç®—å·¥ä½œå› å˜é‡ \\(Z_m(i) = y_i(1+\\exp(-y_i H_{m-1}(\\mathbf{x_i})))\\). è®­ç»ƒå¼±å­¦ä¹ æœº\\(h_m\\)ï¼Œä½¿ä¹‹æœ€å°åŒ–å¦‚ä¸‹æŸå¤±å‡½æ•° \\[\\sum_{i=1}^N D_m(i)(h_m(\\mathbf{x_i})-Z_m(i))^2\\] ä»¤\\(H_m=H_{m-1}+h_m\\) æœ€ç»ˆé¢„æµ‹ç»“æœä¸º\\(\\Pr(Y=y|\\mathbf{x})= \\frac{1}{1+\\exp(-yH_M(\\mathbf{x_i}))}\\), å…¶ä¸­\\(H_M=h_0+\\ldots+h_M\\)ã€‚ 4.3 AdaBoost.M1 \\(Y\\in\\{1,\\ldots,k\\}\\) åˆå§‹æƒé‡ \\(D_1(i)=\\frac{1}{n}\\). ä½¿ç”¨\\((\\mathcal{D},D_m)\\)ï¼Œè®­ç»ƒå¼±å­¦ä¹ æœº\\(h_m\\). è®¡ç®—åŠ æƒåˆ†ç±»é”™è¯¯\\(\\epsilon_m=D_m(i)\\mathbf{I}(Y_i \\neq h_m(\\mathbf{x}_i)|\\). è®¡ç®—æ¨¡å‹æƒé‡\\(\\alpha_m=-\\ln\\beta_m\\), å…¶ä¸­\\(\\beta_m=\\frac{\\epsilon_m}{1-\\epsilon_m}\\). è®¡ç®—æ ·æœ¬æƒé‡\\(D_{m+1}(i)=\\frac{D_m(i)}{Z_m}\\beta_m^{1-\\mathbf{I}(Y_i \\neq h_m(\\mathbf{x}_i))}\\), å…¶ä¸­\\(Z_m\\)ä¸ºæ ‡å‡†åŒ–å¸¸æ•°ã€‚ æœ€ç»ˆé¢„æµ‹ç»“æœä¸º $H()= _{m:h_m()=y}_m $ 4.4 SAMME (Stage-wise Additive Modeling using a Multi-class Exponential loss function) \\(Y\\in \\{1,\\ldots,k\\}\\) åˆå§‹æƒé‡ \\(D_1(i)=\\frac{1}{n}\\). ä½¿ç”¨\\((\\mathcal{D},D_m)\\)ï¼Œè®­ç»ƒå¼±å­¦ä¹ æœº\\(h_m\\). è®¡ç®—åŠ æƒåˆ†ç±»é”™è¯¯\\(\\epsilon_m=D_m(i)\\mathbf{I}(Y_i \\neq h_m(\\mathbf{x}_i)|\\). è®¡ç®—æ¨¡å‹æƒé‡\\(\\alpha_m=\\eta\\left(\\ln\\beta_m + \\ln(k-1) \\right)\\), å…¶ä¸­\\(\\beta_m=\\frac{\\epsilon_m}{1-\\epsilon_m}\\). è®¡ç®—æ ·æœ¬æƒé‡\\(D_{m+1}(i)=\\frac{D_m(i)}{Z_m}\\exp\\left(\\alpha_m\\mathbf{I}(Y_i \\neq h_m(\\mathbf{x}_i))\\right)\\), å…¶ä¸­\\(Z_m\\)ä¸ºæ ‡å‡†åŒ–å¸¸æ•°ã€‚ æœ€ç»ˆé¢„æµ‹ç»“æœä¸º $H()= _{m:h_m()=y}_m $ 4.5 SAMME.R (multi-class real AdaBoost) 4.6 Gradient Boosting 4.7 Newton Boosting 4.8 XGBoost 4.9 Gradient Boost 4.10 XGBoost 4.11 Case study 4.11.1 Commonly used Python code (for py-beginners) "]]
