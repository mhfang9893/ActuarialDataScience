[["index.html", "ç°ä»£ç²¾ç®—ç»Ÿè®¡æ¨¡å‹ ğŸ‘¨ğŸ« æ¬¢è¿ ğŸ¤” ç­”ç–‘ ğŸ—“ï¸ è¯¾ç¨‹å®‰æ’", " ç°ä»£ç²¾ç®—ç»Ÿè®¡æ¨¡å‹ Modern Actuarial Models 2020-11-23 00:11:45 ğŸ‘¨ğŸ« æ¬¢è¿ ã€Šç°ä»£ç²¾ç®—ç»Ÿè®¡æ¨¡å‹ã€‹ä¸»è¦è®²è¿°å¦‚ä½•ä½¿ç”¨ç»Ÿè®¡å­¦ä¹ å’Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæå‡ä¼ ç»Ÿçš„ç²¾ç®—ç»Ÿè®¡æ¨¡å‹æˆ–è€…è§£å†³æ–°çš„ç²¾ç®—é—®é¢˜ã€‚è¿™é—¨è¯¾ä¸»è¦å‚è€ƒç‘å£«ç²¾ç®—å¸ˆåä¼šå‘å¸ƒçš„â€œç²¾ç®—æ•°æ®ç§‘å­¦â€ï¼Œè¯¥æ•™ç¨‹çš„ä¸»è¦ç›®çš„æ˜¯â€œä¸ºç²¾ç®—å¸ˆæä¾›ä¸€ä¸ªå¯¹æ•°æ®ç§‘å­¦å…¨é¢ä¸”æ˜“æ‡‚çš„ä»‹ç»â€ï¼Œè¯¥æ•™ç¨‹æä¾›äº†å¤šç¯‡æ–¹æ³•æ€§æ–‡ç« å¹¶å¼€æºä»£ç ï¼Œè¿™æ ·â€œè¯»è€…å¯ä»¥ç›¸å¯¹å®¹æ˜“åœ°æŠŠè¿™äº›æ•°æ®ç§‘å­¦æ–¹æ³•ç”¨åœ¨è‡ªå·±çš„æ•°æ®ä¸Šâ€ã€‚ æˆ‘ä»¬å»ºè®®å¤§å®¶ä»”ç»†é˜…è¯»ä»¥ä¸‹æ–‡çŒ®ï¼Œå°è¯•å¹¶ç†è§£æ‰€æœ‰ä»£ç ã€‚æ­¤ç½‘ç«™å°†ä½œä¸ºè¯¥è¯¾ç¨‹çš„è¾…åŠ©ï¼Œä¸ºå¤§å®¶ç­”ç–‘ï¼Œæ€»ç»“æ–‡çŒ®ï¼Œå¹¶å¯¹æ–‡çŒ®ä¸­çš„æ–¹æ³•åšæ‰©å±•ã€‚è¯¥ç½‘ç«™ç”±æˆè¯¾è€å¸ˆé«˜å…‰è¿œå’ŒåŠ©æ•™å¼ ç®é’°ç®¡ç†ï¼Œæ¬¢è¿å¤§å®¶åé¦ˆæ„è§åˆ°åŠ©æ•™ã€å¾®ä¿¡ç¾¤ã€æˆ–é‚®ç®± guangyuan.gao@ruc.edu.cnã€‚ ğŸ¤” ç­”ç–‘ æˆ‘å®šæœŸæŠŠåŒå­¦ä»¬çš„æ™®éç–‘é—®åœ¨è¿™é‡Œè§£ç­”ï¼Œæ¬¢è¿æé—®ï¼ ğŸ‘‰ éšæœºç§å­æ•°(2020/11/20) è¾“å…¥RNGversion(\"3.5.0\"); set.seed(100)ï¼Œä½¿å¾—ä½ çš„éšæœºç§å­æ•°å’Œpaperçš„ç›¸åŒï¼Œæ¨¡å‹ç»“æœç›¸è¿‘ã€‚ ğŸ‘‰ MAC OS, Linux, WIN (2020/11/16) æ®è§‚å¯Ÿï¼Œåœ¨MAC OSå’ŒLinuxç³»ç»Ÿä¸‹å®‰è£…kerasæˆåŠŸçš„æ¯”ä¾‹è¾ƒé«˜ã€‚WINç³»ç»Ÿä¸‹ï¼ŒPythonå„ä¸ªåŒ…çš„ä¾èµ–ä»¥åŠå’ŒRåŒ…çš„åŒ¹é…æœ‰ä¸€å®šçš„é—®é¢˜ï¼Œä»Šå¤©æ˜¯é€šè¿‡æ›´æ¢é•œåƒæºè§£å†³äº†Rä¸­æ— æ³•åŠ è½½tensorflow.kerasæ¨¡å—çš„é—®é¢˜ï¼Œæ¨æµ‹æ˜¯TUNAæºä¸­WINåŒ…ä¾èµ–å…³ç³»æ²¡æœ‰åŠæ—¶æ›´æ–°ã€‚ ä¸ºäº†è§£å†³é•œåƒæºæ›´æ–°å»¶è¿Ÿã€æˆ–è€…tensorflowç‰ˆæœ¬è¿‡ä½çš„é—®é¢˜ï¼Œè¿™é‡Œå…±äº«WINä¸‹ç»æµ‹è¯•çš„condaç¯å¢ƒé…ç½®ã€‚ä¸‹è½½è¯¥æ–‡æ¡£ï¼Œä»è¯¥æ–‡æ¡£æ‰€åœ¨æ–‡ä»¶å¤¹å¯åŠ¨å‘½ä»¤è¡Œï¼Œä½¿ç”¨å‘½ä»¤conda env create --name &lt;env&gt; --file filename.yamlï¼Œå®‰è£…è¯¥condaç¯å¢ƒã€‚åœ¨Rä¸­ä½¿ç”¨reticulate::use_condaenv(\"&lt;env&gt;\",required=T)å…³è”è¯¥ç¯å¢ƒã€‚ å¦å¤–ï¼Œå¯ä¸‹è½½MAC OSç³»ç»Ÿä¸‹ç»æµ‹è¯•çš„condaç¯å¢ƒé…ç½®ã€‚å¯é€šè¿‡conda env create --name &lt;env&gt; --file filename.yamlå®‰è£…ã€‚ ğŸ‘‰ CASdatasets (2020/11/13) æºæ–‡ä»¶åœ¨http://cas.uqam.ca/ï¼Œä½†ä¸‹è½½é€Ÿåº¦å¾ˆæ…¢ï¼Œæˆ‘æŠŠå®ƒæ”¾åœ¨åšæœäº‘å…±äº«ã€‚ä¸‹è½½åé€‰æ‹©install from local archive fileã€‚ ğŸ‘‰ å¾®ä¿¡ç¾¤ (2020/11/08) ğŸ—“ï¸ è¯¾ç¨‹å®‰æ’ ä»¥ä¸‹å®‰æ’ä¸ºåˆæ­¥è®¡åˆ’ï¼Œæ ¹æ®å¤§å®¶çš„éœ€æ±‚å’ŒèƒŒæ™¯ï¼Œæˆ‘ä»¬å¯èƒ½è¦èŠ±æ›´å¤šçš„æ—¶é—´åœ¨æŸäº›é‡è¦çš„æ–¹æ³•åŠå…¶åœ¨ç²¾ç®—ä¸Šçš„åº”ç”¨ã€‚ ç¬¬10å‘¨ï¼š å‡†å¤‡å·¥ä½œã€‚ ç¬¬11å‘¨: 1 - French Motor Third-Party Liability Claims https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764 æœºåŠ¨ 2 - Inisghts from Inside Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3226852 3 - Nesting Classical Actuarial Models into Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525 ç¬¬12å‘¨ï¼š 4 - On Boosting: Theory and Applications https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3402687 ç¬¬13å‘¨ï¼š 5 - Unsupervised Learning: What is a Sports Car https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3439358 ç¬¬14å‘¨ï¼š 6 - Lee and Carter go Machine Learning: Recurrent Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3441030 ç¬¬15å‘¨ï¼š 7 - The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547887 ç¬¬16å‘¨ï¼š 8 - Peeking into the Black Box: An Actuarial Case Study for Interpretable Machine Learning https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3595944 ç¬¬17å‘¨ï¼š 9 - Convolutional neural network case studies: (1) Anomalies in Mortality Rates (2) Image Recognition https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3656210 "],["ch0.html", "å‡†å¤‡å·¥ä½œ 0.1 å¸¸ç”¨é“¾æ¥ 0.2 å…‹éš†ä»£ç  0.3 R interface to Keras 0.4 R interface to Python 0.5 Python", " å‡†å¤‡å·¥ä½œ â€œå·¥æ¬²å–„å…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨ã€‚â€ åœ¨ä»¥ä¸‹æ­¥éª¤ä¸­ï¼Œå½“ä½ å‘ç°å®‰è£…éå¸¸æ…¢æ—¶ï¼Œå¯ä»¥å°è¯•4Gç½‘ç»œï¼Œå°è¯•VPNï¼Œå°è¯•æ”¹å˜CRANçš„é•œåƒæºï¼Œæˆ–å°è¯•æ”¹å˜condaçš„é•œåƒæºã€‚condaé•œåƒæºé€šè¿‡ä¿®æ”¹ç”¨æˆ·ç›®å½•ä¸‹çš„.condarcæ–‡ä»¶ä½¿ç”¨TUNAé•œåƒæºï¼Œä½†è¯¥é•œåƒæºå¯èƒ½æœ‰æ›´æ–°å»¶è¿Ÿã€‚ 0.1 å¸¸ç”¨é“¾æ¥ å‡†å¤‡å·¥ä½œä¸­å¸¸ç”¨çš„é“¾æ¥æœ‰ GitHub Git SSH key GitHub and RStudio Jupyter Notebook Anaconda Miniconda å¸¸ç”¨Condaå‘½ä»¤ TUNAé•œåƒæº R interface to Tensorflow and Keras reticulate Tensorflow Pytorch æ ¡çº§è®¡ç®—äº‘ CUDA cuDNN 0.2 å…‹éš†ä»£ç  GitHubæä¾›äº†å¤§é‡å¼€æºä»£ç ï¼Œè¿™é—¨è¯¾çš„ä»£ç ä¸»è¦æ¥è‡ªæ­¤é“¾æ¥ã€‚é€šå¸¸ï¼Œä½¿ç”¨GitHubå¼€æºä»£ç æœ€æ–¹ä¾¿çš„æ˜¯forkåˆ°è‡ªå·±GitHubè´¦æˆ·ä¸‹ï¼Œç„¶åcloneåˆ°æœ¬åœ°ã€‚å…·ä½“è€Œè¨€ï¼Œéœ€è¦è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š æ³¨å†ŒGitHubè´¦æˆ·ã€‚ Forkæ­¤é“¾æ¥åˆ°è‡ªå·±è´¦æˆ·ä¸‹çš„æ–°ä»“åº“,å¯é‡æ–°å‘½åä¸ºå¦‚Modern-Actuarial-Modelsæˆ–å…¶ä»–åç§°ã€‚ å®‰è£…gitã€‚åœ¨å‘½ä»¤çª—å£ä½¿ç”¨$ git config --global user.name \"Your Name\" å’Œ $ git config --global user.email \"youremail@yourdomain.com\" é…ç½®gitçš„ç”¨æˆ·åå’Œé‚®ç®±åˆ†åˆ«ä¸ºGitHubè´¦æˆ·çš„ç”¨æˆ·åå’Œé‚®ç®±ã€‚æœ€åå¯ä½¿ç”¨$ git config --listæŸ¥çœ‹é…ç½®ä¿¡æ¯ã€‚ (é€‰åš)åœ¨æœ¬åœ°ç”µè„‘åˆ›å»ºssh public keyï¼Œå¹¶æ‹·è´åˆ°GitHubä¸­Settingä¸‹SSH and GPG keysã€‚ssh public keyä¸€èˆ¬ä¿å­˜åœ¨æœ¬äººç›®å½•ä¸‹çš„éšè—æ–‡ä»¶å¤¹.sshä¸­ï¼Œæ‰©å±•åä¸º.pubã€‚è¯¦è§é“¾æ¥ã€‚è®¾ç«‹SSHå¯ä»¥é¿å…åç»­pushä»£ç åˆ°äº‘ç«¯æ—¶ï¼Œæ¯æ¬¡éƒ½éœ€è¦è¾“å…¥å¯†ç çš„éº»çƒ¦ ç”µè„‘è¿æ¥æ‰‹æœº4Gçƒ­ç‚¹ã€‚ä¸€èˆ¬åœ°ï¼Œåœ¨æ‰‹æœº4Gç½‘ç»œä¸‹å…‹éš†çš„é€Ÿåº¦æ¯”è¾ƒå¿«ã€‚ åœ¨RStudioä¸­åˆ›å»ºæ–°çš„é¡¹ç›®ï¼Œé€‰æ‹©Version Controlï¼Œç„¶åGitï¼Œåœ¨Repository URLä¸­è¾“å…¥ä½ çš„GitHubä¸­åˆšæ‰forkçš„æ–°ä»“åº“åœ°å€ï¼ˆåœ¨Codeä¸‹èƒ½æ‰¾åˆ°å…‹éš†åœ°å€ï¼Œå¦‚æœç¬¬4æ­¥å®Œæˆå¯ä»¥é€‰æ‹©SSHåœ°å€ï¼Œå¦‚æœç¬¬4æ­¥æ²¡å®Œæˆå¿…é¡»é€‰æ‹©HTTPSåœ°å€ï¼‰ï¼Œè¾“å…¥æ–‡ä»¶å¤¹åç§°ï¼Œé€‰æ‹©å­˜æ”¾ä½ç½®ï¼Œç‚¹å‡»create projectï¼ŒRStudioå¼€å§‹å…‹éš†GitHubä¸Šè¯¥ä»“åº“çš„æ‰€æœ‰å†…å®¹ã€‚ æ­¤æ—¶ï¼Œä½ åœ¨GitHubä¸Šä»“åº“çš„å†…å®¹å…¨éƒ¨å…‹éš†åˆ°äº†æœ¬åœ°ï¼Œä¸”æ”¾åœ¨äº†ä¸€ä¸ªR Projectä¸­ã€‚åœ¨è¯¥Projectä¸­ï¼Œä¼šå¤šä¸¤ä¸ªæ–‡ä»¶ï¼Œ.Rprojå’Œ.gitignoreï¼Œç¬¬ä¸€ä¸ªæ–‡ä»¶ä¿å­˜äº†Projectçš„è®¾ç½®ï¼Œç¬¬äºŒæ–‡ä»¶å‘Šè¯‰gitåœ¨pushæœ¬åœ°æ–‡ä»¶åˆ°GitHubæ—¶å“ªäº›æ–‡ä»¶è¢«å¿½ç•¥ã€‚ å¦‚æœä½ ä¿®æ”¹äº†æœ¬åœ°æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡Rä¸­å†…åµŒçš„Gitä¸Šä¼ åˆ°GitHubï¼ˆå…ˆcommitå†pushï¼‰ï¼Œè¿™æ ·æ–¹ä¾¿åœ¨ä¸åŒç”µè„‘ä¸ŠåŒæ­¥æ–‡ä»¶ã€‚gitæ˜¯ä»£ç ç‰ˆæœ¬æ§åˆ¶å·¥å…·ï¼Œåœ¨pushä¹‹å‰ï¼Œä½ å¯ä»¥æ¯”è¾ƒå’Œä¸Šä¸ªä»£ç ç‰ˆæœ¬çš„å·®å¼‚ã€‚GitHubè®°å½•äº†ä½ æ¯æ¬¡pushçš„è¯¦ç»†ä¿¡æ¯ï¼Œä¸”å­˜æ”¾åœ¨æœ¬åœ°æ–‡ä»¶å¤¹.gitä¸­ã€‚åŒæ—¶ï¼Œå¦‚æœGitHubä¸Šä»£ç æœ‰å˜åŒ–ï¼Œä½ å¯ä»¥pullåˆ°æœ¬åœ°ã€‚å¦‚æœç»å¸¸åœ¨ä¸åŒç”µè„‘ä¸Šä½¿ç”¨æœ¬ä»“åº“ï¼Œä¸€èˆ¬éœ€è¦å…ˆpullæˆæœ€æ–°ç‰ˆæœ¬ï¼Œç„¶åå†ç¼–è¾‘ä¿®æ”¹ï¼Œæœ€åcommit-pushåˆ°GitHubã€‚ (é€‰åš) ä½ å¯ä»¥å»ºç«‹æ–°çš„branchï¼Œä½¿è‡ªå·±çš„ä¿®æ”¹å’Œæºä»£ç åˆ†å¼€ã€‚å…·ä½“æ“ä½œå¯å‚è€ƒé“¾æ¥ï¼Œæˆ–è€…å‚è€ƒè´¦æˆ·å»ºç«‹æ—¶è‡ªåŠ¨äº§ç”Ÿçš„getting-startedä»“åº“ã€‚ (é€‰åš) ä½ å¯ä»¥å°è¯•Github Desktopæˆ–è€…Jupyter Labï¼ˆåŠ è½½git extensionï¼‰ç®¡ç†ï¼Œä½†å¯¹äºè¿™é—¨è¯¾ï¼Œè¿™ä¸¤ç§æ–¹å¼ä¸æ˜¯æœ€ä¼˜ã€‚ ç†è®ºä¸Šï¼ŒGitHubä¸Šæ‰€æœ‰ä»“åº“éƒ½å¯ä»¥é‡‡ç”¨ä»¥ä¸Šæ–¹æ³•åœ¨RStudioä¸­ç®¡ç†ï¼Œå½“ç„¶ï¼ŒRStudioå¯¹äºRä»£ç ä»“åº“ç®¡ç†æœ€æœ‰æ•ˆï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨RStudioä¸­è¿è¡Œä»“åº“ä¸­çš„ä»£ç ã€‚ 0.3 R interface to Keras è¿™é‡Œä¸»è¦è¯´æ˜kerasåŒ…çš„å®‰è£…å’Œä½¿ç”¨ã€‚Kerasæ˜¯tensorflowçš„APIï¼Œåœ¨kerasä¸­å»ºç«‹çš„ç¥ç»ç½‘ç»œæ¨¡å‹éƒ½ç”±tensorflowè®­ç»ƒã€‚å®‰è£…kerasåŒ…ä¸»è¦æ˜¯å®‰è£…Pythonåº“tensorflowï¼Œå¹¶è®©Rä¸ä¹‹ç›¸å…³è”ã€‚ 0.3.1 Rè‡ªåŠ¨å®‰è£… æœ€ç®€å•çš„å®‰è£…æ–¹å¼å¦‚ä¸‹ï¼š ä½¿ç”¨install.packages(\"tensorflow\")å®‰è£…æ‰€æœ‰ç›¸å…³çš„åŒ…ï¼Œç„¶ålibrary(\"tensorflow\")ã€‚ install_tensorflow() è¿™æ—¶å¤§æ¦‚ç‡ä¼šå‡ºç° No non-system installation of Python could be found. Would you like to download and install Miniconda? Miniconda is an open source environment management system for Python. See https://docs.conda.io/en/latest/miniconda.html for more details. Would you like to install Miniconda? [Y/n]: è™½ç„¶ä½ å¯èƒ½å·²ç»æœ‰Anacondaå’ŒPythonï¼Œä½†Ræ²¡æœ‰â€œæ™ºèƒ½â€åœ°è¯†åˆ«å‡ºæ¥ï¼Œè¿™æ—¶ä»å»ºè®®ä½ é€‰Yï¼Œè®©Rè‡ªå·±è£…ä¸€ä¸‹è‡ªå·±èƒ½æ›´å¥½è¯†åˆ«çš„Miniconda, è¿™ä¸ªå‘½ä»¤è¿˜ä¼šè‡ªåŠ¨å»ºç«‹ä¸€ä¸ªç‹¬ç«‹condaç¯å¢ƒr-reticulateï¼Œå¹¶åœ¨å…¶ä¸­è£…å¥½tensorflow, kerasç­‰ã€‚ ä¸Šæ­¥å¦‚æœæ­£å¸¸è¿è¡Œï¼Œç»“æŸåä¼šè‡ªåŠ¨é‡å¯Rã€‚è¿™æ—¶ä½ è¿è¡Œlibrary(tensorflow)ç„¶åtf$constant(\"Hellow Tensorflow\")ï¼Œå¦‚æœæ²¡æŠ¥é”™ï¼Œé‚£ç»§ç»­install_packages(\"keras\"),library(\"keras\")ã€‚ ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) å¦‚æœå‡ºç°ä»¥ä¸‹é”™è¯¯ é”™è¯¯: Installation of TensorFlow not found. Python environments searched for &#39;tensorflow&#39; package: C:\\Users\\...\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\python.exe You can install TensorFlow using the install_tensorflow() function. è¿™ä¸ªé”™è¯¯é€šå¸¸æ˜¯ç”±äºr-reticulateä¸­tensorflowå’Œå…¶ä»–åŒ…çš„ä¾èµ–å…³ç³»å‘ç”Ÿé”™è¯¯ï¼Œæˆ–è€…tensorflowç‰ˆæœ¬å¤ªä½ï¼Œä½ å¯ä»¥æ›´æ¢é•œåƒæºã€ä½¿ç”¨conda/pip installè°ƒæ•´è¯¥ç¯å¢ƒä¸­çš„tensorflowç‰ˆæœ¬å’Œä¾èµ–å…³ç³»ã€‚ æ›´å¥½çš„æ–¹å¼æ˜¯åœ¨condaä¸‹å®‰è£…å¥½æŒ‡å®šç‰ˆæœ¬çš„tensorflowç„¶åå…³è”åˆ°Rï¼Œæˆ–è€…ç”¨å…¶ä»–æ–¹å¼è®©Ræ‰¾åˆ°å…¶ä»–æ–¹å¼å®‰è£…çš„tensorflowã€‚è¿™æ—¶ï¼Œä½ å…ˆæŠŠä¹‹å‰å¤±è´¥çš„å®‰è£…C:\\Users\\...\\AppData\\Local\\r-minicondaï¼Œè¿™ä¸ªæ–‡ä»¶å¤¹å®Œå…¨åˆ æ‰ã€‚ç„¶åå‚è€ƒä»¥ä¸‹å®‰è£…æ­¥éª¤ã€‚ 0.3.2 ä½¿ç”¨reticulateå…³è”condaç¯å¢ƒ ä¸‹è½½å¹¶å®‰è£…Anacondaæˆ–è€…Minicondaã€‚ è¿è¡ŒAnaconda Promptæˆ–è€…Anaconda Powershell Promptï¼Œåœ¨å‘½ä»¤è¡Œè¾“å…¥conda create -n r-tensorflow tensorflow=2.1.0ï¼Œcondaä¼šåˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„r-tensorflowç¯å¢ƒï¼Œå¹¶åœ¨å…¶ä¸­å®‰è£…tensorflowåŒ…ã€‚ ç»§ç»­åœ¨å‘½ä»¤è¡Œè¿è¡Œconda activate r-tensorflowåŠ è½½åˆšåˆšå®‰è£…çš„ç¯å¢ƒï¼Œå¹¶pip install h5py pyyaml requests Pillow scipyåœ¨è¯¥ç¯å¢ƒä¸‹å®‰è£…kerasä¾èµ–çš„åŒ…ã€‚è‡³æ­¤ï¼ŒRéœ€è¦çš„tensorflowç¯å¢ƒå·²ç»å‡†å¤‡å¥½ï¼Œæ¥ä¸‹æ¥è®©Rå…³è”æ­¤ç¯å¢ƒã€‚ é‡å¯Rï¼Œlibrary(\"reticulate\")ç„¶åuse_condaenv(\"r-tensorflow\",required=T),è¿™æ—¶Rå°±å’Œä¸Šé¢å»ºç«‹çš„ç¯å¢ƒå…³è”å¥½ã€‚ library(\"kerasâ€œ)ã€‚è¿™é‡Œå‡è®¾ä½ å·²ç»è£…å¥½tensorflowå’ŒkerasåŒ…ã€‚ ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 0.3.3 æŒ‡å®šcondaå®‰è£… ä¸‹è½½å¹¶å®‰è£…Anacondaæˆ–è€…Minicondaã€‚ å‘½ä»¤è¡Œè¾“å…¥which -a pythonï¼Œæ‰¾åˆ°Anacondaä¸­Pythonçš„è·¯å¾„è®°ä¸ºanapyã€‚ Rä¸­install_packages(\"tensorflow\")ï¼Œç„¶å install_tensorflow(method = &quot;conda&quot;, conda = &quot;anapy&quot;, envname = &quot;r-tensorflow&quot;, version = &quot;2.1.0&quot;) æ­¤å‘½ä»¤ä¼šåœ¨condaä¸‹åˆ›å»ºr-tensorflowçš„ç¯å¢ƒå¹¶è£…å¥½tensorflowåŒ…ã€‚ install_packages(\"keras\"); library(\"keras\") ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 0.3.4 ä½¿ç”¨reticulateå®‰è£… é‡å¯Rï¼Œlibrary(\"reticulate\")ã€‚ options(timeout=300)ï¼Œé˜²æ­¢ä¸‹è½½æ—¶é—´è¿‡é•¿ä¸­æ–­ã€‚ install_miniconda()ï¼Œå°†ä¼šå®‰è£…minicondaå¹¶åˆ›å»ºä¸€ä¸ªr-reticulatecondaç¯å¢ƒã€‚æ­¤ç¯å¢ƒä¸ºRé»˜è®¤è°ƒç”¨çš„Pythonç¯å¢ƒã€‚ ï¼ˆé‡å¯Rï¼‰library(\"tensorflow\"); install_tensorflow(version=\"2.1.0\")ï¼Œå°†ä¼šåœ¨r-reticulateå®‰è£…tensorflowã€‚ install_packages(\"keras\"); library(\"keras\") ç”¨ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æˆåŠŸ model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 0.4 R interface to Python RåŒ…reticulateä¸ºtensorflowçš„ä¾èµ–åŒ…ï¼Œå½“ä½ è£…tensorflowå®ƒä¹Ÿè¢«è‡ªåŠ¨å®‰è£…ã€‚å®ƒå¯ä»¥å»ºç«‹Rä¸Pythonçš„äº¤äº’ã€‚ 0.4.1 reticulate å¸¸è§å‘½ä»¤ conda_list()åˆ—å‡ºå·²å®‰è£…çš„condaç¯å¢ƒ virtualenv_list()åˆ—å‡ºå·²å­˜åœ¨çš„è™šæ‹Ÿç¯å¢ƒ use_python, use_condaenv, use_virtualenvå¯ä»¥æŒ‡å®šä¸Rå…³è”çš„pythonã€‚ py_config()å¯ä»¥æŸ¥çœ‹å½“å‰Pythonå…³è”ä¿¡æ¯ã€‚ å¾ˆå¤šæ—¶å€™ï¼ŒRä¼šåˆ›å»ºä¸€ä¸ªç‹¬ç«‹condaç¯å¢ƒr-miniconda/envs/r-reticulateã€‚ 0.4.2 åˆ‡æ¢Rå…³è”çš„condaç¯å¢ƒ æ ¹æ®éœ€è¦ï¼Œä½ å¯ä»¥åˆ‡æ¢Rå…³è”çš„condaç¯å¢ƒã€‚å…·ä½“æ­¥éª¤ä¸º é‡å¯R library(\"reticulate\") conda_list()åˆ—å‡ºå¯ä»¥å…³è”çš„ç¯å¢ƒå’Œè·¯å¾„ã€‚ use_condaenv(\"env-name\")ã€‚env-nameä¸ºå…³è”çš„condaç¯å¢ƒã€‚ py_configæŸ¥çœ‹æ˜¯å¦å…³è”æˆåŠŸã€‚ 0.5 Python ä¸€èˆ¬åœ¨æ¯ä¸ªPythonï¼ˆCondaï¼‰ç¯å¢ƒéƒ½éœ€è¦å®‰è£…ä¸€ä¸ªJupyter Notebook (conda install notebook)ã€‚ 0.5.1 Condaç¯å¢ƒ Pythonï¼ˆcondaï¼‰ç¯å¢ƒå»ºç«‹æ¯”è¾ƒç®€å•ï¼Œåœ¨ä½¿ç”¨reticulateå…³è”condaç¯å¢ƒæˆ‘ä»¬å·²ç»å»ºç«‹è¿‡ä¸€ä¸ªç¯å¢ƒr-tensorflowã€‚å…·ä½“æ“ä½œå¦‚ä¸‹: å»ºç«‹ç‹¬ç«‹ç¯å¢ƒconda create -n env-name python=3.8 tensorflow=2.1.0 notebookã€‚è¯¥å‘½ä»¤ä¼šå»ºç«‹env-nameçš„ç¯å¢ƒï¼Œå¹¶åœ¨å…¶ä¸­å®‰è£…python=3.8,tensorflowï¼ŒnotebookåŒ…åŠå…¶ä¾èµ–åŒ…ã€‚ æ¿€æ´»ç¯å¢ƒconda activate env-name. cd åˆ°ä½ çš„å·¥ä½œç›®å½•ã€‚ å¯åŠ¨jupyter notebook jupyter notebookã€‚ å¦‚é‡åˆ°ç¼ºå°‘çš„åŒ…ï¼Œåœ¨è¯¥ç¯å¢ƒenv-nameä¸‹ä½¿ç”¨conda install ***å®‰è£…ç¼ºå°‘çš„åŒ…ã€‚ 0.5.2 å¸¸ç”¨çš„Condaå‘½ä»¤ conda create -n env-name2 --clone env-name1:å¤åˆ¶ç¯å¢ƒ conda env listï¼šåˆ—å‡ºæ‰€æœ‰ç¯å¢ƒ conda deactivateï¼šé€€å‡ºå½“å‰ç¯å¢ƒ conda remove -n env-name --allï¼šåˆ é™¤ç¯å¢ƒenv-nameä¸­çš„æ‰€æœ‰åŒ… conda list -n env-name: åˆ—å‡ºç¯å¢ƒenv-nameæ‰€å®‰è£…çš„åŒ… conda clean -pï¼šåˆ é™¤ä¸ä½¿ç”¨çš„åŒ… conda clean -tï¼šåˆ é™¤ä¸‹è½½çš„åŒ… conda clean -aï¼šåˆ é™¤æ‰€æœ‰ä¸å¿…è¦çš„åŒ… pip freeze &gt; pip_pkg.txt, pip install -r pip_pkg.txt ä¿å­˜å½“å‰ç¯å¢ƒPyPIåŒ…ç‰ˆæœ¬ï¼Œä»æ–‡ä»¶å®‰è£…PyPIåŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ conda env export &gt; conda_pkg.yaml, conda env export --name env_name &gt; conda_pkg.yaml, conda env create --name env-name2 --file conda_pkg.yaml ä¿å­˜å½“å‰/env-nameç¯å¢ƒæ‰€æœ‰åŒ…ï¼Œä»æ–‡ä»¶å®‰è£…æ‰€æœ‰åŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ conda list --explicit &gt; spec-list.txt, conda create --name env-name2 --file spec-list.txt ä¿å­˜å½“å‰ç¯å¢ƒCondaåŒ…ä¸‹è½½åœ°å€ï¼Œä»æ–‡ä»¶å®‰è£…CondaåŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ conda list --export &gt; spec-list.txt, conda create --name env-name2 --file spec-list.txt ä¿å­˜å½“å‰ç¯å¢ƒæ‰€æœ‰åŒ…ï¼ˆç±»ä¼¼conda env exportï¼‰ï¼Œä»æ–‡ä»¶å®‰è£…æ‰€æœ‰åŒ…ï¼ˆéœ€åŒç³»ç»Ÿï¼‰ 0.5.3 Tensorflow/Pytorch GPU version Tensorflowå¯ä»¥ç»¼åˆä½¿ç”¨CPUå’ŒGPUè¿›è¡Œè®¡ç®—ï¼ŒGPUçš„ç¡¬ä»¶ç»“æ„é€‚è¿›è¡Œå·ç§¯è¿ç®—ï¼Œæ‰€ä»¥é€‚äºCNNï¼ŒRNNç­‰æ¨¡å‹çš„æ±‚è§£ã€‚ ä½ å¯ä»¥ç”³è¯·ä½¿ç”¨æ ¡çº§è®¡ç®—äº‘æˆ–è€…ä½¿ç”¨å­¦é™¢è®¡ç®—äº‘ï¼Œå®ƒä»¬çš„æœåŠ¡å™¨éƒ½é…ç½®äº†GPUï¼Œå¹¶è£…å¥½äº†å¯ä»¥ä½¿ç”¨GPUçš„Tensorflowæˆ–è€…Pytorchã€‚ä½¿ç”¨æ ¡çº§è®¡ç®—äº‘æ—¶ï¼Œä½ é€šå¸¸åªéœ€è¦è¿è¡ŒJupyter Notebookå°±å¯ä»¥ä½¿ç”¨äº‘ç«¯GPUè¿›è¡Œè®¡ç®—ã€‚ä½¿ç”¨å­¦é™¢è®¡ç®—äº‘æ—¶ï¼Œä½ é€šå¸¸éœ€è¦çŸ¥é“ä¸€äº›å¸¸ç”¨çš„Linuxå‘½ä»¤ï¼Œä½ ä¹Ÿå¯ä»¥å®‰è£…Ubuntuæ¥ç†Ÿæ‚‰Linuxç³»ç»Ÿã€‚ æ ¡çº§è®¡ç®—äº‘å’Œå­¦é™¢è®¡ç®—äº‘æœ‰ä¸“é—¨çš„ITäººå‘˜å¸®ä½ è§£å†³å¦‚æœ¬é¡µæ‰€ç¤ºçš„å¤§éƒ¨åˆ†ITé—®é¢˜ã€‚ ä½ çš„æœºå™¨å¦‚æœæœ‰GPUï¼Œå¯ä»¥æŒ‰å¦‚ä¸‹æ­¥éª¤è®©GPUå‘æŒ¥å®ƒçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œå…³é”®ç‚¹æ˜¯è®©GPUå‹å·ã€GPUé©±åŠ¨ã€CUDAç‰ˆæœ¬ã€Tensorflowæˆ–Pytorchç‰ˆæœ¬å½¼æ­¤åŒ¹é…ï¼Œä¸”å½¼æ­¤â€œç›¸è¿â€ã€‚ç™¾åº¦æˆ–è€…å¿…åº”ä¸Šæœ‰å¾ˆå¤šç›¸å…³èµ„æ–™å¯ä»¥ä½œä¸ºå‚è€ƒã€‚ æŸ¥çœ‹ç”µè„‘GPUå’Œé©±åŠ¨ï¼Œä»¥åŠæ”¯æŒçš„CUDAç‰ˆæœ¬ã€‚ æˆ–è€…åœ¨ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼šnvidia-smiï¼ŒæŸ¥çœ‹ä½ çš„NVIDIAæ˜¾å¡é©±åŠ¨æ”¯æŒçš„CUDAç‰ˆæœ¬ã€‚ æŸ¥çœ‹å„ä¸ªTensorflowç‰ˆæœ¬ï¼ŒPytorchç‰ˆæœ¬å¯¹åº”çš„CUDAå’ŒcuDNN. ä¸‹è½½å¹¶å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„CUDAã€‚æ³¨å†Œã€ä¸‹è½½å¹¶å®‰è£…æ­£ç¡®ç‰ˆæœ¬çš„cuDNN é…ç½®CUDAå’ŒcuDNN. å®‰è£…Tensorflowæˆ–è€…Pytorch. "],["ch2.html", "1 è½¦é™©ç´¢èµ”é¢‘ç‡é¢„æµ‹ 1.1 èƒŒæ™¯ä»‹ç» 1.2 é¢„æµ‹æ¨¡å‹æ¦‚è¿° 1.3 ç‰¹å¾å·¥ç¨‹ 1.4 è®­ç»ƒé›†-éªŒè¯é›†-æµ‹è¯•é›† 1.5 æ³Šæ¾åå·®æŸå¤±å‡½æ•° 1.6 GLM &amp; GAM 1.7 Possion tree 1.8 Random forest 1.9 Boosting Poisson tree 1.10 Summary", " 1 è½¦é™©ç´¢èµ”é¢‘ç‡é¢„æµ‹ 1.1 èƒŒæ™¯ä»‹ç» è½¦é™©æ•°æ®é‡å¤§ï¼Œé£é™©ç‰¹å¾å¤šï¼Œå¯¹è½¦é™©æ•°æ®åˆ†ææ—¶å¯ä»¥ä½“ç°å‡ºæœºå™¨å­¦ä¹ ç®—æ³•çš„ä¼˜åŠ¿ï¼Œå³ä½¿ç”¨ç®—æ³•ä»å¤§æ•°æ®ä¸­æŒ–æ˜æœ‰ç”¨ä¿¡æ¯ã€æå–ç‰¹å¾ã€‚ åœ¨ç²¾ç®—ä¸­ï¼Œå¸¸å¸¸ä½¿ç”¨è½¦é™©ä¿å•æ•°æ®å’Œå†å²ç´¢èµ”æ•°æ®è¿›è¡Œé£é™©åˆ†æã€è½¦é™©å®šä»·ç­‰ã€‚ä¿å•æ•°æ®åº“æ˜¯åœ¨æ‰¿ä¿çš„æ—¶å€™å»ºç«‹çš„ï¼Œç´¢èµ”æ•°æ®åº“æ˜¯åœ¨ç´¢èµ”å‘ç”Ÿæ—¶å»ºç«‹çš„ï¼Œå¤§éƒ¨åˆ†ä¿å•æ²¡æœ‰å‘ç”Ÿç´¢èµ”ï¼Œæ‰€ä»¥å®ƒä»¬ä¸ä¼šåœ¨ç´¢èµ”æ•°æ®åº“ä¸­ä½“ç°ã€‚ ä¿å•æ•°æ®åº“è®°å½•äº†è½¦é™©çš„é£é™©ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š é©¾é©¶å‘˜ç‰¹å¾ï¼šå¹´é¾„ã€æ€§åˆ«ã€å·¥ä½œã€å©šå§»ã€åœ°å€ç­‰ è½¦è¾†ç‰¹å¾ï¼šå“ç‰Œã€è½¦åº§æ•°ã€è½¦é¾„ã€ä»·æ ¼ã€é©¬åŠ›ç­‰ ä¿å•ä¿¡æ¯ï¼šä¿å•ç¼–å·ã€æ‰¿ä¿æ—¥æœŸã€åˆ°æœŸæ—¥æœŸ å¥–æƒ©ç³»æ•° ç´¢èµ”æ•°æ®åº“è®°å½•äº†ä¿å•çš„ç´¢èµ”ä¿¡æ¯ï¼Œå¯ä»¥å¾—åˆ°ç´¢èµ”æ¬¡æ•°\\(N\\)å’Œæ¯æ¬¡çš„ç´¢èµ”é‡‘é¢\\(Y_l,l=1,\\ldots,N\\)ã€‚ç†è®ºä¸Šï¼Œè½¦é™©çš„çº¯ä¿è´¹ä¸ºä»¥ä¸‹éšæœºå’Œçš„æœŸæœ› \\[S=\\sum_{l=1}^N Y_l\\] å‡è®¾ç´¢èµ”æ¬¡æ•°\\(N\\)å’Œç´¢èµ”é‡‘é¢\\(Y_l\\)ç‹¬ç«‹ä¸”\\(Y_l\\)æœä»ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œåˆ™ \\[\\mathbf{E}(S)=\\mathbf{E}(N)\\times\\mathbf{E}(Y)\\] æ‰€ä»¥ï¼Œè½¦é™©å®šä»·é—®é¢˜å¾ˆå¤šæ—¶å€™éƒ½è½¬åŒ–ä¸ºä¸¤ä¸ªç‹¬ç«‹æ¨¡å‹ï¼šç´¢èµ”æ¬¡æ•°ï¼ˆé¢‘ç‡ï¼‰æ¨¡å‹å’Œç´¢èµ”é‡‘é¢ï¼ˆå¼ºåº¦ï¼‰æ¨¡å‹ã€‚å¯¹äºç´¢èµ”æ¬¡æ•°æ¨¡å‹ï¼Œé€šå¸¸å‡è®¾å› å˜é‡æœä»æ³Šæ¾åˆ†å¸ƒï¼Œå»ºç«‹æ³Šæ¾å›å½’æ¨¡å‹ï¼Œä½¿ç”¨çš„æ•°æ®é‡ç­‰äºä¿å•æ•°ï¼›å¯¹äºç´¢èµ”é‡‘é¢æ¨¡å‹ï¼Œé€šå¸¸å‡è®¾å› å˜é‡æœä»ä¼½é©¬åˆ†å¸ƒï¼Œå»ºç«‹ä¼½é©¬å›å½’æ¨¡å‹ï¼Œä½¿ç”¨çš„æ•°æ®é‡ç­‰äºå‘ç”Ÿç´¢èµ”çš„ä¿å•æ•°ã€‚é€šå¸¸ï¼Œåœ¨æ•°æ®é‡ä¸å¤§æ—¶ï¼Œç´¢èµ”é‡‘é¢æ¨¡å‹çš„å»ºç«‹éš¾äºç´¢èµ”æ¬¡æ•°æ¨¡å‹ï¼Œå› ä¸ºåªæœ‰å‘ç”Ÿç´¢èµ”çš„ä¿å•æ‰èƒ½ç”¨äºç´¢èµ”é‡‘é¢æ¨¡å‹çš„å»ºç«‹ã€‚ è®°ç¬¬\\(i\\)ä¸ªä¿å•çš„é£é™©ä¿¡æ¯ä¸º\\(x_i\\in\\mathcal{X}\\)ï¼Œä¿é™©å…¬å¸å®šä»·çš„ç›®æ ‡å°±æ˜¯æ‰¾åˆ°ä¸¤ä¸ªï¼ˆæœ€ä¼˜ï¼‰å›å½’æ–¹ç¨‹ï¼ˆæ˜ å°„ï¼‰ï¼Œä½¿ä¹‹å°½å¯èƒ½å‡†ç¡®åœ°é¢„æµ‹ç´¢èµ”é¢‘ç‡å’Œç´¢èµ”å¼ºåº¦: \\[\\lambda: \\mathcal{X}\\rightarrow \\mathbf{R}_+, ~~~ x \\mapsto \\lambda(x_i)\\] \\[\\mu: \\mathcal{X}\\rightarrow \\mathbf{R}_+, ~~~ x \\mapsto \\mu(x_i)\\] è¿™é‡Œï¼Œ\\(\\lambda(x_i)\\)æ˜¯å¯¹\\(N\\)çš„æœŸæœ›çš„ä¼°è®¡ï¼Œ\\(\\mu(x_i)\\)æ˜¯å¯¹\\(Y\\)çš„æœŸæœ›çš„ä¼°è®¡ã€‚åŸºäºè¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œçº¯ä¿è´¹ä¼°è®¡ä¸º\\(\\lambda(x_i)\\mu(x_i)\\)ã€‚ 1.2 é¢„æµ‹æ¨¡å‹æ¦‚è¿° å¦‚ä½•å¾—åˆ°ä¸€ä¸ªå¥½çš„é¢„æµ‹æ¨¡å‹å‘¢ï¼Ÿå¯ä»¥ä»ä¸¤ä¸ªæ–¹é¢è€ƒè™‘ï¼š è®©é£é™©ä¿¡æ¯ç©ºé—´\\(\\mathcal{X}\\)ä¸°å¯Œï¼Œä¹Ÿç§°ä¸ºç‰¹å¾å·¥ç¨‹ï¼Œæ¯”å¦‚åŒ…å«\\(x,x^2,\\ln x\\)ã€æˆ–è€…åŠ å…¥è½¦è”ç½‘ä¿¡æ¯ã€‚ è®©æ˜ å°„ç©ºé—´\\(\\lambda\\in{\\Lambda},\\mu\\in M\\)ä¸°å¯Œï¼Œå¦‚GLMåªåŒ…å«çº¿æ€§æ•ˆåº”ã€ç›¸åŠ æ•ˆåº”ï¼Œæ˜ å°„ç©ºé—´è¾ƒå°ï¼Œç¥ç»ç½‘ç»œåŒ…å«éçº¿æ€§æ•ˆåº”ã€äº¤äº’ä½œç”¨ï¼Œæ˜ å°„ç©ºé—´è¾ƒå¤§ã€‚ å½“ä½ é€‰å–äº†æ˜ å°„ç©ºé—´è¾ƒå°çš„GLMï¼Œé€šå¸¸éœ€è¦è¿›è¡Œä»”ç»†çš„ç‰¹å¾å·¥ç¨‹ï¼Œä½¿å¾—é£é™©ä¿¡æ¯ç©ºé—´é€‚äºGLMï¼›å½“ä½ é€‰å–äº†æ˜ å°„ç©ºé—´è¾ƒå¤§çš„ç¥ç»ç½‘ç»œï¼Œé€šå¸¸ä¸éœ€è¦è¿›è¡Œç‰¹åˆ«ä»”ç»†çš„ç‰¹å¾å·¥ç¨‹ï¼Œç¥ç»ç½‘ç»œå¯ä»¥è‡ªåŠ¨è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Œå‘æ˜é£é™©ä¿¡æ¯ä¸­çš„æœ‰ç”¨ç‰¹å¾ã€‚ å¯¹äºä¼ ç»Ÿçš„ç»Ÿè®¡å›å½’æ¨¡å‹ï¼ŒGLMï¼ŒGAMï¼ŒMARSï¼Œæˆ‘ä»¬ä½¿ç”¨æå¤§ä¼¼ç„¶æ–¹æ³•åœ¨æ˜ å°„ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä¼˜çš„å›å½’æ–¹ç¨‹ï¼Œåœ¨æå¤§ä¼¼ç„¶ä¸­ä½¿ç”¨çš„æ•°æ®é›†ç§°ä¸ºå­¦ä¹ é›†ï¼ˆlearning data setï¼‰ã€‚ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œåå˜é‡é€‰æ‹©ï¼Œå¯ä»¥åˆ æ‰ä¸æ˜¾è‘—çš„åå˜é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é€æ­¥å›å½’ã€æœ€ä¼˜å­é›†ã€LASSOç­‰ï¼Œåˆ¤æ–­æ ‡å‡†ä¸ºAICç­‰ã€‚ å¯¹äºæ ‘æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ recursive partitioning by binary splits ç®—æ³•å¯¹é£é™©ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œä½¿å¾—å„å­ç©ºé—´å†…çš„åº”å˜é‡å·®å¼‚æœ€å°ï¼Œå·®å¼‚é€šå¸¸ä½¿ç”¨åå·®æŸå¤±ï¼ˆdeviance lossï¼‰åº¦é‡ã€‚ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé€šå¸¸ä½¿ç”¨äº¤å‰éªŒè¯å¯¹æ ‘çš„æ·±åº¦è¿›è¡Œæ§åˆ¶ã€‚æ ‘æ¨¡å‹è®­ç»ƒä½¿ç”¨çš„æ•°æ®ä¸ºå­¦ä¹ é›†ã€‚ æ ‘æ¨¡å‹çš„æ‰©å±•ä¸ºbootstrap aggregationï¼ˆbaggingï¼‰å’Œrandom forestã€‚ç¬¬ä¸€ç§ç®—æ³•æ˜¯å¯¹æ¯ä¸ªbootstrapæ ·æœ¬å»ºç«‹æ ‘æ¨¡å‹ï¼Œç„¶åå¹³å‡æ¯ä¸ªæ ‘æ¨¡å‹çš„é¢„æµ‹ï¼›ç¬¬äºŒç§ç®—æ³•ç±»ä¼¼ç¬¬ä¸€ç§ï¼Œä½†åœ¨å»ºç«‹æ ‘æ¨¡å‹æ—¶ï¼Œè¦æ±‚åªåœ¨æŸäº›éšæœºé€‰å®šçš„åå˜é‡ä¸Šåˆ†æ”¯ã€‚è¿™ä¸¤ç§æ‰©å±•éƒ½å±äºé›†æˆå­¦ä¹ ï¼ˆensemble learningï¼‰ã€‚ æå‡ç®—æ³•æœ‰å¤šç§ä¸åŒå½¢å¼ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³ç±»ä¼¼é€æ­¥å›å½’ï¼ŒåŒºåˆ«æ˜¯æ¯æ­¥å›å½’ä¸­éœ€è¦ä¾æ®ä¸Šæ­¥çš„é¢„æµ‹ç»“æœè°ƒæ•´å„ä¸ªæ ·æœ¬çš„æƒé‡ï¼Œè®©ä¸Šæ­¥é¢„æµ‹ç»“æœå·®çš„æ ·æœ¬åœ¨ä¸‹æ­¥å›å½’ä¸­å çš„æƒé‡è¾ƒå¤§ã€‚é€šå¸¸ï¼Œæ¯æ­¥å›å½’ä½¿ç”¨çš„æ¨¡å‹æ¯”è¾ƒç®€å•ï¼Œå¦‚æ·±åº¦ä¸º3çš„æ ‘æ¨¡å‹ã€‚æå‡ç®—æ³•ä¹Ÿå±äºé›†æˆå­¦ä¹ ï¼Œå’Œå‰é¢ä¸åŒæ˜¯å®ƒçš„å¼±å­¦ä¹ å™¨ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œè€Œbaggingå’Œrandom forestçš„å¼±å­¦ä¹ å™¨æ˜¯å½¼æ­¤ç‹¬ç«‹çš„ã€‚ å¯¹äºé›†æˆç®—æ³•ï¼Œé€šå¸¸éœ€è¦è°ƒæ•´å¼±å­¦ä¹ å™¨çš„ç»“æ„å‚æ•°ï¼Œå¦‚æ ‘çš„æ·±åº¦ï¼Œä¹Ÿè¦åˆ¤æ–­å¼±å­¦ä¹ å™¨çš„ä¸ªæ•°ï¼Œè¿™äº›ç§°ä¸ºtuning parametersï¼Œé€šå¸¸é€šè¿‡æ¯”è¾ƒåœ¨éªŒè¯é›†ï¼ˆvalidationï¼‰çš„æŸå¤±è¿›è¡Œè°ƒå‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚å¼±å­¦ä¹ å™¨ä¸­çš„å‚æ•°é€šè¿‡åœ¨è®­ç»ƒé›†ï¼ˆtrainingï¼‰ä¸Šè®­ç»ƒæ¨¡å‹å¾—åˆ°ã€‚è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„å¹¶é›†ä¸ºå­¦ä¹ é›†ã€‚ å‰é¦ˆç¥ç»ç½‘ç»œçš„è¾“å…¥ç¥ç»å…ƒä¸ºé£é™©ä¿¡æ¯ï¼Œä¸‹ä¸€å±‚ç¥ç»å…ƒä¸ºä¸Šä¸€å±‚ç¥ç»å…ƒçš„çº¿æ€§ç»„åˆå¹¶é€šè¿‡æ¿€æ´»å‡½æ•°çš„éçº¿æ€§å˜æ¢ï¼Œæœ€åè¾“å‡ºç¥ç»å…ƒä¸ºç¥ç»ç½‘ç»œå¯¹å› å˜é‡æœŸæœ›çš„é¢„æµ‹ï¼Œé€šè¿‡å‡å°è¾“å‡ºç¥ç»å…ƒä¸å› å˜é‡è§‚å¯Ÿå€¼çš„å·®å¼‚ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œä¸­çš„å‚æ•°ã€‚ç¥ç»ç½‘ç»œå«æœ‰éå¸¸å¤šçš„å‚æ•°ï¼Œå¾ˆéš¾æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ï¼Œè€Œä¸”æœ€ä¼˜è§£å¿…ç„¶é€ æˆè¿‡æ‹Ÿåˆï¼Œæ‰€ä»¥ä¸€èˆ¬é‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•å¯¹å‚æ•°è¿›è¡Œè¿­ä»£ï¼Œä½¿å¾—è®­ç»ƒé›†æŸå¤±åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½æœ‰ä¸‹é™è¶‹åŠ¿ã€‚é€šè¿‡æ¯”è¾ƒéªŒè¯é›†æŸå¤±ç¡®å®šè¿­ä»£æ¬¡æ•°å’Œç¥ç»ç½‘ç»œçš„ç»“æ„å‚æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚ å¦‚ä½•è¯„ä»·ä¸€ä¸ªé¢„æµ‹æ¨¡å‹çš„å¥½åå‘¢ï¼Ÿé€šå¸¸ç”¨æ ·æœ¬å¤–æŸå¤±ï¼ˆtest errorï¼‰è¯„ä»·ã€‚å¯¹äºç´¢èµ”é¢‘ç‡ï¼Œä½¿ç”¨æ³Šæ¾åå·®æŸå¤±ï¼Œå¯¹äºç´¢èµ”å¼ºåº¦ï¼Œä½¿ç”¨ä¼½é©¬åå·®æŸå¤±ï¼Œå¯ä»¥è¯æ˜è¿™ä¸¤ä¸ªæŸå¤±å‡½æ•°å’Œä¼¼ç„¶å‡½æ•°æˆè´Ÿç›¸å…³ã€‚å…¶ä¸­ï¼Œå¹³å‡æ³Šæ¾åå·®æŸå¤±ä¸ºï¼š \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{2}{|\\mathbf{N}|}\\sum_{i}N_i\\left[\\frac{\\hat{N}_i}{N_i}-1-\\ln\\left(\\frac{\\hat{N}_i}{N_i}\\right)\\right]\\] Kerasä¸­å®šä¹‰çš„æŸå¤±å‡½æ•°ä¸º \\[\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{1}{|\\mathbf{N}|}\\sum_{i}\\left[\\hat{N}_i-N_i\\ln\\left(\\hat{N}_i\\right)\\right]\\] 1.3 ç‰¹å¾å·¥ç¨‹ åŠ è½½åŒ…ã€‚ rm(list=ls()) library(CASdatasets) library(keras) library(data.table) library(glmnet) library(plyr) library(mgcv) library(rpart) # library(rpart.plot) library(Hmisc) # library(randomForest) # library(distRforest) # devtools::install_github(&#39;henckr/distRforest&#39;) library(gbm) data(freMTPL2freq) #data(freMTPL2sev) textwidth&lt;-7.3 #inch #fwrite(freMTPL2freq,&quot;data/freMTPL2freq.txt&quot;) #freMTPL2freq&lt;-fread(&quot;data/freMTPL2freq_mac.txt&quot;) 1.3.1 æˆªæ–­ å‡å°‘outliers/influential points çš„å½±å“ éœ€æ ¹æ®æ¯ä¸ªå˜é‡çš„åˆ†å¸ƒç¡®å®šåœ¨å“ªé‡Œæˆªæ–­ ç´¢èµ”æ¬¡æ•°åœ¨4æˆªæ–­ é£é™©æš´éœ²åœ¨1æˆªæ–­ é©¬åŠ›åœ¨9æˆªæ–­ è½¦é¾„åœ¨20æˆªæ–­ å¹´é¾„åœ¨90æˆªæ–­ å¥–æƒ©ç³»æ•°åœ¨150æˆªæ–­ 1.3.2 ç¦»æ•£åŒ– ç›®çš„æ˜¯ä¸ºäº†åˆ»ç”»éçº¿æ€§æ•ˆåº” éœ€ç”»å‡ºåå˜é‡çš„è¾¹ç¼˜ç»éªŒç´¢èµ”é¢‘ç‡åˆ¤æ–­ ç¦»æ•£åŒ–é©¬åŠ›ã€è½¦é¾„ã€å¹´é¾„ VehPowerFac, VehAgeFacï¼ŒDrivAgeFac 1.3.3 è®¾å®šåŸºç¡€æ°´å¹³ æ–¹ä¾¿å‡è®¾æ£€éªŒ è®¾å®šå«æœ‰æœ€å¤šé£é™©æš´éœ²çš„æ°´å¹³ä¸ºåŸºå‡†æ°´å¹³ 1.3.4 åå˜é‡å˜å½¢ ç›®çš„æ˜¯ä¸ºäº†åˆ»ç”»éçº¿æ€§æ•ˆåº” è€ƒè™‘åå˜é‡åˆ†å¸ƒï¼Œä½¿ä¹‹å˜å½¢åè¿‘ä¼¼æœä»å¯¹ç§°åˆ†å¸ƒ DriveAgeLn/2/3/4, DensityLn dat &lt;- freMTPL2freq dat1 &lt;- dat # claim number dat1$ClaimNb &lt;- pmin(dat1$ClaimNb, 4) # exposure dat1$Exposure &lt;- pmin(dat1$Exposure, 1) # vehicle power dat1$VehPowerFac &lt;- as.factor(pmin(dat1$VehPower,9)) aggregate(dat1$Exposure,by=list(dat1$VehPowerFac),sum) dat1[,&quot;VehPowerFac&quot;] &lt;-relevel(dat1[,&quot;VehPowerFac&quot;], ref=&quot;6&quot;) # vehicle age dat1$VehAge &lt;- pmin(dat1$VehAge,20) VehAgeFac &lt;- cbind(c(0:110), c(1, rep(2,5), rep(3,5),rep(4,5), rep(5,5), rep(6,111-21))) dat1$VehAgeFac &lt;- as.factor(VehAgeFac[dat1$VehAge+1,2]) aggregate(dat1$Exposure,by=list(dat1$VehAgeFac),sum) dat1[,&quot;VehAgeFac&quot;] &lt;-relevel(dat1[,&quot;VehAgeFac&quot;], ref=&quot;2&quot;) # driver age dat1$DrivAge &lt;- pmin(dat1$DrivAge,90) DrivAgeFac &lt;- cbind(c(18:100), c(rep(1,21-18), rep(2,26-21), rep(3,31-26), rep(4,41-31), rep(5,51-41), rep(6,71-51), rep(7,101-71))) dat1$DrivAgeFac &lt;- as.factor(DrivAgeFac[dat1$DrivAge-17,2]) aggregate(dat1$Exposure,by=list(dat1$DrivAgeFac),sum) dat1[,&quot;DrivAgeFac&quot;] &lt;-relevel(dat1[,&quot;DrivAgeFac&quot;], ref=&quot;6&quot;) dat1$DrivAgeLn&lt;-log(dat1$DrivAge) dat1$DrivAge2&lt;-dat1$DrivAge^2 dat1$DrivAge3&lt;-dat1$DrivAge^3 dat1$DrivAge4&lt;-dat1$DrivAge^4 # bms dat1$BonusMalus &lt;- as.integer(pmin(dat1$BonusMalus, 150)) # vehicle brand dat1$VehBrand &lt;- factor(dat1$VehBrand) # consider VehGas as categorical aggregate(dat1$Exposure,by=list(dat1$VehBrand),sum) dat1[,&quot;VehBrand&quot;] &lt;-relevel(dat1[,&quot;VehBrand&quot;], ref=&quot;B1&quot;) # vehicle gas dat1$VehGas &lt;- factor(dat1$VehGas) # consider VehGas as categorical aggregate(dat1$Exposure,by=list(dat1$VehGas),sum) dat1[,&quot;VehGas&quot;] &lt;-relevel(dat1[,&quot;VehGas&quot;], ref=&quot;Regular&quot;) # area (related to density) dat1$Area &lt;- as.integer(dat1$Area) # density dat1$DensityLn &lt;- as.numeric(log(dat1$Density)) # region aggregate(dat1$Exposure,by=list(dat1$Region),sum)[order(aggregate(dat1$Exposure,by=list(dat1$Region),sum)$x),] dat1[,&quot;Region&quot;] &lt;-relevel(dat1[,&quot;Region&quot;], ref=&quot;Centre&quot;) str(dat1) # model matrix for GLM design_matrix&lt;-model.matrix( ~ ClaimNb + Exposure + VehPowerFac + VehAgeFac + DrivAge + DrivAgeLn + DrivAge2 + DrivAge3 + DrivAge4 + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data=dat1)[,-1] # VehPower, VehAge as factor variables # design_matrix2&lt;-model.matrix( ~ ClaimNb + Exposure + VehPower + VehAge + DrivAge + # BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data=dat1)[,-1] # VehPower, VehAge, and DrivAge as continuous variables # dim(design_matrix2) 1.4 è®­ç»ƒé›†-éªŒè¯é›†-æµ‹è¯•é›† æ¯”ä¾‹ä¸º\\(0.6:0.2:0.2\\) æ ¹æ®ç´¢èµ”æ¬¡æ•°åˆ†å±‚æŠ½æ · ç»éªŒç´¢èµ”é¢‘ç‡çº¦ä¸º\\(10%\\) seed_split&lt;-11 # claim 0/1 proportions index_zero&lt;-which(dat1$ClaimNb==0) index_one&lt;-which(dat1$ClaimNb&gt;0) prop_zero&lt;-round(length(index_zero)/(length(index_one)+length(index_zero)),2);prop_zero prop_one&lt;-round(length(index_one)/(length(index_one)+length(index_zero)),2);prop_one # 0.6:0.2:0.2 size_valid&lt;-round(nrow(dat1)*0.2,0) size_test&lt;-size_valid size_train&lt;-nrow(dat1)-2*size_valid # stratified sampling set.seed(seed_split) index_train_0&lt;-sample(index_zero,size_train*prop_zero) index_train_1&lt;-sample(index_one, size_train-length(index_train_0)) index_train&lt;-union(index_train_0,index_train_1) length(index_train);size_train index_valid&lt;-c(sample(setdiff(index_zero,index_train_0),round(size_valid*prop_zero,0)), sample(setdiff(index_one,index_train_1),size_valid-round(size_valid*prop_zero,0))) length(index_valid);size_valid index_test&lt;-setdiff(union(index_zero,index_one),union(index_train,index_valid)) index_learn&lt;-union(index_train,index_valid) length(index_train);length(index_valid);length(index_test) # train-validation-test; learn-testçš„ dat1_train&lt;-dat1[index_train,] dat1_valid&lt;-dat1[index_valid,] dat1_test&lt;-dat1[index_test,] dat1_learn&lt;-dat1[index_learn,] sum(dat1_train$ClaimNb)/sum(dat1_train$Exposure) sum(dat1_valid$ClaimNb)/sum(dat1_valid$Exposure) sum(dat1_test$ClaimNb)/sum(dat1_test$Exposure) sum(dat1_learn$ClaimNb)/sum(dat1_learn$Exposure) # glm matrix matrix_train&lt;-design_matrix[index_train,] matrix_valid&lt;-design_matrix[index_valid,] matrix_test&lt;-design_matrix[index_test,] matrix_learn&lt;-design_matrix[index_learn,] # gbm matrix (learn) dat1_learn_gbm&lt;-dat1_learn[,c(&quot;ClaimNb&quot;, &quot;Exposure&quot;, &quot;VehPower&quot;, &quot;VehAge&quot;, &quot;DrivAge&quot;, &quot;BonusMalus&quot;, &quot;VehBrand&quot;, &quot;VehGas&quot;, &quot;Area&quot;, &quot;DensityLn&quot;, &quot;Region&quot;)] class(dat1_learn_gbm) train_pro&lt;-size_train/(size_train+size_valid) 1.5 æ³Šæ¾åå·®æŸå¤±å‡½æ•° å¹³å‡æ³Šæ¾åå·®æŸå¤± \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{2}{|\\mathbf{N}|}\\sum_{i}N_i\\left[\\frac{\\hat{N}_i}{N_i}-1-\\ln\\left(\\frac{\\hat{N}_i}{N_i}\\right)\\right]\\] Keraså®šä¹‰å¹³å‡æ³Šæ¾åå·®æŸå¤±ä¸º \\[\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{1}{|\\mathbf{N}|}\\sum_{i}\\left[\\hat{N}_i-N_i\\ln\\left(\\hat{N}_i\\right)\\right]\\] å› ä¸ºå¯¹äºå¤§éƒ¨åˆ†ä¿å•ï¼Œ\\(N_i-N_i\\ln N_i\\approx0\\)ï¼Œæ‰€ä»¥æ³Šæ¾åå·®æŸå¤±å‡½æ•°çº¦ä¸ºKeraså®šä¹‰çš„2å€ï¼ˆè‡³å°‘åœ¨ä¸€ä¸ªé‡çº§ï¼‰ã€‚ \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})\\approx2\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})\\] Poisson.Deviance &lt;- function(pred,obs) {200*(sum(pred)-sum(obs)+sum(log((obs/pred)^(obs))))/length(pred)} keras_poisson_dev&lt;-function(y_hat,y_true) {100*sum(y_hat-y_true*log(y_hat))/length(y_true)} f_keras&lt;-function(x) 100*(x-x*log(x)) f_keras(0.1);f_keras(0.2) # png(&quot;./plots/poi_dev.png&quot;) plot(seq(0.05,0.15,0.01),f_keras(seq(0.05,0.15,0.01)),type=&quot;l&quot;, xlab=&quot;frequency&quot;,ylab=&quot;approximated Poisson deviance&quot;) abline(v=0.1,lty=2);abline(h=f_keras((0.1)),lty=2) # dev.off() knitr::opts_chunk$set(fig.pos = &quot;!H&quot;, out.extra = &quot;&quot;) knitr::include_graphics(&quot;./plots/poi_dev.png&quot;) 1.6 GLM &amp; GAM ä½¿ç”¨æå¤§ä¼¼ç„¶æ–¹æ³•åœ¨æ˜ å°„ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä¼˜çš„å›å½’æ–¹ç¨‹ åœ¨æå¤§ä¼¼ç„¶ä¸­ä½¿ç”¨çš„æ•°æ®é›†ç§°ä¸ºå­¦ä¹ é›†ï¼ˆlearning data setï¼‰ã€‚ ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œåå˜é‡é€‰æ‹©ï¼Œå¯ä»¥åˆ æ‰ä¸æ˜¾è‘—çš„åå˜é‡ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é€æ­¥å›å½’ã€æœ€ä¼˜å­é›†ã€LASSOç­‰ï¼Œåˆ¤æ–­æ ‡å‡†ä¸ºAICç­‰ã€‚ 1.6.1 GLM åŒè´¨æ¨¡å‹ \\[\\mathbf{E}(N)=\\beta_0\\] å…¨æ¨¡å‹ \\[\\ln \\mathbf{E}(N)=\\ln e + \\beta_0 + \\beta_{\\text{VehPowerFac}} + \\beta_{\\text{VehAgeFac}} \\\\ + \\beta_1\\text{DrivAge} + \\beta_2\\ln\\text{DrivAge} + \\beta_3\\text{DrivAge}^2 + \\beta_4\\text{DrivAge}^3 + \\beta_5\\text{DrivAge}^4 \\\\ \\beta_6\\text{BM} + \\beta_{\\text{VehBrand}} + \\beta_{\\text{VehGas}} + \\beta_7\\text{Area} + \\beta_8\\text{DensityLn} + \\beta_{\\text{Region}}\\] # homogeneous model d.glm0 &lt;- glm(ClaimNb ~ 1 + offset(log (Exposure)), data=data.frame(matrix_learn), family=poisson()) #summary(d.glm0) dat1_test$fitGLM0 &lt;- predict(d.glm0, newdata=data.frame(matrix_test), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM0,matrix_test[,1]) Poisson.Deviance(dat1_test$fitGLM0,matrix_test[,1]) # full GLM names(data.frame(matrix_learn)) {t1 &lt;- proc.time() d.glm1 &lt;- glm(ClaimNb ~ .-Exposure + offset(log(Exposure)), data=data.frame(matrix_learn), family=poisson()) (proc.time()-t1)} # summary(d.glm1) dat1_train$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_train), type=&quot;response&quot;) dat1_valid$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_valid), type=&quot;response&quot;) dat1_test$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_test), type=&quot;response&quot;) dat1_learn$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_learn), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM1,matrix_test[,1]) Poisson.Deviance(dat1_test$fitGLM1,matrix_test[,1]) 1.6.2 GAM GAMè¾¹ç¼˜æå‡æ¨¡å‹ \\[\\ln \\mathbf{E}(N)=\\ln\\hat{\\lambda}_{\\text{GLM}}+s_1(\\text{VehAge})+s_2(\\text{BM})\\] \\(s_1,s_2\\)ä¸ºæ ·æ¡å¹³æ»‘å‡½æ•°ã€‚ ä½¿ç”¨ddplyèšåˆæ•°æ®ï¼Œæ‰¾åˆ°å……åˆ†ç»Ÿè®¡é‡ï¼ŒåŠ å¿«æ¨¡å‹æ‹Ÿåˆé€Ÿåº¦ã€‚ # GAM marginals improvement (VehAge and BonusMalus) {t1 &lt;- proc.time() dat.GAM &lt;- ddply(dat1_learn, .(VehAge, BonusMalus), summarise, fitGLM1=sum(fitGLM1), ClaimNb=sum(ClaimNb)) set.seed(1) d.gam &lt;- gam(ClaimNb ~ s(VehAge, bs=&quot;cr&quot;)+s(BonusMalus, bs=&quot;cr&quot;) + offset(log(fitGLM1)), data=dat.GAM, method=&quot;GCV.Cp&quot;, family=poisson) (proc.time()-t1)} summary(d.gam) dat1_train$fitGAM1 &lt;- predict(d.gam, newdata=dat1_train,type=&quot;response&quot;) dat1_valid$fitGAM1 &lt;- predict(d.gam, newdata=dat1_valid,type=&quot;response&quot;) dat1_test$fitGAM1 &lt;- predict(d.gam, newdata=dat1_test,type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGAM1, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitGAM1,matrix_test[,1]) 1.6.3 Step wiseã€LASSOåå˜é‡é€‰æ‹© é€æ­¥å›å½’éå¸¸æ…¢ï¼Œåœ¨Linux 8æ ¸i7 3.4GHz 16Gå†…å­˜éƒ½éœ€è¦50å¤šåˆ†é’Ÿã€‚ä¸”æ ·æœ¬å¤–æŸå¤±å’Œå…¨æ¨¡å‹æ²¡æœ‰æ˜æ˜¾å‡å°ã€‚ 5æŠ˜CV Lassoåœ¨Linux 8æ ¸i7 3.4GHz 16Gå†…å­˜éœ€è¦5åˆ†é’Ÿã€‚ æ ¹æ®5æŠ˜CV-erroré€‰å–æ­£åˆ™å‚æ•°beta=4*10^-5ï¼Œä½†æ ·æœ¬å¤–æŸå¤±å’Œå…¨æ¨¡å‹æ²¡æœ‰æ˜æ˜¾å‡å°ã€‚ # step wise selectionï¼› this takes a long time (more than 50 minutes!) # d.glm00 &lt;- glm(ClaimNb ~ VehAgeFac1 + VehAgeFac3 + VehAgeFac4 + VehAgeFac5 + # DrivAge + DrivAge2 + DrivAge3 + DrivAge4 + DrivAgeLn + # BonusMalus + VehBrandB12 + VehGasDiesel + DensityLn + # offset(log (Exposure)), # data=data.frame(matrix_learn), family=poisson()) # {t1 &lt;- proc.time() # d.glm2&lt;-step(d.glm00,direction=&quot;forward&quot;,trace = 1, # scope =list(lower=formula(d.glm00), upper=formula(d.glm1))) # (proc.time()-t1)} d.glm2&lt;-glm(ClaimNb ~ VehAgeFac1 + VehAgeFac3 + VehAgeFac4 + VehAgeFac5 + DrivAge + DrivAge2 + DrivAge3 + DrivAge4 + DrivAgeLn + BonusMalus + VehBrandB12 + VehGasDiesel + DensityLn + VehPowerFac4 + VehPowerFac8 + RegionNord.Pas.de.Calais + VehPowerFac7 + RegionRhone.Alpes + RegionBretagne + RegionAuvergne + RegionLimousin + RegionLanguedoc.Roussillon + RegionIle.de.France + RegionAquitaine + RegionMidi.Pyrenees + RegionPays.de.la.Loire + RegionProvence.Alpes.Cotes.D.Azur + RegionPoitou.Charentes + RegionHaute.Normandie + VehBrandB5 + VehBrandB11 + RegionBasse.Normandie + VehBrandB14 + RegionCorse + offset(log(Exposure)), data=data.frame(matrix_learn), family=poisson()) summary(d.glm2) dat1_test$fitGLM2 &lt;- predict(d.glm2, newdata=data.frame(matrix_test), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM2,data.frame(matrix_test)$ClaimNb) Poisson.Deviance(dat1_test$fitGLM2,matrix_test[,1]) # lasso regressionï¼› this takes a few minutes alpha0=1 # 1 for lasso, 0 for ridge. set.seed(7) # {t1 &lt;- proc.time() # cvfit = cv.glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], # family = &quot;poisson&quot;,offset=log(matrix_learn[,2]), # alpha = alpha0,nfolds = 5,trace.it = 1) # (proc.time()-t1)} # cvfit$lambda.min #4*10^-5 # cvfit$lambda.1se # 0.0016 # plot(cvfit) d.glm3 = glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], family = &quot;poisson&quot;, offset=log(matrix_learn[,2]), alpha=alpha0, lambda=4.024746e-05, trace.it = 1) dat1_test$fitLasso&lt;-predict(d.glm3, newx = matrix_test[,-c(1,2)], newoffset=log(matrix_test[,2]),type = &quot;response&quot;) keras_poisson_dev(dat1_test$fitLasso, matrix_test[,1]) Poisson.Deviance(dat1_test$fitLasso, matrix_test[,1]) 1.7 Possion tree ä½¿ç”¨ recursive partitioning by binary splits ç®—æ³•å¯¹é£é™©ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œä½¿å¾—å„å­ç©ºé—´å†…çš„åº”å˜é‡å·®å¼‚æœ€å° ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½¿ç”¨äº¤å‰éªŒè¯ç¡®å®šcost-complexity parameter cp=10^-3.32(1-SD rule)æˆ–è€…cp=10^-4(min CV rule)ï¼Œè¿›è€Œå¯¹æ ‘çš„æ·±åº¦è¿›è¡Œæ§åˆ¶ã€‚ # cross validation using xval in rpart.control names(dat1_learn) set.seed(1) {t1 &lt;- proc.time() tree0&lt;-rpart(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_learn, method = &quot;poisson&quot;, seed =1, control = rpart.control (xval=5, minbucket=1000, cp=10^-5, maxcompete = 0, maxsurrogate = 0)) (proc.time()-t1)} # printcp(tree0) x0 &lt;- log10(tree0$cptable[,1]) err0&lt;-tree0$cptable[,4] std0&lt;-tree0$cptable[,5] xmain &lt;- &quot;cross-validation error plot&quot; xlabel &lt;- &quot;cost-complexity parameter (log-scale)&quot; ylabel &lt;- &quot;relative CV error&quot; # png(&quot;./plots/tree_cv.png&quot;) errbar(x=x0, y=err0*100, yplus=(err0+std0)*100, yminus=(err0-std0)*100, xlim=rev(range(x0)), col=&quot;blue&quot;, main=xmain, ylab=ylabel, xlab=xlabel) lines(x=x0, y=err0*100, col=&quot;blue&quot;) abline(h=c(min(err0+std0)*100), lty=1, col=&quot;orange&quot;) abline(h=c(min(err0)*100), lty=1, col=&quot;magenta&quot;) abline(v=-3.32,lty=2) legend(x=&quot;topright&quot;, col=c(&quot;blue&quot;, &quot;orange&quot;, &quot;magenta&quot;,&quot;black&quot;), lty=c(1,1,1,2), lwd=c(1,1,1,1), pch=c(19,-1,-1,-1), legend=c(&quot;tree0&quot;, &quot;1-SD rule&quot;, &quot;min.CV rule&quot;,&quot;log cp = -3.32&quot;)) # dev.off() tree1 &lt;- prune(tree0, cp=10^-3.32) # cp=10^-3.32 tree11&lt;- prune(tree0, cp=tree0$cptable[which.min(err0),1]) # cp=10^-4 # printcp(tree1) # printcp(tree11) # tree1 dat1_test$fitRT_1se &lt;- predict(tree1, newdata=dat1_test)*dat1_test$Exposure dat1_test$fitRT_min &lt;- predict(tree11, newdata=dat1_test)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitRT_1se, dat1_test$ClaimNb) keras_poisson_dev(dat1_test$fitRT_min, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT_1se, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT_min, dat1_test$ClaimNb) tree1$variable.importance tree11$variable.importance knitr::opts_chunk$set(fig.pos = &quot;!H&quot;, out.extra = &quot;&quot;) knitr::include_graphics(&quot;./plots/tree_cv.png&quot;) äº¤å‰éªŒè¯å¯ä½¿ç”¨rpart(..., control=rpart.control(xval= ,...))æˆ–è€…xpred.rpart(tree, group)ã€‚ ä»¥ä¸Šä¸¤ç§æ–¹å¼å¾—åˆ°ç›¸åŒçš„å‰ªææ ‘ï¼Œcp=10^-4ï¼ˆmin CV ruleï¼‰ Variable importance (min CV rule) BonusMalus VehAge VehBrand DrivAge VehGas VehPower Region DensityLn 4675.0231 4396.8667 1389.2909 877.9473 795.6308 715.3584 480.3459 140.5463 # K-fold cross-validation using xpred.rpart set.seed(1) {t1 &lt;- proc.time() tree00&lt;-rpart(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_learn, method = &quot;poisson&quot;, seed = 1, control = rpart.control (xval=1, minbucket=1000 ,cp=5*10^-5, maxcompete = 0, maxsurrogate = 0)) (proc.time()-t1)} (n_subtrees &lt;- dim(tree00$cptable)[1]) std1&lt;- numeric(n_subtrees) err1 &lt;- numeric(n_subtrees) K &lt;- 10 xgroup &lt;- rep(1:K, length = nrow(dat1_learn)) xfit &lt;- xpred.rpart(tree00, xgroup) dim(xfit) for (i in 1:n_subtrees){ err_group&lt;-rep(NA,K) for (k in 1:K){ ind_group &lt;- which(xgroup ==k) err_group[k] &lt;- keras_poisson_dev(dat1_learn[ind_group,&quot;Exposure&quot;]*xfit[ind_group,i], dat1_learn[ind_group,&quot;ClaimNb&quot;]) } err1[i] &lt;- mean(err_group) std1[i] &lt;- sd(err_group) } x1 &lt;- log10(tree00$cptable[,1]) xmain &lt;- &quot;cross-validation error plot&quot; xlabel &lt;- &quot;cost-complexity parameter (log-scale)&quot; ylabel &lt;- &quot;CV error (in 10^(-2))&quot; errbar(x=x1, y=err1*100, yplus=(err1+std1)*100, yminus=(err1-std1)*100, xlim=rev(range(x1)), col=&quot;blue&quot;, main=xmain, ylab=ylabel, xlab=xlabel) lines(x=x1, y=err1*100, col=&quot;blue&quot;) abline(h=c(min(err1+std1)*100), lty=1, col=&quot;orange&quot;) abline(h=c(min(err1)*100), lty=1, col=&quot;magenta&quot;) abline(v=-3.12,lty=2) legend(x=&quot;topright&quot;, col=c(&quot;blue&quot;, &quot;orange&quot;, &quot;magenta&quot;,&quot;black&quot;), lty=c(1,1,1,2), lwd=c(1,1,1,1), pch=c(19,-1,-1,-1), legend=c(&quot;tree1&quot;, &quot;1-SD rule&quot;, &quot;min.CV rule&quot;,&quot;log cp = -3.12&quot;)) tree2 &lt;- prune(tree00, cp=10^-3.12) tree22 &lt;- prune(tree00, cp=tree00$cptable[which.min(err1),1]) printcp(tree2) printcp(tree22) dat1_test$fitRT2 &lt;- predict(tree2, newdata=dat1_test)*dat1_test$Exposure dat1_test$fitRT22 &lt;- predict(tree22, newdata=dat1_test)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitRT2, dat1_test$ClaimNb) keras_poisson_dev(dat1_test$fitRT22, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT2, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT22, dat1_test$ClaimNb) sum((dat1_test$fitRT22-dat1_test$fitRT11)^2) tree2$variable.importance tree22$variable.importance tree11$variable.importance 1.8 Random forest ä½¿ç”¨https://github.com/henckr/distRforestå»ºç«‹æ³Šæ¾éšæœºæ£®æ—ã€‚ ncandæ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„åå˜é‡ä¸ªæ•°ï¼›subsampleè®­ç»ƒæ¯æ£µæ ‘çš„æ ·æœ¬ã€‚ ä½¿ç”¨éªŒè¯æŸå¤±ç¡®å®šæ ‘çš„æ•°é‡ã€‚ # fit the random forest library(distRforest) ntrees0&lt;-100 set.seed(1) {t1 &lt;- proc.time() forest1&lt;-rforest(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_train, method = &quot;poisson&quot;, control = rpart.control (xval=0, minbucket=1000 ,cp=5*10^-5, maxcompete = 0,maxsurrogate = 0), parms=list(shrink=0), ncand=5,ntrees = ntrees0, subsample = 0.5, red_mem = T) (proc.time()-t1)} # determine number of trees using validation error fit_valid&lt;-rep(0,nrow(dat1_valid)) error_valid&lt;-rep(0,ntrees0) for (i in 1:ntrees0){ fit_valid&lt;-fit_valid + predict(forest1$trees[[i]], newdata=dat1_valid) * dat1_valid$Exposure fit_valid_norm &lt;- fit_valid/i error_valid[i]&lt;-Poisson.Deviance(fit_valid_norm, dat1_valid$ClaimNb) } # png(&quot;./plots/random_forest_error.png&quot;) plot(error_valid,type=&quot;l&quot;,xlab=&quot;number of trees&quot;,ylab=&quot;validation error in 10^-2&quot;) abline(v=which.min(error_valid),lty=2) # dev.off() (best.trees=which.min(error_valid)) # test error fitRF&lt;-rep(0,nrow(dat1_test)) for (i in 1:best.trees){ fitRF&lt;-fitRF+predict(forest1$trees[[i]], newdata=dat1_test)*dat1_test$Exposure } dat1_test$fitRF &lt;- fitRF/best.trees keras_poisson_dev(dat1_test$fitRF, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRF, dat1_test$ClaimNb) names(forest1$trees[[2]]$variable.importance) sum(forest1$trees[[3]]$variable.importance) knitr::opts_chunk$set(fig.pos = &quot;!H&quot;, out.extra = &quot;&quot;) knitr::include_graphics(&quot;./plots/random_forest_error.png&quot;) 1.9 Boosting Poisson tree n.trees æ ‘çš„æ•°é‡ï¼›shrinkage å­¦ä¹ æ­¥é•¿ï¼Œå’Œæ ‘çš„æ•°é‡æˆåæ¯”ï¼›interaction.depth äº¤äº’é¡¹æ·±åº¦ï¼›bag.fraction æ¯æ£µæ ‘ä½¿ç”¨çš„æ•°æ®æ¯”ä¾‹ï¼›train.fraction è®­ç»ƒé›†æ¯”ä¾‹ï¼›n.minobsinnodeå¶å­ä¸Šæœ€å°‘æ ·æœ¬é‡ã€‚ Variable importance rel.inf BonusMalus 27.687137 VehAge 19.976441 VehBrand 13.515198 Region 13.495375 DrivAge 9.284520 VehGas 7.082648 VehPower 4.583522 DensityLn 4.375159 Area 0.000000 set.seed(1) {t1 &lt;- proc.time() gbm1 &lt;- gbm( ClaimNb ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region + offset(log(Exposure)), data = dat1_learn_gbm, distribution = &quot;poisson&quot;, n.trees = 200, shrinkage = 0.3, interaction.depth = 5, bag.fraction = 0.5, train.fraction = train_pro, cv.folds = 0, n.minobsinnode = 1000, verbose = T ) (proc.time()-t1)} # plot the performance # png(&quot;./plots/gbm_error.png&quot;) gbm.perf(gbm1,method=&quot;test&quot;) legend(&quot;topright&quot;,lty=c(1,1,2),col=c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;),c(&quot;training error&quot;, &quot;validation error&quot;, &quot;best iterations&quot;)) # dev.off() best.iter&lt;-gbm.perf(gbm1,method=&quot;test&quot;) dat1_test$fitGBM1&lt;-predict(gbm1, dat1_test,n.trees=best.iter,type=&quot;response&quot;)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitGBM1,dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitGBM1,dat1_test$ClaimNb) # plot variable importance # png(&quot;./plots/gbm_imp.png&quot;) summary(gbm1) # dev.off() gbm1 # plot the important variable after &quot;best&quot; iterations # png(&quot;./plots/gbm_mar.png&quot;) par(mfrow=c(2,2)) plot(gbm1,4,best.iter) plot(gbm1,2,best.iter) plot(gbm1,5,best.iter) plot(gbm1,3,best.iter) # dev.off() par(mfrow=c(2,2)) plot(gbm1,c(4,2),best.iter,col=terrain.colors(20)) plot(gbm1,c(4,5),best.iter) plot(gbm1,c(4,3),best.iter) plot(gbm1,c(2,3),best.iter) 1.10 Summary dev_sum &lt;- data.frame(model=c(&quot;Intercept&quot;,&quot;GLM&quot;,&quot;GLM Lasso&quot;,&quot;GAM&quot;,&quot;Decision tree&quot;, &quot;Random forest&quot;,&quot;Generalized boosted model&quot;), test_error=rep(NA,7),test_error_keras=rep(NA,7)) dev_sum$test_error[1]&lt;-Poisson.Deviance(dat1_test$fitGLM0,matrix_test[,1]) dev_sum$test_error_keras[1]&lt;-keras_poisson_dev(dat1_test$fitGLM0,matrix_test[,1]) dev_sum$test_error[2]&lt;-Poisson.Deviance(dat1_test$fitGLM1,matrix_test[,1]) dev_sum$test_error_keras[2]&lt;-keras_poisson_dev(dat1_test$fitGLM1,matrix_test[,1]) dev_sum$test_error[3]&lt;-Poisson.Deviance(dat1_test$fitLasso,matrix_test[,1]) dev_sum$test_error_keras[3]&lt;-keras_poisson_dev(dat1_test$fitLasso,matrix_test[,1]) dev_sum$test_error[4]&lt;-Poisson.Deviance(dat1_test$fitGAM1, dat1_test$ClaimNb) dev_sum$test_error_keras[4]&lt;-keras_poisson_dev(dat1_test$fitGAM1, dat1_test$ClaimNb) dev_sum$test_error[5]&lt;-Poisson.Deviance(dat1_test$fitRT_min, dat1_test$ClaimNb) dev_sum$test_error_keras[5]&lt;-keras_poisson_dev(dat1_test$fitRT_min, dat1_test$ClaimNb) dev_sum$test_error[6]&lt;-Poisson.Deviance(dat1_test$fitRF, dat1_test$ClaimNb) dev_sum$test_error_keras[6]&lt;-keras_poisson_dev(dat1_test$fitRF, dat1_test$ClaimNb) dev_sum$test_error[7]&lt;-Poisson.Deviance(dat1_test$fitGBM1,dat1_test$ClaimNb) dev_sum$test_error_keras[7]&lt;-keras_poisson_dev(dat1_test$fitGBM1,dat1_test$ClaimNb) dev_sum[,2:3]&lt;-round(dev_sum[,2:3],4) write.csv(dev_sum,&quot;./plots/dev_sum.csv&quot;) dev_sum "]]
